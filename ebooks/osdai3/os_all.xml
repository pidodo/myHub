<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V5.0//EN" "/usr/share/xml/docbook5/schema/dtd/5.0/docbook.dtd">
<book xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xl="http://www.w3.org/1999/xlink" version="5.0">
  <info>

    <title>Operating Systems Design and Implementation, 3/E </title>

    <authorgroup>
      <author>
	<personname>
	  <firstname>Andrew S.</firstname>
	  <surname>Tanenbaum - Vrije Universiteit Amsterdam, The Netherlands</surname>
	</personname>
      </author>
      <author>
	<personname>
	  <firstname>Albert S.</firstname>
	  <surname>Woodhull - Amherst, Massachusetts</surname>
	</personname>
      </author>
    </authorgroup>

    <pubdate>:January 04, 2006</pubdate>

    <copyright>
      <year>2006</year>
      <holder>Prentice Hall</holder>
    </copyright>
    
    <abstract>
      <para>
	Revised to address the latest version of MINIX (MINIX 3), this streamlined, simplified new edition remains the only operating systems text to first explain relevant principles, then demonstrate their applications using a Unix-like operating system as a detailed example. It has been especially designed for high reliability, for use in embedded systems, and for ease of teaching.
      </para>
      <para>
	For the latest version of MINIX and simulators for running MINIX on other systems visit:  <emphasis role="bold">www.minix3.org</emphasis>
      </para>
    </abstract>
  </info>

  <dedication>
    <para>
      To Suzanne, Barbara, Marvin, and the memory of Sweetie p and Bram
    </para>
    <para>
      AST
    </para>
    <para>
      To Barbara and Gordon
    </para>
    <para>
      ASW
    </para>
    <mediaobject>
      <imageobject>
	<imagedata fileref="images/minix3.jpg" format="jpg">
	</imagedata>
      </imageobject>
    </mediaobject>
    <para>
      The MINIX 3 Mascot
    </para>
    <para>
      Other operating systems have an animal mascot, so we felt MINIX 3 ought to have one too. We chose the raccoon because raccoons are small, cute, clever, agile, eat bugs, and are user-friendlyat least if you keep your garbage can well locked.
    </para>
  </dedication>

  <preface>
    <title>Preface</title>
    <para>
      Most books on operating systems are strong on theory and weak on practice. This one aims to provide a better balance between the two. It covers all the fundamental principles in great detail, including processes, interprocess communication, semaphores, monitors, message passing, scheduling algorithms, input/output, deadlocks, device drivers, memory management, paging algorithms, file system design, security, and protection mechanisms. But it also discusses one particular systemMINIX 3a UNIX-compatible operating system in detail, and even provides a source code listing for study. This arrangement allows the reader not only to learn the principles, but also to see how they are applied in a real operating system.
    </para>
    <para>
      When the first edition of this book appeared in 1987, it caused something of a small revolution in the way operating systems courses were taught. Until then, most courses just covered theory. With the appearance of MINIX, many schools began to have laboratory courses in which students examined a real operating system to see how it worked inside. We consider this trend highly desirable and hope it continues.
    </para>
    <para>
      It its first 10 years, MINIX underwent many changes. The original code was designed for a 256K 8088-based IBM PC with two diskette drives and no hard disk. It was also based on UNIX Version 7 As time went on, MINIX evolved in many ways: it supported 32-bit protected mode machines with large memories and hard disks. It also changed from being based on Version 7, to being based on the international POSIX standard (IEEE 1003.1 and ISO 9945-1). Finally, many new features were added, perhaps too many in our view, but too few in the view of some other people, which led to the creation of Linux. In addition, MINIX was ported to many other platforms, including the Macintosh, Amiga, Atari, and SPARC. A second edition of the book, covering this system, was published in 1997 and was widely used at universities.
    </para>
    <para>
      The popularity of MINIX has continued, as can be observed by examining the number of hits for MINIX found by Google.
    </para>
    <para>
      This third edition of the book has many changes throughout. Nearly all of the material on principles has been revised, and considerable new material has been added. However, the main change is the discussion of the new version of the system, called MINIX 3. and the inclusion of the new code in this book. Although loosely based on MINIX 2, MINIX 3 is fundamentally different in many key ways.
    </para>
    <para>
      The design of MINIX 3 was inspired by the observation that operating systems are becoming bloated, slow, and unreliable. They crash far more often than other electronic devices such as televisions, cell phones, and DVD players and have so many features and options that practically nobody can understand them fully or manage them well. And of course, computer viruses, worms, spyware, spam, and other forms of malware have become epidemic.
    </para>
    <para>
      To a large extent, many of these problems are caused by a fundamental design flaw in current operating systems: their lack of modularity. The entire operatng system is typically millions of lines of C/C++ code compiled into a single massive executable program run in kernel mode. A bug in any one of those millions of lines of code can cause the system to malfunction. Getting all this code correct is impossible, especially when about 70% consists of device drivers, written by third parties, and outside the purview of the people maintaining the operating system.
    </para>
    <para>
      With MINIX 3, we demonstrate that this monolithic design is not the only possibility. The MINIX 3 kernel is only about 4000 lines of executable code, not the millions found in Windows, Linux, Mac OSX, or FreeBSD. The rest of the system, including all the device drivers (except the clock driver), is a collection of small, modular, user-mode processes, each of which is tightly restricted in what it can do and with which other processes it may communicate.
    </para>
    <para>
      While MINIX 3 is a work in progress, we believe that this model of building an operating system as a collection of highly-encapsulated user-mode processes holds promise for building more reliable systems in the future. MINIX 3 is especially focused on smaller PCs (such as those commonly found in Third-World countries and on embedded systems, which are always resource constrained). In any event, this design makes it much easier for students to learn how an operating system works than attempting to study a huge monolithic system.
    </para>
    <para>
      The CD-ROM that is included in this book is a live CD. You can put it in your CD-ROM drive, reboot the computer, and MINIX 3 will give a login prompt within a few seconds. You can log in as root and give the system a try without first having to install it on your hard disk. Of course, it can also be installed on the hard disk. Detailed installation instructions are given in <tag><xref linkend="AppendixA"/></tag>.
    </para>
    <para>
      As suggested above, MINIX 3 is rapidly evolving, with new versions being issued frequently. To download the current CD-ROM image file for burning, please go to the official Website:<tag><link xl:href="www.minix3.org">www.minix3.org</link></tag>. This site also contains a large amount of new software, documentation, and news about MINIX 3 development. For discussions about MINIX 3, or to ask questions, there is a USENET newsgroup: comp.os.minix. People without newsreaders can follow discussions on the Web at <tag><link xl:href="http://groups.google.com/group/comp.os.minix">http://groups.google.com/group/comp.os.minix</link></tag>.
    </para>
    <para>
      As an alternative to installing MINIX 3 on your hard disk, it is possible to run it on any one of several PC simulators now available. Some of these are listed on the main page of the Website.
    </para>
    <para>
      Instructors who are using the book as the text for a university course can get the problem solutions from their local Prentice Hall representative. The book has its own Website. It can be found by going to <tag><link xl:href="www.prenhall.com/tanenbaum">www.prenhall.com/tanenbaum</link></tag> and selecting this title.
    </para>
    <para>
      We have been extremely fortunate in having the help of many people during the course of this project. First and foremost, Ben Gras and Jorrit Herder have done most of the programming of the new version. They did a great job under tight time constraints, including responding to e-mail well after midnight on many occasions. They also read the manuscript and made many useful comments. Our deepest appreciation to both of them.
    </para>
    <para>
      Kees Bot also helped greatly with previous versions, giving us a good base to work with. Kees wrote large chunks of code for versions up to 2.0.4, repaired bugs, and answered numerous questions. Philip Homburg wrote most of the networking code as well as helping out in numerous other useful ways, especially providing detailed feedback on the manuscript.
    </para>
    <para>
      People too numerous to list contributed code to the very early versions, helping to get MINIX off the ground in the first place. There were so many of them and their contributions have been so varied that we cannot even begin to list them all here, so the best we can do is a generic thank you to all of them.
    </para>
    <para>
      Several people read parts of the manuscript and made suggestions. We would like to give our special thanks to Gojko Babic, Michael Crowley, Joseph M. Kizza, Sam Kohn Alexander Manov, and Du Zhang for their help.
    </para>
    <para>
      Finally, we would like to thank our families. Suzanne has been through this 16 times now. Barbara has been through it 15 times now. Marvin has been through it 14 times now. It's kind of getting to be routine, but the love and support is still much appreciated. (AST)
    </para>
    <para>
      Al's Barbara has been through this twice now. Her support, patience, and good humor were essential. Gordon has been a patient listener. It is still a delight to have a son who understands and cares about the things that fascinate me. Finally, step-grandson Zain's first birthday coincides with the release of MINIX 3. Some day he will appreciate this. (ASW)
    </para>
    <para>
      Andrew S. Tanenbaum
    </para>
    <para>
      Albert S. Woodhull
    </para>
  </preface>

  <chapter xmlns:xl="http://www.w3.org/1999/xlink" id="Chapter1">
  <title>Introduction</title>
  <para>
    Without its software, a computer is basically a useless lump of metal. With its software, a computer can store, process, and retrieve information; play music and videos; send e-mail, search the Internet; and engage in many other valuable activities to earn its keep. Computer software can be divided roughly into two kinds: system programs, which manage the operation of the computer itself, and application programs, which perform the actual work the user wants. The most fundamental system program is the <command><emphasis>operating system</emphasis></command>, whose job is to control all the computer's resources and provide a base upon which the application programs can be written. Operating systems are the topic of this book. In particular, an operating system called MINIX 3 is used as a model, to illustrate design principles and the realities of implementing a design.
  </para>
  <para>
    A modern computer system consists of one or more processors, some main memory, disks, printers, a keyboard, a display, network interfaces, and other input/output devices. All in all, a complex system. Writing programs that keep track of all these components and use them correctly, let alone optimally, is an extremely difficult job. If every programmer had to be concerned with how disk drives work, and with all the dozens of things that could go wrong when reading a disk block, it is unlikely that many programs could be written at all.
  </para>
  <para>
    Many years ago it became abundantly clear that some way had to be found to shield programmers from the complexity of the hardware. The way that has evolved gradually is to put a layer of software on top of the bare hardware, to manage all parts of the system, and present the user with an interface or <command><emphasis>virtual machine</emphasis></command> that is easier to understand and program. This layer of software is the operating system.
  </para>
  <para>
    The placement of the operating system is shown in <tag><link linkend="1-1">Fig. 1-1</link></tag>. At the bottom is the hardware, which, in many cases, is itself composed of two or more levels (or layers). The lowest level contains physical devices, consisting of integrated circuit chips, wires, power supplies, cathode ray tubes, and similar physical devices. How these are constructed and how they work is the province of the electrical engineer.
  </para>

  <para id="1-1"/>
  <screenshot>
    <mediaobject>
      <imageobject>
	<imagedata align="center" fileref="images/1-1.jpg" format="jpg">
	</imagedata>
      </imageobject>
      <caption>
	<para>
	  <command>Figure 1-1. A computer system consists of hardware, system programs, and application programs.</command>
	</para>
      </caption>
    </mediaobject>
  </screenshot>

  <para>
    Next comes the <command><emphasis>microarchitecture level</emphasis></command>, in which the physical devices are grouped together to form functional units. Typically this level contains some registers internal to the CPU (Central Processing Unit) and a data path containing an arithmetic logic unit. In each clock cycle, one or two operands are fetched from the registers and combined in the arithmetic logic unit (for example, by addition or Boolean AND). The result is stored in one or more registers. On some machines, the operation of the data path is controlled by software, called the <command><emphasis>microprogram</emphasis></command>. On other machines, it is controlled directly by hardware circuits.
  </para>
  <para>
    The purpose of the data path is to execute some set of instructions. Some of these can be carried out in one data path cycle; others may require multiple data path cycles. These instructions may use registers or other hardware facilities. Together, the hardware and instructions visible to an assembly language programmer form the <command><emphasis>ISA (Instruction Set Architecture)</emphasis></command> This level is often called <command><emphasis>machine language</emphasis></command>.
  </para>
  <para>
    The machine language typically has between 50 and 300 instructions, mostly for moving data around the machine, doing arithmetic, and comparing values. In this level, the input/output devices are controlled by loading values into special <command><emphasis>device registers</emphasis></command>. For example, a disk can be commanded to read by loading the values of the disk address, main memory address, byte count, and direction (read or write) into its registers. In practice, many more parameters are needed, and the status returned by the drive after an operation may be complex. Furthermore, for many I/O (Input/Output) devices, timing plays an important role in the programming.
  </para>
  <para>
    A major function of the operating system is to hide all this complexity and give the programmer a more convenient set of instructions to work with. For example, <command>read block from file</command> is conceptually much simpler than having to worry about the details of moving disk heads, waiting for them to settle down, and so on.
  </para>
  <para>
    On top of the operating system is the rest of the system software. Here we find the command interpreter (shell), window systems, compilers, editors, and similar application-independent programs. It is important to realize that these programs are definitely not part of the operating system, even though they are typically supplied preinstalled by the computer manufacturer, or in a package with the operating system if it is installed after purchase. This is a crucial, but subtle, point. The operating system is (usually) that portion of the software that runs in <command><emphasis>kernel mode</emphasis></command> or <command><emphasis>supervisor mode</emphasis></command>. It is protected from user tampering by the hardware (ignoring for the moment some older or low-end microprocessors that do not have hardware protection at all). Compilers and editors run in <command><emphasis>user mode</emphasis></command>. If a user does not like a particular compiler, he[] is free to write his own if he so chooses; he is not free to write his own clock interrupt handler, which is part of the operating system and is normally protected by hardware against attempts by users to modify it.
  </para>

  <para/>
  <screenshot>
    <mediaobject>
      <imageobject>
	<imagedata align="center" fileref="images/u2020.jpg" format="jpg">
	</imagedata>
      </imageobject>
      <caption>
	<para>
	  "He" should be read as "he or she" throughout the book.
	</para>
      </caption>
    </mediaobject>
  </screenshot>

  <para>
    This distinction, however, is sometimes blurred in embedded systems (which may not have kernel mode) or interpreted systems (such as Java-based systems that use interpretation, not hardware, to separate the components). Still, for traditional computers, the operating system is what runs in kernel mode.
  </para>
  <para>
    That said, in many systems there are programs that run in user mode but which help the operating system or perform privileged functions. For example, there is often a program that allows users to change their passwords. This program is not part of the operating system and does not run in kernel mode, but it clearly carries out a sensitive function and has to be protected in a special way.
  </para>
  <para>
    In some systems, including MINIX 3, this idea is carried to an extreme form, and pieces of what is traditionally considered to be the operating system (such as the file system) run in user space. In such systems, it is difficult to draw a clear boundary. Everything running in kernel mode is clearly part of the operating system, but some programs running outside it are arguably also part of it, or at least closely associated with it. For example, in MINIX 3, the file system is simply a big C program running in user-mode.
  </para>
  <para>
    Finally, above the system programs come the application programs. These programs are purchased (or written by) the users to solve their particular problems, such as word processing, spreadsheets, engineering calculations, or storing information in a database.
  </para>

  <sect1 id="sect-1.1">
    <title>1.1. What Is an Operating System?</title>
    <para>
      Most computer users have had some experience with an operating system, but it is difficult to pin down precisely what an operating system is. Part of the problem is that operating systems perform two basically unrelated functions, extending the machine and managing resources, and depending on who is doing the talking, you hear mostly about one function or the other. Let us now look at both.
    </para>
    <sect2 id="sect-1.1.1">
      <title>1.1.1. The Operating System as Extended Machine</title>
      <para>
	As mentioned earlier, the <command><emphasis>architecture</emphasis></command> (instruction set, memory organization, I/O, and bus structure) of most computers at the machine language level is primitive and awkward to program, especially for input/output. To make this point more concrete, let us briefly look at how floppy disk I/O is done using the NEC PD765 compatible controller chips used on many Intel-based personal computers. (Throughout this book we will use the terms "floppy disk" and "diskette" interchangeably.) The PD765 has 16 commands, each specified by loading between 1 and 9 bytes into a device register. These commands are for reading and writing data, moving the disk arm, and formatting tracks, as well as initializing, sensing, resetting, and recalibrating the controller and the drives.
      </para>
      <para>
	The most basic commands are <command>read</command> and <command>write</command>, each of which requires 13 parameters, packed into 9 bytes. These parameters specify such items as the address of the disk block to be read, the number of sectors per track, the recording mode used on the physical medium, the intersector gap spacing, and what to do with a deleted-data-address-mark. If you do not understand this mumbo jumbo, do not worry; that is precisely the pointit is rather esoteric. When the operation is completed, the controller chip returns 23 status and error fields packed into 7 bytes. As if this were not enough, the floppy disk programmer must also be constantly aware of whether the motor is on or off. If the motor is off, it must be turned on (with a long startup delay) before data can be read or written. The motor cannot be left on too long, however, or the floppy disk will wear out. The programmer is thus forced to deal with the trade-off between long startup delays versus wearing out floppy disks (and losing the data on them).
      </para>
      <para>
	Without going into the real details, it should be clear that the average programmer probably does not want to get too intimately involved with the programming of floppy disks (or hard disks, which are just as complex and quite different). Instead, what the programmer wants is a simple, high-level abstraction to deal with. In the case of disks, a typical abstraction would be that the disk contains a collection of named files. Each file can be opened for reading or writing, then read or written, and finally closed. Details such as whether or not recording should use modified frequency modulation and what the current state of the motor is should not appear in the abstraction presented to the user.
      </para>
      <para>
	The program that hides the truth about the hardware from the programmer and presents a nice, simple view of named files that can be read and written is, of course, the operating system. Just as the operating system shields the programmer from the disk hardware and presents a simple file-oriented interface, it also conceals a lot of unpleasant business concerning interrupts, timers, memory management, and other low-level features. In each case, the abstraction offered by the operating system is simpler and easier to use than that offered by the underlying hardware.
      </para>
      <para>
	In this view, the function of the operating system is to present the user with the equivalent of an <command><emphasis>extended machine</emphasis></command> or <command><emphasis>virtual machine</emphasis></command> that is easier to program than the underlying hardware. How the operating system achieves this goal is a long story, which we will study in detail throughout this book. To summarize it in a nutshell, the operating system provides a variety of services that programs can obtain using special instructions called system calls. We will examine some of the more common system calls later in this chapter.
      </para>
    </sect2>
    <sect2 id="sect-1.1.2">
      <title>1.1.2. The Operating System as a Resource Manager</title>
      <para>
	The concept of the operating system as primarily providing its users with a convenient interface is a top-down view. An alternative, bottom-up, view holds that the operating system is there to manage all the pieces of a complex system. Modern computers consist of processors, memories, timers, disks, mice, network interfaces, printers, and a wide variety of other devices. In the alternative view, the job of the operating system is to provide for an orderly and controlled allocation of the processors, memories, and I/O devices among the various programs competing for them.
      </para>
      <para>
	Imagine what would happen if three programs running on some computer all tried to print their output simultaneously on the same printer. The first few lines of printout might be from program 1, the next few from program 2, then some from program 3, and so forth. The result would be chaos. The operating system can bring order to the potential chaos by buffering all the output destined for the printer on the disk. When one program is finished, the operating system can then copy its output from the disk file where it has been stored to the printer, while at the same time the other program can continue generating more output, oblivious to the fact that the output is not really going to the printer (yet).
      </para>
      <para>
      When a computer (or network) has multiple users, the need for managing and protecting the memory, I/O devices, and other resources is even greater, since the users might otherwise interfere with one another. In addition, users often need to share not only hardware, but information (files, databases, etc.) as well. In short, this view of the operating system holds that its primary task is to keep track of who is using which resource, to grant resource requests, to account for usage, and to mediate conflicting requests from different programs and users.q
      </para>
      <para>
	Resource management includes multiplexing (sharing) resources in two ways: in time and in space. When a resource is time multiplexed, different programs or users take turns using it. First one of them gets to use the resource, then another, and so on. For example, with only one CPU and multiple programs that want to run on it, the operating system first allocates the CPU to one program, then after it has run long enough, another one gets to use the CPU, then another, and then eventually the first one again. Determining how the resource is time multiplexedwho goes next and for how longis the task of the operating system. Another example of time multiplexing is sharing the printer. When multiple print jobs are queued up for printing on a single printer, a decision has to be made about which one is to be printed next.
      </para>
      <para>
	The other kind of multiplexing is space multiplexing. Instead of the customers taking turns, each one gets part of the resource. For example, main memory is normally divided up among several running programs, so each one can be resident at the same time (for example, in order to take turns using the CPU). Assuming there is enough memory to hold multiple programs, it is more efficient to hold several programs in memory at once rather than give one of them all of it, especially if it only needs a small fraction of the total. Of course, this raises issues of fairness, protection, and so on, and it is up to the operating system to solve them. Another resource that is space multiplexed is the (hard) disk. In many systems a single disk can hold files from many users at the same time. Allocating disk space and keeping track of who is using which disk blocks is a typical operating system resource management task.
      </para>
    </sect2>
  </sect1>

  <sect1 id="sect-1.2">
    <title>1.2. History of Operating Systems</title>
    <para>
      Operating systems have been evolving through the years. In the following sections we will briefly look at a few of the highlights. Since operating systems have historically been closely tied to the architecture of the computers on which they run, we will look at successive generations of computers to see what their operating systems were like. This mapping of operating system generations to computer generations is crude, but it does provide some structure where there would otherwise be none.
    </para>
    <para>
      The first true digital computer was designed by the English mathematician Charles Babbage (17921871). Although Babbage spent most of his life and fortune trying to build his "analytical engine," he never got it working properly because it was purely mechanical, and the technology of his day could not produce the required wheels, gears, and cogs to the high precision that he needed. Needless to say, the analytical engine did not have an operating system.
    </para>
    <para>
      As an interesting historical aside, Babbage realized that he would need software for his analytical engine, so he hired a young woman named Ada Lovelace, who was the daughter of the famed British poet Lord Byron, as the world's first programmer. The programming language Ada® was named after her.
    </para>

    <sect2 id="sect-1.2.1">
      <title>1.2.1. The First Generation (194555) Vacuum Tubes and Plugboards</title>
      <para>
	After Babbage's unsuccessful efforts, little progress was made in constructing digital computers until World War II. Around the mid-1940s, Howard Aiken at Harvard University, John von Neumann at the Institute for Advanced Study in Princeton, J. Presper Eckert and John Mauchley at the University of Pennsylvania, and Konrad Zuse in Germany, among others, all succeeded in building calculating engines. The first ones used mechanical relays but were very slow, with cycle times measured in seconds. Relays were later replaced by vacuum tubes. These machines were enormous, filling up entire rooms with tens of thousands of vacuum tubes, but they were still millions of times slower than even the cheapest personal computers available today.
      </para>
      <para>
	In these early days, a single group of people designed, built, programmed, operated, and maintained each machine. All programming was done in absolute machine language, often by wiring up plugboards to control the machine's basic functions. Programming languages were unknown (even assembly language was unknown). Operating systems were unheard of. The usual mode of operation was for the programmer to sign up for a block of time on the signup sheet on the wall, then come down to the machine room, insert his or her plugboard into the computer, and spend the next few hours hoping that none of the 20,000 or so vacuum tubes would burn out during the run. Virtually all the problems were straightforward numerical calculations, such as grinding out tables of sines, cosines, and logarithms.
      </para>
      <para>
	By the early 1950s, the routine had improved somewhat with the introduction of punched cards. It was now possible to write programs on cards and read them in instead of using plugboards; otherwise, the procedure was the same.
      </para>
    </sect2>

    <sect2 id="sect-1.2.2">
      <title>1.2.2. The Second Generation (195565) Transistors and Batch Systems</title>
      <para>
	The introduction of the transistor in the mid-1950s changed the picture radically. Computers became reliable enough that they could be manufactured and sold to paying customers with the expectation that they would continue to function long enough to get some useful work done. For the first time, there was a clear separation between designers, builders, operators, programmers, and maintenance personnel.
      </para>
      <para>
	These machines, now called <command><emphasis>mainframes</emphasis></command>, were locked away in specially airconditioned computer rooms, with staffs of specially-trained professional operators to run them. Only big corporations or major government agencies or universities could afford their multimillion dollar price tags. To run a <command><emphasis>job</emphasis></command> (i.e., a program or set of programs), a programmer would first write the program on paper (in FORTRAN or possibly even in assembly language), then punch it on cards. He would then bring the card deck down to the input room and hand it to one of the operators and go drink coffee until the output was ready.
      </para>
      <para>
	When the computer finished whatever job it was currently running, an operator would go over to the printer and tear off the output and carry it over to the output-room, so that the programmer could collect it later. Then he would take one of the card decks that had been brought from the input room and read it in. If the FORTRAN compiler was needed, the operator would have to get it from a file cabinet and read it in. Much computer time was wasted while operators were walking around the machine room.
      </para>
      <para>
	Given the high cost of the equipment, it is not surprising that people quickly looked for ways to reduce the wasted time. The solution generally adopted was the <command><emphasis>batch system</emphasis></command>. The idea behind it was to collect a tray full of jobs in the input room and then read them onto a magnetic tape using a small (relatively) inexpensive computer, such as the IBM 1401, which was very good at reading cards, copying tapes, and printing output, but not at all good at numerical calculations. Other, much more expensive machines, such as the IBM 7094, were used for the real computing. This situation is shown in <tag><link linkend="1-2">Fig. 1-2</link></tag>.
      </para>

      <para id="1-2"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/1-2.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 1-2. An early batch system. (a) Programmers bring cards to 1401. (b) 1401 reads batch of jobs onto tape. (c) Operator carries input tape to 7094. (d) 7094 does computing. (e) Operator carries output tape to 1401. (f) 1401 prints output.</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	After about an hour of collecting a batch of jobs, the tape was rewound and brought into the machine room, where it was mounted on a tape drive. The operator then loaded a special program (the ancestor of today's operating system), which read the first job from tape and ran it. The output was written onto a second tape, instead of being printed. After each job finished, the operating system automatically read the next job from the tape and began running it. When the whole batch was done, the operator removed the input and output tapes, replaced the input tape with the next batch, and brought the output tape to a 1401 for printing <command><emphasis>off line</emphasis></command> (i.e., not connected to the main computer).
      </para>
      <para>
	The structure of a typical input job is shown in <tag><link linkend="1-3">Fig. 1-3</link></tag>. It started out with a $JOB card, specifying the maximum run time in minutes, the account number to be charged, and the programmer's name. Then came a $FORTRAN card, telling the operating system to load the FORTRAN compiler from the system tape. It was followed by the program to be compiled, and then a $LOAD card, directing the operating system to load the object program just compiled. (Compiled programs were often written on scratch tapes and had to be loaded explicitly.) Next came the $RUN card, telling the operating system to run the program with the data following it. Finally, the $END card marked the end of the job. These primitive control cards were the forerunners of modern job control languages and command interpreters.
      </para>

      <para id="1-3"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/1-3.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 1-3. Structure of a typical FMS job.</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	Large second-generation computers were used mostly for scientific and engineering calculations, such as solving the partial differential equations that often occur in physics and engineering. They were largely programmed in FORTRAN and assembly language. Typical operating systems were FMS (the Fortran Monitor System) and IBSYS, IBM's operating system for the 7094.
      </para>
    </sect2>

    <sect2 id="sect-1.2.3">
      <title>1.2.3. The Third Generation (19651980) ICs and Multiprogramming</title>
      <para>
	By the early 1960s, most computer manufacturers had two distinct, and totally incompatible, product lines. On the one hand there were the word-oriented, large-scale scientific computers, such as the 7094, which were used for numerical calculations in science and engineering. On the other hand, there were the character-oriented, commercial computers, such as the 1401, which were widely used for tape sorting and printing by banks and insurance companies.
      </para>
      <para>
	Developing, maintaining, and marketing two completely different product lines was an expensive proposition for the computer manufacturers. In addition, many new computer customers initially needed a small machine but later outgrew it and wanted a bigger machine that had the same architectures as their current one so it could run all their old programs, but faster.
      </para>
      <para>
	IBM attempted to solve both of these problems at a single stroke by introducing the System/360. The 360 was a series of software-compatible machines ranging from 1401-sized to much more powerful than the 7094. The machines differed only in price and performance (maximum memory, processor speed, number of I/O devices permitted, and so forth). Since all the machines had the same architecture and instruction set, programs written for one machine could run on all the others, at least in theory. Furthermore, the 360 was designed to handle both scientific (i.e., numerical) and commercial computing. Thus a single family of machines could satisfy the needs of all customers. In subsequent years, IBM has come out with compatible successors to the 360 line, using more modern technology, known as the 370, 4300, 3080, 3090, and Z series.
      </para>
      <para>
	The 360 was the first major computer line to use (small-scale) Integrated Circuits (ICs), thus providing a major price/performance advantage over the second-generation machines, which were built up from individual transistors. It was an immediate success, and the idea of a family of compatible computers was soon adopted by all the other major manufacturers. The descendants of these machines are still in use at computer centers today. Nowadays they are often used for managing huge databases (e.g., for airline reservation systems) or as servers for World Wide Web sites that must process thousands of requests per second.
      </para>
      <para>
	The greatest strength of the "one family" idea was simultaneously its greatest weakness. The intention was that all software, including the operating system, <command><emphasis>OS/360</emphasis></command>, had to work on all models. It had to run on small systems, which often just replaced 1401s for copying cards to tape, and on very large systems, which often replaced 7094s for doing weather forecasting and other heavy computing. It had to be good on systems with few peripherals and on systems with many peripherals. It had to work in commercial environments and in scientific environments. Above all, it had to be efficient for all of these different uses.
      </para>
      <para>
	There was no way that IBM (or anybody else) could write a piece of software to meet all those conflicting requirements. The result was an enormous and extraordinarily complex operating system, probably two to three orders of magnitude larger than FMS. It consisted of millions of lines of assembly language written by thousands of programmers, and contained thousands upon thousands of bugs, which necessitated a continuous stream of new releases in an attempt to correct them. Each new release fixed some bugs and introduced new ones, so the number of bugs probably remained constant in time.
      </para>
      <para>
	One of the designers of OS/360, Fred Brooks, subsequently wrote a witty and incisive book describing his experiences with OS/360 (Brooks, 1995). While it would be impossible to summarize the book here, suffice it to say that the cover shows a herd of prehistoric beasts stuck in a tar pit. The cover of Silberschatz et al. (2004) makes a similar point about operating systems being dinosaurs.
      </para>
      <para>
	Despite its enormous size and problems, OS/360 and the similar third-generation operating systems produced by other computer manufacturers actually satisfied most of their customers reasonably well. They also popularized several key techniques absent in second-generation operating systems. Probably the most important of these was <command><emphasis>multiprogramming</emphasis></command>. On the 7094, when the current job paused to wait for a tape or other I/O operation to complete, the CPU simply sat idle until the I/O finished. With heavily CPU-bound scientific calculations, I/O is infrequent, so this wasted time is not significant. With commercial data processing, the I/O wait time can often be 80 or 90 percent of the total time, so something had to be done to avoid having the (expensive) CPU be idle so much.
      </para>
      <para>
	The solution that evolved was to partition memory into several pieces, with a different job in each partition, as shown in <tag><link linkend="1-4">Fig. 1-4</link></tag>. While one job was waiting for I/O to complete, another job could be using the CPU. If enough jobs could be held in main memory at once, the CPU could be kept busy nearly 100 percent of the time. Having multiple jobs safely in memory at once requires special hardware to protect each job against snooping and mischief by the other ones, but the 360 and other third-generation systems were equipped with this hardware.
      </para>

      <para id="1-4"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/1-4.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 1-4. A multiprogramming system with three jobs in memory.</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	Another major feature present in third-generation operating systems was the ability to read jobs from cards onto the disk as soon as they were brought to the computer room. Then, whenever a running job finished, the operating system could load a new job from the disk into the now-empty partition and run it. This technique is called <command><emphasis>spooling</emphasis></command>; (from Simultaneous Peripheral Operation On Line) and was also used for output. With spooling, the 1401s were no longer needed, and much carrying of tapes disappeared.
      </para>
      <para>
      Although third-generation operating systems were well suited for big scientific calculations and massive commercial data processing runs, they were still basically batch systems. Many programmers pined for the first-generation days when they had the machine all to themselves for a few hours, so they could debug their programs quickly. With third-generation systems, the time between submitting a job and getting back the output was often hours, so a single misplaced comma could cause a compilation to fail, and the programmer to waste half a day.
      </para>
      <para>
	This desire for quick response time paved the way for <command><emphasis>timesharing</emphasis></command>, a variant of multiprogramming, in which each user has an online terminal. In a timesharing system, if 20 users are logged in and 17 of them are thinking or talking or drinking coffee, the CPU can be allocated in turn to the three jobs that want service. Since people debugging programs usually issue short commands (e.g., compile a five-page procedure[]) rather than long ones (e.g., sort a million-record file), the computer can provide fast, interactive service to a number of users and perhaps also work on big batch jobs in the background when the CPU is otherwise idle. The first serious timesharing system, <command><emphasis>CTSS</emphasis></command> (Compatible Time Sharing System), was developed at M.I.T. on a specially modified 7094 (Corbató et al., 1962). However, timesharing did not really become popular until the necessary protection hardware became widespread during the third generation.
      </para>

      <para/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/u2020.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      We will use the terms "procedure," "subroutine," and "function" interchangeably in this book.
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	After the success of the CTSS system, MIT, Bell Labs, and General Electric (then a major computer manufacturer) decided to embark on the development of a "computer utility," a machine that would support hundreds of simultaneous timesharing users. Their model was the electricity distribution systemwhen you need electric power, you just stick a plug in the wall, and within reason, as much power as you need will be there. The designers of this system, known as <command><emphasis>MULTICS</emphasis></command> (MULTiplexed Information and Computing Service), envisioned one huge machine providing computing power for everyone in the Boston area. The idea that machines far more powerful than their GE-645 mainframe would be sold for under a thousand dollars by the millions only 30 years later was pure science fiction, like the idea of supersonic trans-Atlantic underse a trains would be now.
      </para>
      <para>
	MULTICS was a mixed success. It was designed to support hundreds of users on a machine only slightly more powerful than an Intel 80386-based PC, although it had much more I/O capacity. This is not quite as crazy as it sounds, since people knew how to write small, efficient programs in those days, a skill that has subsequently been lost. There were many reasons that MULTICS did not take over the world, not the least of which is that it was written in PL/I, and the PL/I compiler was years late and barely worked at all when it finally arrived. In addition, MULTICS was enormously ambitious for its time, much like Charles Babbage's analytical engine in the nineteenth century.
      </para>
      <para>
	MULTICS introduced many seminal ideas into the computer literature, but turning it into a serious product and a commercial success was a lot harder than anyone had expected. Bell Labs dropped out of the project, and General Electric quit the computer business altogether. However, M.I.T. persisted and eventually got MULTICS working. It was ultimately sold as a commercial product by the company that bought GE's computer business (Honeywell) and installed by about 80 major companies and universities worldwide. While their numbers were small, MULTICS users were fiercely loyal. General Motors, Ford, and the U.S. National Security Agency, for example, only shut down their MULTICS systems in the late 1990s. The last MULTICS running, at the Canadian Department of National Defence, shut down in October 2000. Despite its lack of commercial success, MULTICS had a huge influence on subsequent operating systems. A great deal of information about it exists (Corbató et al., 1972; Corbató and Vyssotsky, 1965; Daley and Dennis, 1968; Organick, 1972; and Saltzer, 1974). It also has a stillactive Web site, <tag><link xl:href="www.multicians.org">www.multicians.org</link></tag>, with a great deal of information about the system, its designers, and its users.
      </para>
      <para>
	The phrase "computer utility" is no longer heard, but the idea has gained new life in recent years. In its simplest form, PCs or <command><emphasis>workstations</emphasis></command> (high-end PCs) in a business or a classroom may be connected via a <command><emphasis>LAN</emphasis></command> (<command><emphasis>Local Area Network</emphasis></command>) to a <command><emphasis>file server</emphasis></command> on which all programs and data are stored. An administrator then has to install and protect only one set of programs and data, and can easily reinstall local software on a malfunctioning PC or workstation without worrying about retrieving or preserving local data. In more heterogeneous environments, a class of software called <command><emphasis>middleware</emphasis></command> has evolved to bridge the gap between local users and the files, programs, and databases they use on remote servers. Middleware makes networked computers look local to individual users' PCs or workstations and presents a consistent user interface even though there may be a wide variety of different servers, PCs, and workstations in use. The World Wide Web is an example. A web browser presents documents to a user in a uniform way, and a document as seen on a user's browser can consist of text from one server and graphics from another server, presented in a format determined by a style sheet on yet another server. Businesses and universities commonly use a web interface to access databases and run programs on a computer in another building or even another city. Middleware appears to be the operating system of a <command><emphasis>distributed system</emphasis></command>, but it is not really an operating system at all, and is beyond the scope of this book. For more on distributed systems see Tanenbaum and Van Steen (2002).
      </para>
      <para>
	Another major development during the third generation was the phenomenal growth of minicomputers, starting with the Digital Equipment Company (DEC) PDP-1 in 1961. The PDP-1 had only 4K of 18-bit words, but at $120,000 per machine (less than 5 percent of the price of a 7094), it sold like hotcakes. For certain kinds of nonnumerical work, it was almost as fast as the 7094 and gave birth to a whole new industry. It was quickly followed by a series of other PDPs (unlike IBM's family, all incompatible) culminating in the PDP-11.
      </para>
      <para>
	One of the computer scientists at Bell Labs who had worked on the MULTICS project, Ken Thompson, subsequently found a small PDP-7 minicomputer that no one was using and set out to write a stripped-down, one-user version of MULTICS. This work later developed into the <command><emphasis>UNIX</emphasis></command> operating system, which became popular in the academic world, with government agencies, and with many companies.
      </para>
      <para>
	The history of UNIX has been told elsewhere (e.g., Salus, 1994). Because the source code was widely available, various organizations developed their own (incompatible) versions, which led to chaos. Two major versions developed, <command><emphasis>System V</emphasis></command>, from AT&amp;T, and <command><emphasis>BSD</emphasis></command>, (Berkeley Software Distribution) from the University of California at Berkeley. These had minor variants as well, now including FreeBSD, OpenBSD, and NetBSD. To make it possible to write programs that could run on any UNIX system, IEEE developed a standard for UNIX, called <command><emphasis>POSIX</emphasis></command>, that most versions of UNIX now support. POSIX defines a minimal system call interface that conformant UNIX systems must support. In fact, some other operating systems now also support the POSIX interface. The information needed to write POSIX-compliant software is available in books (IEEE, 1990; Lewine, 1991), and online as the Open Group's "Single UNIX Specification" at <tag><link xl:href="www.unix.org">www.unix.org</link></tag>. Later in this chapter, when we refer to UNIX, we mean all of these systems as well, unless stated otherwise. While they differ internally, all of them support the POSI X standard, so to the programmer they are quite similar.
      </para>
    </sect2>

    <sect2 id="sect-1.2.4">
      <title>1.2.4. The Fourth Generation (1980Present) Personal Computers</title>
      <para>
	With the development of LSI (Large Scale Integration) circuits, chips containing thousands of transistors on a square centimeter of silicon, the age of the <command><emphasis>microprocessor</emphasis></command>-based personal computer dawned. In terms of architecture, personal computers (initially called <command><emphasis>microcomputers</emphasis></command>) were not all that different from minicomputers of the PDP-11 class, but in terms of price they certainly were different. The minicomputer made it possible for a department in a company or university to have its own computer. The microcomputer made it possible for an individual to have his or her own computer.
      </para>
      <para>
	There were several families of microcomputers. Intel came out with the 8080, the first general-purpose 8-bit microprocessor, in 1974. A number of companies produced complete systems using the 8080 (or the compatible Zilog Z80) and the <command><emphasis>CP/M</emphasis></command> (Control Program for Microcomputers) operating system from a company called Digital Research was widely used with these. Many application programs were written to run on CP/M, and it dominated the personal computing world for about 5 years.
      </para>
      <para>
	Motorola also produced an 8-bit microprocessor, the 6800. A group of Motorola engineers left to form MOS Technology and manufacture the 6502 CPU after Motorola rejected their suggested improvements to the 6800. The 6502 was the CPU of several early systems. One of these, the Apple II, became a major competitor for CP/M systems in the home and educational markets. But CP/M was so popular that many owners of Apple II computers purchased Z-80 coprocessor add-on cards to run CP/M, since the 6502 CPU was not compatible with CP/M. The CP/M cards were sold by a little company called Microsoft, which also had a market niche supplying BASIC interpreters used by a number of microcomputers running CP/M.
      </para>
      <para>
	The next generation of microprocessors were 16-bit systems. Intel came out with the 8086, and in the early 1980s, IBM designed the IBM PC around Intel's 8088 (an 8086 on the inside, with an 8 bit external data path). Microsoft offered IBM a package which included Microsoft's BASIC and an operating system, <command><emphasis>DOS</emphasis></command> (Disk Operating System) originally developed by another companyMicrosoft bought the product and hired the original author to improve it. The revised system was renamed <command><emphasis>MS-DOS</emphasis></command> (MicroSoft Disk Operating System) and quickly came to dominate the IBM PC market.
      </para>
      <para>
	CP/M, MS-DOS, and the Apple DOS were all command-line systems: users typed commands at the keyboard. Years earlier, Doug Engelbart at Stanford Research Institute had invented the <command><emphasis>GUI</emphasis></command> (<command><emphasis>Graphical User Interface</emphasis></command>), pronounced "gooey," complete with windows, icons, menus, and mouse. Apple's Steve Jobs saw the possibility of a truly <command><emphasis>user-friendly</emphasis></command> personal computer (for users who knew nothing about computers and did not want to learn), and the Apple Macintosh was announced in early 1984. It used Motorola's 16-bit 68000 CPU, and had 64 KB of <command><emphasis>ROM</emphasis></command> (<command><emphasis>Read Only Memory</emphasis></command>), to support the GUI. The Macintosh has evolved over the years. Subsequent Motorola CPUs were true 32-bit systems, and later still Apple moved to IBM PowerPC CPUs, with RISC 32-bit (and later, 64-bit) architecture. In 2001 Apple made a major operating system change, releasing <command><emphasis>Mac OS X</emphasis></command>, with a new version of the Macintosh GUI on top of Berkeley UNIX. And in 2005 Apple announced that it would be switching to Intel processors.
      </para>
      <para>
	To compete with the Macintosh, Microsoft invented Windows. Originally Windows was just a graphical environment on top of 16-bit MS-DOS (i.e., it was more like a shell than a true operating system). However, current versions of Windows are descendants of Windows NT, a full 32-bit system, rewritten from scratch.
      </para>
      <para>
	The other major contender in the personal computer world is UNIX (and its various derivatives). UNIX is strongest on workstations and other high-end computers, such as network servers. It is especially popular on machines powered by high-performance RISC chips. On Pentium-based computers, Linux is becoming a popular alternative to Windows for students and increasingly many corporate users. (Throughout this book we will use the term "Pentium" to mean the entire Pentium family, including the low-end Celeron, the high end Xeon, and compatible AMD microprocessors).
      </para>
      <para>
	Although many UNIX users, especially experienced programmers, prefer a command-based interface to a GUI, nearly all UNIX systems support a windowing system called the <command><emphasis>X Window</emphasis></command> system developed at M.I.T. This system handles the basic window management, allowing users to create, delete, move, and resize windows using a mouse. Often a complete GUI, such as <command><emphasis>Motif</emphasis></command>, is available to run on top of the X Window system giving UNIX a look and feel something like the Macintosh or Microsoft Windows for those UNIX users who want such a thing.
      </para>
      <para>
	An interesting development that began taking place during the mid-1980s is the growth of networks of personal computers running <command><emphasis>network operating systems</emphasis></command> and <command><emphasis>distributed operating systems</emphasis></command> (Tanenbaum and Van Steen, 2002). In a network operating system, the users are aware of the existence of multiple computers and can log in to remote machines and copy files from one machine to another. Each machine runs its own local operating system and has its own local user (or users). Basically, the machines are independent of one another.
      </para>
      <para>
	Network operating systems are not fundamentally different from single-processor operating systems. They obviously need a network interface controller and some low-level software to drive it, as well as programs to achieve remote login and remote file access, but these additions do not change the essential structure of the operating system.
      </para>
      <para>
	A distributed operating system, in contrast, is one that appears to its users as a traditional uniprocessor system, even though it is actually composed of multiple processors. The users should not be aware of where their programs are being run or where their files are located; that should all be handled automatically and efficiently by the operating system.
      </para>
      <para>
	True distributed operating systems require more than just adding a little code to a uniprocessor operating system, because distributed and centralized systems differ in critical ways. Distributed systems, for example, often allow applications to run on several processors at the same time, thus requiring more complex processor scheduling algorithms in order to optimize the amount of parallelism.
      </para>
      <para>
	Communication delays within the network often mean that these (and other) algorithms must run with incomplete, outdated, or even incorrect information. This situation is radically different from a single-processor system in which the operating system has complete information about the system state.
      </para>
    </sect2>

    <sect2 id="sect-1.2.5">
      <title>1.2.5. History of MINIX 3</title>
      <para>
	When UNIX was young (Version 6), the source code was widely available, under AT&amp;T license, and frequently studied. John Lions, of the University of New South Wales in Australia, even wrote a little booklet describing its operation, line by line (Lions, 1996). This booklet was used (with permission of AT&amp;T) as a text in many university operating system courses.
      </para>
      <para>
	When AT&amp;T released Version 7, it dimly began to realize that UNIX was a valuable commercial product, so it issued Version 7 with a license that prohibited the source code from being studied in courses, in order to avoid endangering its status as a trade secret. Many universities complied by simply dropping the study of UNIX and teaching only theory.
      </para>
      <para>
	Unfortunately, teaching only theory leaves the student with a lopsided view of what an operating system is really like. The theoretical topics that are usually covered in great detail in courses and books on operating systems, such as scheduling algorithms, are in practice not really that important. Subjects that really are important, such as I/O and file systems, are generally neglected because there is little theory about them.
      </para>
      <para>
	To remedy this situation, one of the authors of this book (Tanenbaum) decided to write a new operating system from scratch that would be compatible with UNIX from the user's point of view, but completely different on the inside. By not using even one line of AT&amp;T code, this system avoided the licensing restrictions, so it could be used for class or individual study. In this manner, readers could dissect a real operating system to see what is inside, just as biology students dissect frogs. It was called MINIX and was released in 1987 with its complete source code for anyone to study or modify. The name <command><emphasis>MINIX</emphasis></command> stands for mini-UNIX because it is small enough that even a nonguru can understand how it works.
      </para>
      <para>
	In addition to the advantage of eliminating the legal problems, MINIX had another advantage over UNIX. It was written a decade after UNIX and was structured in a more modular way. For instance, from the very first release of MINIX the file system and the memory manager were not part of the operating system at all but ran as user programs. In the current release (MINIX 3) this modularization has been extended to the I/O device drivers, which (with the exception of the clock driver) all run as user programs. Another difference is that UNIX was designed to be efficient; MINIX was designed to be readable (inasmuch as one can speak of any program hundreds of pages long as being readable). The MINIX code, for example, has thousands of comments in it.
      </para>
      <para>
	MINIX was originally designed for compatibility with Version 7 (V7) UNIX. Version 7 was used as the model because of its simplicity and elegance. It is sometimes said that Version 7 was an improvement not only over all its predecessors, but also over all its successors. With the advent of POSIX, MINIX began evolving toward the new standard, while maintaining backward compatibility with existing programs. This kind of evolution is common in the computer industry, as no vendor wants to introduce a new system that none of its existing customers can use without great upheaval. The version of MINIX described in this book, MINIX 3, is based on the POSIX standard.
      </para>
      <para>
	Like UNIX, MINIX was written in the C programming language and was intended to be easy to port to various computers. The initial implementation was for the IBM PC. MINIX was subsequently ported to several other platforms. In keeping with the "Small is Beautiful" philosophy, MINIX originally did not even require a hard disk to run (in the mid-1980s hard disks were still an expensive novelty). As MINIX grew in functionality and size, it eventually got to the point that a hard disk was needed for PCs, but in keeping with the MINIX philosophy, a 200-MB partition is sufficient (for embedded applications, no hard disk is required though). In contrast, even small Linux systems require 500-MB of disk space, and several GB will be needed to install common applications.
      </para>
      <para>
	To the average user sitting at an IBM PC, running MINIX is similar to running UNIX. All of the basic programs, such as cat, grep, ls, make, and the shell are present and perform the same functions as their UNIX counterparts. Like the operating system itself, all these utility programs have been rewritten completely from scratch by the author, his students, and some other dedicated people, with no AT&amp;T or other proprietary code. Many other freely-distributable programs now exist, and in many cases these have been successfully ported (recompiled) on MINIX.
      </para>
      <para>
	MINIX continued to develop for a decade and MINIX 2 was released in 1997, together with the second edition of this book, which described the new release. The changes between versions 1 and 2 were substantial (e.g., from 16-bit real mode on an 8088 using floppy disks to 32-bit protected mode on a 386 using a hard disk) but evolutionary.
      </para>
      <para>
	Development continued slowly but systematically until 2004, when Tanenbaum became convinced that software was getting too bloated and unreliable and decided to pick up the slightly-dormant MINIX thread again. Together with his students and programmers at the Vrije Universiteit in Amsterdam, he produced MINIX 3, a major redesign of the system, greatly restructuring the kernel, reducing its size, and emphasizing modularity and reliability. The new version was intended both for PCs and embedded systems, where compactness, modularity, and reliability are crucial. While some people in the group called for a completely new name, it was eventually decided to call it MINIX 3 since the name MINIX was already well known. By way of analogy, when Apple abandoned it own operating system, Mac OS 9 and replaced it with a variant of Berkeley UNIX, the name chosen was Mac OS X rather than APPLIX or something like that. Similar fundamental changes have happened in the Windows family while retaining the Windows name.
      </para>
      <para>
	The MINIX 3 kernel is well under 4000 lines of executable code, compared to millions of executable lines of code for Windows, Linux, FreeBSD, and other operating systems. Small kernel size is important because kernel bugs are far more devastating than bugs in user-mode programs and more code means more bugs. One careful study has shown that the number of detected bugs per 1000 executable lines of code varies from 6 to 16 (Basili and Perricone, 1984). The actual number of bugs is probably much higher since the researchers could only count reported bugs, not unreported bugs. Yet another study (Ostrand et al., 2004) showed that even after more than a dozen releases, on the average 6% of all files contained bugs that were later reported and after a certain point the bug level tends to stabilize rather than go asymptotically to zero. This result is supported by the fact that when a very simple, automated, model-checker was let loose on stable versions of Linux and OpenBSD, it found hundreds of kernel bugs, overwhelmingly in device drivers (Chou et al., 2001; and Engler et al., 2001). This is the reason the device drivers were moved out of the kernel in MINIX 3; they can do less damage in user mode.
      </para>
      <para>
	Throughout this book MINIX 3 will be used as an example. Most of the comments about the MINIX 3 system calls, however (as opposed to comments about the actual code), also apply to other UNIX systems. This remark should be kept in mind when reading the text.
      </para>
      <para>
	A few words about Linux and its relationship to MINIX may possibly be of interest to some readers. Shortly after MINIX was released, a USENET newsgroup, comp.os.minix, was formed to discuss it. Within weeks, it had 40,000 subscribers, most of whom wanted to add vast numbers of new features to MINIX to make it bigger and better (well, at least bigger). Every day, several hundred of them offered suggestions, ideas, and frequently snippets of source code. The author of MINIX was able to successfully resist this onslaught for several years, in order to keep MINIX clean enough for students to understand and small enough that it could run on computers that students could afford. For people who thought little of MS-DOS, the existence of MINIX (with source code) as an alternative was even a reason to finally go out and buy a PC.
      </para>
      <para>
	One of these people was a Finnish student named Linus Torvalds. Torvalds installed MINIX on his new PC and studied the source code carefully. Torvalds wanted to read USENET newsgroups (such as comp.os.minix) on his own PC rather than at his university, but some features he needed were lacking in MINIX, so he wrote a program to do that, but soon discovered he needed a different terminal driver, so he wrote that too. Then he wanted to download and save postings, so he wrote a disk driver, and then a file system. By Aug. 1991 he had produced a primitive kernel. On Aug. 25, 1991, he announced it on comp.os.minix. This announcement attracted other people to help him, and on March 13, 1994 Linux 1.0 was released. Thus was Linux born.
      </para>
      <para>
	Linux has become one of the notable successes of the <command><emphasis>open source</emphasis></command> movement (which MINIX helped start). Linux is challenging UNIX (and Windows) in many environments, partly because commodity PCs which support Linux are now available with performance that rivals the proprietary RISC systems required by some UNIX implementations. Other open source software, notably the Apache web server and the MySQL database, work well with Linux in the commercial world. Linux, Apache, MySQL, and the open source Perl and PHP programming languages are often used together on web servers and are sometimes referred to by the acronym <command><emphasis>LAMP</emphasis></command>. For more on the history of Linux and open source software see DiBona et al. (1999), Moody (2001), and Naughton (2000).
      </para>
    </sect2>
  </sect1>

  <sect1 id="sect-1.3">
    <title>1.3. Operating System Concepts</title>
    <para>
      The interface between the operating system and the user programs is defined by the set of "extended instructions" that the operating system provides. These extended instructions have been traditionally known as <command><emphasis>system calls</emphasis></command>, although they can be implemented in several ways. To really understand what operating systems do, we must examine this interface closely. The calls available in the interface vary from operating system to operating system (although the underlying concepts tend to be similar).
    </para>
    <para>
      We are thus forced to make a choice between (1) vague generalities ("operating systems have system calls for reading files") and (2) some specific system ("MINIX 3 has a read system call with three parameters: one to specify the file, one to tell where the data are to be put, and one to tell how many bytes to read").
    </para>
    <para>
      We have chosen the latter approach. It's more work that way, but it gives more insight into what operating systems really do. In <tag><link linkend="sect-1.4">Sec. 1.4</link></tag> we will look closely at the basic system calls present in UNIX (including the various versions of BSD), Linux, and MINIX 3. For simplicity's sake, we will refer only to MINI 3, but the corresponding UNIX and Linux system calls are based on POSIX in most cases. Before we look at the actual system calls, however, it is worth taking a bird's-eye view of MINIX 3, to get a general feel for what an operating system is all about. This overview applies equally well to UNIX and Linux, as mentioned above.
    </para>
    <para>
      The MINIX 3 system calls fall roughly in two broad categories: those dealing with processes and those dealing with the file system. We will now examine each of these in turn.
    </para>

    <sect2 id="sect-1.3.1">
      <title>1.3.1. Processes</title>
      <para>
	A key concept in MINIX 3, and in all operating systems, is the <command><emphasis>process</emphasis></command>. A process is basically a program in execution. Associated with each process is its <command><emphasis>address space</emphasis></command>, a list of memory locations from some minimum (usually 0) to some maximum, which the process can read and write. The address space contains the executable program, the program's data, and its stack. Also associated with each process is some set of registers, including the program counter, stack pointer, and other hardware registers, and all the other information needed to run the program.
      </para>
      <para>
	We will come back to the process concept in much more detail in <tag><xref linkend="Chapter2"/></tag>, but for the time being, the easiest way to get a good intuitive feel for a process is to think about multiprogramming systems. Periodically, the operating system decides to stop running one process and start running another, for example, because the first one has had more than its share of CPU time in the past second.
      </para>
      <para>
	When a process is suspended temporarily like this, it must later be restarted in exactly the same state it had when it was stopped. This means that all information about the process must be explicitly saved somewhere during the suspension. For example, the process may have several files open for reading at once. Associated with each of these files is a pointer giving the current position (i.e., the number of the byte or record to be read next). When a process is temporarily suspended, all these pointers must be saved so that a read call executed after the process is restarted will read the proper data. In many operating systems, all the information about each process, other than the contents of its own address space, is stored in an operating system table called the <command><emphasis>process table</emphasis></command>, which is an array (or linked list) of structures, one for each process currently in existence.
      </para>
      <para>
	Thus, a (suspended) process consists of its address space, usually called the <command><emphasis>core image</emphasis></command> (in honor of the magnetic core memories used in days of yore), and its process table entry, which contains its registers, among other things.
      </para>
      <para>
	The key process management system calls are those dealing with the creation and termination of processes. Consider a typical example. A process called the <command><emphasis>command interpreter</emphasis></command> or <command><emphasis>shell</emphasis></command> reads commands from a terminal. The user has just typed a command requesting that a program be compiled. The shell must now create a new process that will run the compiler. When that process has finished the compilation, it executes a system call to terminate itself.
      </para>
      <para>
	On Windows and other operating systems that have a GUI, (double) clicking on a desktop icon launches a program in much the same way as typing its name at the command prompt. Although we will not discuss GUIs much, they are really simple command interpreters.
      </para>
      <para>
	If a process can create one or more other processes (usually referred to as <command><emphasis>child processes</emphasis></command>) and these processes in turn can create child processes, we quickly arrive at the process tree structure of <tag><link linkend="1-5">Fig. 1-5</link></tag>. Related processes that are cooperating to get some job done often need to communicate with one another and synchronize their activities. This communication is called interprocess communication, and will be addressed in detail in <tag><xref linkend="Chapter2"/></tag>
      </para>

      <para id="1-5"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/1-5.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 1-5. A process tree. Process A created two child processes, B and C. Process B created three child processes, D, E, and F.</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	Other process system calls are available to request more memory (or release unused memory), wait for a child process to terminate, and overlay its program with a different one.
      </para>
      <para>
	Occasionally, there is a need to convey information to a running process that is not sitting around waiting for it. For example, a process that is communicating with another process on a different computer does so by sending messages to the remote process over a network. To guard against the possibility that a message or its reply is lost, the sender may request that its own operating system notify it after a specified number of seconds, so that it can retransmit the message if no acknowledgement has been received yet. After setting this timer, the program may continue doing other work.
      </para>
      <para>
	When the specified number of seconds has elapsed, the operating system sends an <command><emphasis>alarm signal</emphasis></command> to the process. The signal causes the process to temporarily suspend whatever it was doing, save its registers on the stack, and start running a special signal handling procedure, for example, to retransmit a presumably lost message. When the signal handler is done, the running process is restarted in the state it was in just before the signal. Signals are the software analog of hardware interrupts. They are generated by a variety of causes in addition to timers expiring. Many traps detected by hardware, such as executing an illegal instruction or using an invalid address, are also converted into signals to the guilty process.
      </para>
      <para>
	Each person authorized to use a MINIX 3 system is assigned a <command><emphasis>UID</emphasis></command> (User IDentification) by the system administrator. Every process started has the UID of the person who started it. A child process has the same UID as its parent. Users can be members of groups, each of which has a <command><emphasis>GID</emphasis></command> (Group IDentification).
      </para>
      <para>
	One UID, called the <command><emphasis>superuser</emphasis></command> (in UNIX), has special power and may violate many of the protection rules. In large installations, only the system administrator knows the password needed to become superuser, but many of the ordinary users (especially students) devote considerable effort to trying to find flaws in the system that allow them to become superuser without the password.
      </para>
      <para>
	We will study processes, interprocess communication, and related issues in <tag><xref linkend="Chapter2"/></tag>
      </para>
    </sect2>

    <sect2 id="sect-1.3.2">
      <title>1.3.2. Files</title>
      <para>
	The other broad category of system calls relates to the file system. As noted before, a major function of the operating system is to hide the peculiarities of the disks and other I/O devices and present the programmer with a nice, clean abstract model of device-independent files. System calls are obviously needed to create files, remove files, read files, and write files. Before a file can be read, it must be opened, and after it has been read it should be closed, so calls are provided to do these things.
      </para>
      <para>
	To provide a place to keep files, MINIX 3 has the concept of a <command><emphasis>directory</emphasis></command> as a way of grouping files together. A student, for example, might have one directory for each course he is taking (for the programs needed for that course), another directory for his electronic mail, and still another directory for his World Wide Web home page. System calls are then needed to create and remove directories. Calls are also provided to put an existing file into a directory, and to remove a file from a directory. Directory entries may be either files or other directories. This model also gives rise to a hierarchythe file systemas shown in <tag><link linkend="1-6">Fig. 1-6</link></tag>.
      </para>

      <para id="1-6"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/1-6.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 1-6. A file system for a university department.</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	The process and file hierarchies both are organized as trees, but the similarity stops there. Process hierarchies usually are not very deep (more than three levels is unusual), whereas file hierarchies are commonly four, five, or even more levels deep. Process hierarchies are typically short-lived, generally a few minutes at most, whereas the directory hierarchy may exist for years. Ownership and protection also differ for processes and files. Typically, only a parent process may control or even access a child process, but mechanisms nearly always exist to allow files and directories to be read by a wider group than just the owner.
      </para>
      <para>
	Every file within the directory hierarchy can be specified by giving its <command><emphasis>path name</emphasis></command> from the top of the directory hierarchy, the <command><emphasis>root directory</emphasis></command>. Such absolute path names consist of the list of directories that must be traversed from the root directory to get to the file, with slashes separating the components. In <tag><link linkend="1-6">Fig. 1-6</link></tag>, the path for file <emphasis>CS101</emphasis> is <emphasis>/Faculty/Prof.Brown/Courses/CS101</emphasis>. The leading slash indicates that the path is absolute, that is, starting at the root directory. As an aside, in Windows, the backslash (\) character is used as the separator instead of the slash (/) character, so the file path given above would be written as <emphasis>\Faculty\Prof.Brown\Courses\CS101</emphasis>. Throughout this book we will use the UNIX convention for paths.
      </para>
      <para>
	At every instant, each process has a current <command><emphasis>working directory</emphasis></command>, in which path names not beginning with a slash are looked for. As an example, in <tag><link linkend="1-6">Fig. 1-6</link></tag>, if <emphasis>/Faculty/Prof.Brown</emphasis> were the working directory, then use of the path name <emphasis>Courses/CS101</emphasis> would yield the same file as the absolute path name given above. Processes can change their working directory by issuing a system call specifying the new working directory.
      </para>
      <para>
	Files and directories in MINIX 3 are protected by assigning each one an 11-bit binary protection code. The protection code consists of three 3-bit fields: one for the owner, one for other members of the owner's group (users are divided into groups by the system administrator), one for everyone else, and 2 bits we will discuss later. Each field has a bit for read access, a bit for write access, and a bit for execute access. These 3 bits are known as the <command><emphasis>rwx bits</emphasis></command>. For example, the protection code rwxr-x--x means that the owner can read, write, or execute the file, other group members can read or execute (but not write) the file, and everyone else can execute (but not read or write) the file. For a directory (as opposed to a file), x indicates search permission. A dash means that the corresponding permission is absent (the bit is zero).
      </para>
      <para>
	Before a file can be read or written, it must be opened, at which time the permissions are checked. If access is permitted, the system returns a small integer called a <command><emphasis>file descriptor</emphasis></command> to use in subsequent operations. If the access is prohibited, an error code (1) is returned.
      </para>
      <para>
	Another important concept in MINIX 3 is the mounted file system. Nearly all personal computers have one or more CD-ROM drives into which CD-ROMs can be inserted and removed. To provide a clean way to deal with removable media (CD-ROMs, DVDs, floppies, Zip drives, etc.), MINIX 3 allows the file system on a CD-ROM to be attached to the main tree. Consider the situation of <tag><link linkend="1-7">Fig. 1-7(a)</link></tag>. Before the <command>mount</command> call, the <command><emphasis>root file system</emphasis></command>, on the hard disk, and a second file system, on a CD-ROM, are separate and unrelated.
      </para>
      
      <para id="1-7"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/1-7.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 1-7. (a) Before mounting, the files on drive 0 are not accessible. (b) After mounting, they are part of the file hierarchy.</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	However, the file system on the CD-ROM cannot be used, because there is no way to specify path names on it. MINIX 3 does not allow path names to be prefixed by a drive name or number; that is precisely the kind of device dependence that operating systems ought to eliminate. Instead, the <command>mount</command> system call allows the file system on the CD-ROM to be attached to the root file system wherever the program wants it to be. In <tag><link linkend="1-7">Fig. 1-7(b)</link></tag> the file system on drive 0 has been mounted on directory <emphasis>b</emphasis>, thus allowing access to files <emphasis>/b/x</emphasis> and <emphasis>/b/y</emphasis>. If directory <emphasis>b</emphasis> had originally contained any files they would not be accessible while the CD-ROM was mounted, since <emphasis>/b</emphasis> would refer to the root directory of drive 0. (Not being able to access these files is not as serious as it at first seems: file systems are nearly always mounted on empty directories.) If a system contains multiple hard disks, they can all be mounted into a single tree as well.
      </para>
      <para>
	Another important concept in MINIX 3 is the <command><emphasis>special file</emphasis></command>. Special files are provided in order to make I/O devices look like files. That way, they can be read and written using the same system calls as are used for reading and writing files. Two kinds of special files exist: <command><emphasis>block special files</emphasis></command> and <command><emphasis>character special files</emphasis></command>. Block special files are normally used to model devices that consist of a collection of randomly addressable blocks, such as disks. By opening a block special file and reading, say, block 4, a program can directly access the fourth block on the device, without regard to the structure of the file system contained on it. Similarly, character special files are used to model printers, modems, and other devices that accept or output a character stream. By convention, the special files are kept in the /dev directory. For example, /dev/lp might be the line printer.
      </para>
      <para>
	The last feature we will discuss in this overview is one that relates to both processes and files: pipes. A <command><emphasis>pipe</emphasis></command> is a sort of pseudofile that can be used to connect two processes, as shown in <tag><link linkend="1-8">Fig. 1-8</link></tag>. If processes A and B wish to talk using a pipe, they must set it up in advance. When process A wants to send data to process B, it writes on the pipe as though it were an output file. Process B can read the data by reading from the pipe as though it were an input file. Thus, communication between processes in MINIX 3 looks very much like ordinary file reads and writes. Stronger yet, the only way a process can discover that the output file it is writing on is not really a file, but a pipe, is by making a special system call.
      </para>

      <para id="1-8"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/1-8.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 1-8. Two processes connected by a pipe.</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>
    </sect2>

    <sect2 id="sect-1.3.3">
      <title>1.3.3. The Shell</title>
      <para>
	The operating system is the code that carries out the system calls. Editors, compilers, assemblers, linkers, and command interpreters definitely are not part of the operating system, even though they are important and useful. At the risk of confusing things somewhat, in this section we will look briefly at the MINIX 3 command interpreter, called the <command><emphasis>shell</emphasis></command>. Although it is not part of the operating system, it makes heavy use of many operating system features and thus serves as a good example of how the system calls can be used. It is also the primary interface between a user sitting at his terminal and the operating system, unless the user is using a graphical user interface. Many shells exist, including csh, ksh, zsh, and bash. All of them support the functionality described below, which derives from the original shell (sh).
      </para>
      <para>
	When any user logs in, a shell is started up. The shell has the terminal as standard input and standard output. It starts out by typing the <command><emphasis>prompt</emphasis></command>, a character such as a dollar sign, which tells the user that the shell is waiting to accept a command. If the user now types
      </para>
      <para>
	<command>date</command>
      </para>
      <para>
	for example, the shell creates a child process and runs the date program as the child. While the child process is running, the shell waits for it to terminate. When the child finishes, the shell types the prompt again and tries to read the next input line.
      </para>
      <para>
	The user can specify that standard output be redirected to a file, for example,
      </para>
      <para>
	<command>date &gt;file</command>
      </para>
      <para>
	Similarly, standard input can be redirected, as in
      </para>
      <para>
	<command>sort &lt;file1 &gt;file2</command>
      </para>
      <para>
	which invokes the sort program with input taken from <emphasis>file1</emphasis> and output sent to <emphasis>file2</emphasis>.
      </para>
      <para>
	The output of one program can be used as the input for another program by connecting them with a pipe. Thus
      </para>
      <para>
	<command>cat file1 file2 file3 | sort &gt;/dev/lp</command>
      </para>
      <para>
	invokes the <emphasis>cat</emphasis> program to concatenate three files and send the output to <emphasis>sort</emphasis> to arrange all the lines in alphabetical order. The output of <emphasis>sort</emphasis> is redirected to the file <emphasis>/dev/lp</emphasis>, typically the printer.
      </para>
      <para>
	If a user puts an ampersand after a command, the shell does not wait for it to complete. Instead it just gives a prompt immediately. Consequently,
      </para>
      <para>
	<command>cat file1 file2 file3 | sort &gt;/dev/lp &amp;</command>
      </para>
      <para>
	starts up the sort as a background job, allowing the user to continue working normally while the sort is going on. The shell has a number of other interesting features, which we do not have space to discuss here. Most books for UNIX beginners are useful for MINIX 3 users who want to learn more about using the system. Examples are Ray and Ray (2003) and Herborth (2005).
      </para>
    </sect2>
  </sect1>

  <sect1 id="sect-1.4">
    <title>1.4. System Calls</title>
    <para>
      Armed with our general knowledge of how MINIX 3 deals with processes and files, we can now begin to look at the interface between the operating system and its application programs, that is, the set of system calls. Although this discussion specifically refers to POSIX (International Standard 9945-1), hence also to MINI 3, UNIX, and Linux, most other modern operating systems have system calls that perform the same functions, even if the details differ. Since the actual mechanics of issuing a system call are highly machine dependent, and often must be expressed in assembly code, a procedure library is provided to make it possible to make system calls from C programs.
    </para>
    <para>
      It is useful to keep the following in mind: any single-CPU computer can execute only one instruction at a time. If a process is running a user program in user mode and needs a system service, such as reading data from a file, it has to execute a trap or system call instruction to transfer control to the operating system. The operating system then figures out what the calling process wants by inspecting the parameters. Then it carries out the system call and returns control to the instruction following the system call. In a sense, making a system call is like making a special kind of procedure call, only system calls enter the kernel or other privileged operating system components and procedure calls do not.
    </para>
    <para>
      To make the system call mechanism clearer, let us take a quick look at <command>read</command>. It has three parameters: the first one specifying the file, the second one specifying the buffer, and the third one specifying the number of bytes to read. A call to <command>read</command> from a C program might look like this:
    </para>
    <para>
      <command>count = read(fd, buffer, nbytes);</command>
    </para>
    <para>
      The system call (and the library procedure) return the number of bytes actually read in count. This value is normally the same as nbytes, but may be smaller, if, for example, end-of-file is encountered while reading.
    </para>
    <para>
      If the system call cannot be carried out, either due to an invalid parameter or a disk error, count is set to 1, and the error number is put in a global variable, errno. Programs should always check the results of a system call to see if an error occurred.
    </para>
    <para>
      MINIX 3 has a total of 53 main system calls. These are listed in <tag><link linkend="1-9">Fig. 1-9</link></tag>, grouped for convenience in six categories. A few other calls exist, but they have very specialized uses so we will omit them here. In the following sections we will briefly examine each of the calls of <tag><link linkend="1-9">Fig. 1-9</link></tag> to see what it does. To a large extent, the services offered by these calls determine most of what the operating system has to do, since the resource management on personal computers is minimal (at least compared to big machines with many users).
    </para>
    <para/>

    <para id="1-9"/>
    <table xml:id="ex.calstable" frame="all">
      <tgroup cols="3" align="left" colsep="1" rowsep="1">
	<tbody>
	  <row>
	    <entry><command>Process management</command></entry>
	    <entry>
	      <para>pid = fork()</para>
	      <para/>
	      <para>pid = waitpid(pid, &amp;statloc, opts)</para>
	      <para>s = wait(&amp;status)</para>
	      <para>s = execve(name, argv, envp)</para>
	      <para>exit(status)</para>
	      <para/>
	      <para>size = brk(addr)</para>
	      <para>pid = getpid()</para>
	      <para>pid = getpgrp()</para>
	      <para/>
	      <para>pid = setsid()</para>
	      <para/><para/>
	      <para>l = ptrace(req, pid, addr, data)</para>
 	    </entry>
	    <entry>
	      <para>Create a child process identical to the parent</para>
	      <para>Wait for a child to terminate</para>
	      <para>Old version of waitpid</para>
	      <para>Replace a process core image</para>
	      <para>Terminate process execution and return status</para>
	      <para>Set the size of the data segment</para>
	      <para>Return the caller's process id</para>
	      <para>Return the id of the caller's process group</para>
	      <para>Create a new session and return its proc. group id</para>
	      <para>Used for debugging</para>
	    </entry>
	  </row>
	  <row>
	    <entry><command>Signals</command></entry>
	    <entry>
	      <para>s = sigaction(sig, &amp;act, &amp;oldact)</para>
	      <para>s = sigreturn(&amp;context)</para>
	      <para>s = sigprocmask(how, &amp;set, &amp;old)</para>
	      <para>s = sigpending(set)</para>
	      <para>s = sigsuspend(sigmask)</para>
	      <para/>
	      <para>s = kill(pid, sig)</para>
	      <para>residual = alarm(seconds)</para>
	      <para>s = pause()</para>
	    </entry>
	    <entry>
	      <para>Define action to take on signals</para>
	      <para>Return from a signal</para>
	      <para>Examine or change the signal mask</para>
	      <para>Get the set of blocked signals</para>
	      <para>Replace the signal mask and suspend the process</para>
	      <para>Send a signal to a process</para>
	      <para>Set the alarm clock</para>
	      <para>Suspend the caller until the next signal</para>
	    </entry>
	  </row>
	  <row>
	    <entry>
	      <para><command>File Management</command></para>
	    </entry>
	    <entry>
	      <para>fd = creat(name, mode)</para>
	      <para>fd = mknod(name, mode, addr)</para>
	      <para/>
	      <para>fd = open(file, how, ...)</para>
	      <para/><para/>
	      <para>s = close(fd)</para>
	      <para>n = read(fd, buffer, nbytes)</para>
	      <para/>
	      <para>n = write(fd, buffer, nbytes)</para>
	      <para/>
	      <para>pos = lseek(fd, offset, whence)</para>
	      <para>s = stat(name, &amp;buf)</para>
	      <para>s  = fstat(fd, &amp;buf)</para>
	      <para>fd = dup(fd)</para>
	      <para/>
	      <para>s = pipe(&amp;fd[0])</para>
	      <para>s = ioctl(fd, request, argp)</para>
	      <para/>
	      <para>s = access(name, amode)</para>
	      <para>s = rename(old, new)</para>
	      <para>s = fcntl(fd, cmd, ...)</para>
 	    </entry>
	    <entry>
	      <para>Obsolete way to create a new file</para>
	      <para>Create a regular, special, or directory i-node</para>
	      <para>Open a file for reading, writing or both</para>
	      <para>Close an open file</para>
	      <para>Read data from a file into a buffer</para>
	      <para>Write data from a buffer into a file</para>
	      <para>Move the file pointer</para>
	      <para>Get a file's status information</para>
	      <para>Get a file's status information</para>
	      <para>Allocate a new file descriptor for an open file</para>
	      <para>Create a pipe</para>
	      <para>Perform special operations on a file</para>
	      <para>Check a file's accessibility</para>
	      <para>Give a file a new name</para>
	      <para>File locking and other operations</para>
	    </entry>
	  </row>
	  <row>
	    <entry>
	      <para><command>Dir. &amp; File System Mgt.</command></para> 
	    </entry>
	    <entry>
	      <para>s = mkdir(name, mode)</para>
	      <para>s = rmdir(name)</para>
	      <para>s = link(name1, name2)</para>
	      <para/>
	      <para>s = unlink(name)</para>
	      <para>s = mount(special, name, flag)</para>
	      <para>s = umount(special)</para>
	      <para>s = sync()</para>
	      <para/>
	      <para>s = chdir(dirname)</para>
	      <para>s = chroot(dirname)</para>
   	    </entry>
	    <entry>
	      <para>Create a new directory</para>
	      <para>Remove an empty directory</para>
	      <para>Create a new entry, name2, pointing to name1</para>
	      <para>Remove a directory entry</para>
	      <para>Mount a file system</para>
	      <para>Unmount a file system</para>
	      <para>Flush all cached blocks to the disk</para>
	      <para>Change the working directory</para>
	      <para>Change the root directory</para>
	    </entry>
	  </row>
	  <row>
	    <entry>
	      <para><command>Protection</command></para>
	    </entry>
	    <entry>
	      <para>s = chmod(name, mode)</para>
	      <para>uid = getuid()</para>
	      <para>gid = getgid()</para>
	      <para>s = setuid(uid)</para>
	      <para>s = setgid(gid)</para>
	      <para>s = chown(name, owner, group)</para>
	      <para>oldmask = umask(complmode)</para>
	    </entry>
	    <entry>
	      <para>Change a file's protection bits</para>
	      <para>Get the caller's uid</para>
	      <para>Get the caller's gid</para>
	      <para>Set the caller's uid</para>
	      <para>Set the caller's gid</para>
	      <para>Change a file's owner and group</para>
	      <para>Change the mode mask</para>
	    </entry>
	  </row>
	  <row>
	    <entry>
	      <para><command>Time Management</command></para>
	    </entry>
	    <entry>
	      <para>seconds = time(&amp;seconds)</para>
	      <para/>
	      <para>s = stime(tp)</para>
	      <para/>
	      <para>s = utime(file, timep)</para>
	      <para>s = times(buffer)</para>
	    </entry>
	    <entry>
	      <para>Get the elapsed time since Jan. 1, 1970</para>
	      <para>Set the elapsed time since Jan. 1, 1970</para>
	      <para>Set a file's "last access" time</para>
	      <para>Get the user and system times used so far</para>
	    </entry>
	  </row>
	</tbody>
      </tgroup>
    </table>
    <para>
      <command>Figure 1-9. The main MINIX system calls. fd is a file descriptor; n is a byte count.</command>
    </para>

    <para>
      This is a good place to point out that the mapping of POSIX procedure calls onto system calls is not necessarily one-to-one. The POSIX standard specifies a number of procedures that a conformant system must supply, but it does not specify whether they are system calls, library calls, or something else. In some cases, the POSIX procedures are supported as library routines in MINIX 3. In others, several required procedures are only minor variations of one another, and one system call handles all of them.
    </para>

    <sect2 id="sect-1.4.1">
      <title>1.4.1. System Calls for Process Management</title>
      <para>
	The first group of calls in <tag><link linkend="1-9">Fig. 1-9</link></tag> deals with process management. <command>Fork</command> is a good place to start the discussion. <command>Fork</command> is the only way to create a new process in MINIX 3. It creates an exact duplicate of the original process, including all the file descriptors, registerseverything. After the <command>fork</command>, the original process and the copy (the parent and child) go their separate ways. All the variables have identical values at the time of the <command>fork</command>, but since the parent's data are copied to create the child, subsequent changes in one of them do not affect the other one. (The program text, which is unchangeable, is shared between parent and child.) The <command>fork</command> call returns a value, which is zero in the child and equal to the child's process identifier or PID in the parent. Using the returned <command><emphasis>PID</emphasis></command>, the two processes can see which one is the parent process and which one is the child process.
      </para>
      <para>
	In most cases, after a <command>fork</command>, the child will need to execute different code from the parent. Consider the shell. It reads a command from the terminal, forks off a child process, waits for the child to execute the command, and then reads the next command when the child terminates. To wait for the child to finish, the parent executes a <command>waitpid</command> system call, which just waits until the child terminates (any child if more than one exists). <command>Waitpid</command> can wait for a specific child, or for any old child by setting the first parameter to 1. When <command>waitpid</command> completes, the address pointed to by the second parameter, statloc, will be set to the child's exit status (normal or abnormal termination and exit value). Various options are also provided, specified by the third parameter. The <command>waitpid</command> call replaces the previous <command>wait</command> call, which is now obsolete but is provided for reasons of backward compatibility.
      </para>
      <para>
	Now consider how <command>fork</command> is used by the shell. When a command is typed, the shell forks off a new process. This child process must execute the user command. It does this by using the <command>execve</command> system call, which causes its entire core image to be replaced by the file named in its first parameter. (Actually, the system call itself is <command>exec</command>, but several different library procedures call it with different parameters and slightly different names. We will treat these as system calls here.)A highly simplified shell illustrating the use of <command>fork</command>, <command>waitpid</command>, and <command>execve</command> is shown in <tag><link linkend="1-10">Fig. 1-10</link></tag>.
      </para>
      
      <para id="1-10"><command>Figure 1-10. A stripped-down shell. Throughout this book, TRUE is assumed to be defined as 1.</command></para>
      <programlisting>#define TRUE 1

while (TRUE){         /* repeat forever */
  type_prompt();      /* display prompt on the screen */
  read_command(command, parameters);  /* read input from terminal */

  if (fork() != 0){   /* fork off child process */
    /* Parent code. */
    waitpid(1, &amp;status, 0);  /* wait for child to exit */
  } else {
    /* Child code. */
    execve(command, parameters, 0);  /* execute command */
  }
 }

      </programlisting>

      <para>
	In the most general case, <command>execve</command> has three parameters: the name of the file to be executed, a pointer to the argument array, and a pointer to the environment array. These will be described shortly. Various library routines, including execl, execv, execle, and execve, are provided to allow the parameters to be omitted or specified in various ways. Throughout this book we will use the name <command>exec</command> to represent the system call invoked by all of these.
      </para>
      <para>
	Let us consider the case of a command such as
      </para>
      <para>
	<command>cp file1 file2</command>
      </para>
      <para>
	used to copy <emphasis>file1</emphasis> to <emphasis>file2</emphasis>. After the shell has forked, the child process locates and executes the file <emphasis>cp</emphasis> and passes to it the names of the source and target files.
      </para>
      <para>
	The main program of <emphasis>cp</emphasis> (and main program of most other C programs) contains the declaration
      </para>
      <para>
	<command>main(argc, argv, envp)</command>
      </para>
      <para>
	where <emphasis>argc</emphasis> is a count of the number of items on the command line, including the program name. For the example above, <emphasis>argc</emphasis> is 3.
      </para>
      <para>
	The second parameter, <emphasis>argv</emphasis>, is a pointer to an array. Element <emphasis>i</emphasis> of that array is a pointer to the <emphasis>i</emphasis>-th string on the command line. In our example, <emphasis>argv</emphasis>[0] would point to the string "cp", <emphasis>argv</emphasis>[1] would point to the string "file1", and <emphasis>argv</emphasis>[2] would point to the string "file2".
      </para>
      <para>
	The third parameter of <emphasis>main</emphasis>, <emphasis>envp</emphasis>, is a pointer to the environment, an array of strings containing assignments of the form <emphasis>name</emphasis>=<emphasis>value</emphasis> used to pass information such as the terminal type and home directory name to a program. In <tag><link linkend="1-10">Fig. 1-10</link></tag>, no environment is passed to the child, so the third parameter of execve is a zero.
      </para>
      <para>
	If <command>exec</command> seems complicated, do not despair; it is (semantically) the most complex of all the POSIX system calls. All the other ones are much simpler. As an example of a simple one, consider <command>exit</command>, which processes should use when they are finished executing. It has one parameter, the exit status (0 to 255), which is returned to the parent via statloc in the <command>waitpid</command> system call. The low-order byte of <emphasis>status</emphasis> contains the termination status, with 0 being normal termination and the other values being various error conditions. The high-order byte contains the child's exit status (0 to 255). For example, if a parent process executes the statement
      </para>
      <para>
	<command>n = waitpid(1, &amp;statloc, options);</command>
      </para>
      <para>
	it will be suspended until some child process terminates. If the child exits with, say, 4 as the parameter to <emphasis>exit</emphasis>, the parent will be awakened with <emphasis>n</emphasis> set to the child's PID and <emphasis>statloc</emphasis> set to 0x0400 (the C convention of prefixing hexadecimal constants with 0x will be used throughout this book).
      </para>
      <para>
	Processes in MINIX 3 have their memory divided up into three segments: the <command><emphasis>text segment</emphasis></command> (i.e., the program code), the <command><emphasis>data segment</emphasis></command> (i.e., the variables), and the <command><emphasis>stack segment</emphasis></command>. The data segment grows upward and the stack grows downward, as shown in <tag><link linkend="1-11">Fig. 1-11</link></tag>. Between them is a gap of unused address space. The stack grows into the gap automatically, as needed, but expansion of the data segment is done explicitly by using a system call, <command>brk</command>, which specifies the new address where the data segment is to end. This address may be more than the current value (data segment is growing) or less than the current value (data segment is shrinking). The parameter must, of course, be less than the stack pointer or the data and stack segments would overlap, which is forbidden.
      </para>

      <para id="1-11"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/1-11.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 1-11. Processes have three segments: text, data, and stack. In this example, all three are in one address space, but separate instruction and data space is also supported.</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	As a convenience for programmers, a library routine sbrk is provided that also changes the size of the data segment, only its parameter is the number of bytes to add to the data segment (negative parameters make the data segment smaller). It works by keeping track of the current size of the data segment, which is the value returned by <command>brk</command>, computing the new size, and making a call asking for that number of bytes. The <command>brk</command> and <command>sbrk</command> calls, however, are not defined by the POSIX standard. Programmers are encouraged to use the <emphasis>malloc</emphasis> library procedure for dynamically allocating storage, and the underlying implementation of malloc was not thought to be a suitable subject for standardization since few programmers use it directly.
      </para>
      <para>
	The next process system call is also the simplest, <command>getpid</command>. It just returns the caller's PID. Remember that in <command>fork</command>, only the parent was given the child's PID. If the child wants to find out its own PID, it must use <command>getpid</command>. The <command>getpgrp</command> call returns the PID of the caller's process group. <command>setsid</command> creates a new session and sets the process group's PID to the caller's. Sessions are related to an optional feature of POSIX, <command><emphasis>job control</emphasis></command>, which is not supported by MINIX 3 and which will not concern us further.
      </para>
      <para>
	The last process management system call, <command>ptrace</command>, is used by debugging programs to control the program being debugged. It allows the debugger to read and write the controlled process' memory and manage it in other ways.
      </para>
    </sect2>

    <sect2 id="sect-1.4.2">
      <title>1.4.2. System Calls for Signaling</title>
      <para>
	Although most forms of interprocess communication are planned, situations exist in which unexpected communication is needed. For example, if a user accidently tells a text editor to list the entire contents of a very long file, and then realizes the error, some way is needed to interrupt the editor. In MINIX 3, the user can hit the CTRL-C key on the keyboard, which sends a <command><emphasis>signal</emphasis></command> to the editor. The editor catches the signal and stops the print-out. Signals can also be used to report certain traps detected by the hardware, such as illegal instruction or floating point overflow. Timeouts are also implemented as signals.
      </para>
      <para>
	When a signal is sent to a process that has not announced its willingness to accept that signal, the process is simply killed without further ado. To avoid this fate, a process can use the <command>sigaction</command> system call to announce that it is prepared to accept some signal type, and to provide the address of the signal handling procedure and a place to store the address of the current one. After a <command>sigaction</command> call, if a signal of the relevant type is generated (e.g., by pressing CTRL-C), the state of the process is pushed onto its own stack, and then the signal handler is called. It may run for as long as it wants to and perform any system calls it wants to. In practice, though, signal handlers are usually fairly short. When the signal handling procedure is done, it calls <command>sigreturn</command> to continue where it left off before the signal. The <command>sigaction</command> call replaces the older <command>signal</command> call, which is now provided as a library procedure, however, for backward compatibility.
      </para>
      <para>
	Signals can be blocked in MINIX 3. A blocked signal is held pending until it is unblocked. It is not delivered, but also not lost. The <command>sigprocmask</command> call allows a process to define the set of blocked signals by presenting the kernel with a bitmap. It is also possible for a process to ask for the set of signals currently pending but not allowed to be delivered due to their being blocked. The <command>sigpending</command> call returns this set as a bitmap. Finally, the <command>sigsuspend</command> call allows a process to atomically set the bitmap of blocked signals and suspend itself.
      </para>
      <para>
	Instead of providing a function to catch a signal, the program may also specify the constant SIG_IGN to have all subsequent signals of the specified type ignored, or SIG_DFL to restore the default action of the signal when it occurs. The default action is either to kill the process or ignore the signal, depending upon the signal. As an example of how SIG_IGN is used, consider what happens when the shell forks off a background process as a result of
      </para>
      <para>
	<command>command &amp;</command>
      </para>
      <para>
	It would be undesirable for a SIGINT signal (generated by pressing CTRL-C) to affect the background process, so after the <command>fork</command> but before the <command>exec</command>, the shell does
      </para>
      <para>
	<command>sigaction(SIGINT, SIG_IGN, NULL);</command>
      </para>
      <para>
	and
      </para>
      <para>
	<command>sigaction(SIGQUIT, SIG_IGN, NULL);</command>
      </para>
      <para>
	to disable the SIGINT and SIGQUIT signals. (SIGQUIT is generated by CTRL-\; it is the same as SIGINT generated by CTRL-C except that if it is not caught or ignored it makes a core dump of the process killed.) For foreground processes (no ampersand), these signals are not ignored.
      </para>
      <para>
	Hitting CTRL-C is not the only way to send a signal. The <command>kill</command> system call allows a process to signal another process (provided they have the same UID unrelated processes cannot signal each other). Getting back to the example of background processes used above, suppose a background process is started up, but later it is decided that the process should be terminated. SIGINT and SIGQUIT have been disabled, so something else is needed. The solution is to use the kill program, which uses the <command>kill</command> system call to send a signal to any process. By sending signal 9 (SIGKILL), to a background process, that process can be killed. SIGKILL cannot be caught or ignored.
      </para>
      <para>
	For many real-time applications, a process needs to be interrupted after a specific time interval to do something, such as to retransmit a potentially lost packet over an unreliable communication line. To handle this situation, the <command>alarm</command> system call has been provided. The parameter specifies an interval, in seconds, after which a SIGALRM signal is sent to the process. A process may only have one alarm outstanding at any instant. If an <command>alarm</command> call is made with a parameter of 10 seconds, and then 3 seconds later another <command>alarm</command> call is made with a parameter of 20 seconds, only one signal will be generated, 20 seconds after the second call. The first signal is canceled by the second call to <command>alarm</command>. If the parameter to alarm is zero, any pending alarm signal is canceled. If an alarm signal is not caught, the default action is taken and the signaled process is killed.
      </para>
      <para>
	It sometimes occurs that a process has nothing to do until a signal arrives. For example, consider a computer-aided-instruction program that is testing reading speed and comprehension. It displays some text on the screen and then calls <command>alarm</command> to signal it after 30 seconds. While the student is reading the text, the program has nothing to do. It could sit in a tight loop doing nothing, but that would waste CPU time that another process or user might need. A better idea is to use <command>pause</command>, which tells MINIX 3 to suspend the process until the next signal.
      </para>
    </sect2>

    <sect2 id="sect-1.4.3">
      <title>1.4.3. System Calls for File Management</title>
      <para>
	Many system calls relate to the file system. In this section we will look at calls that operate on individual files; in the next one we will examine those that involve directories or the file system as a whole. To create a new file, the <command>creat</command> call is used (why the call is <command>creat</command> and not <command>create</command> has been lost in the mists of time). Its parameters provide the name of the file and the protection mode. Thus
      </para>
      <para>
	<command>fd = creat("abc", 0751);</command>
      </para>
      <para>
	creates a file called <emphasis>abc</emphasis> with mode 0751 octal (in C, a leading zero means that a constant is in octal). The low-order 9 bits of 0751 specify the <emphasis>rwx</emphasis> bits for the owner (7 means read-write-execute permission), his group (5 means read-execute), and others (1 means execute only).
      </para>
      <para>
	<command>Creat</command> not only creates a new file but also opens it for writing, regardless of the file's mode. The file descriptor returned, fd, can be used to write the file. If a <command>creat</command> is done on an existing file, that file is truncated to length 0, provided, of course, that the permissions are all right. The creat call is obsolete, as open can now create new files, but it has been included for backward compatibility.
      </para>
      <para>
	Special files are created using <command>mknod</command> rather than <command>creat</command>. A typical call is
      </para>
      <para>
	<command>fd = mknod("/dev/ttyc2", 020744, 0x0402);</command>
      </para>
      <para>
	which creates a file named <emphasis>/dev/ttyc2</emphasis> (the usual name for console 2) and gives it mode 020744 octal (a character special file with protection bits <emphasis>rwxr--r--</emphasis>). The third parameter contains the major device (4) in the high-order byte and the minor device (2) in the low-order byte. The major device could have been anything, but a file named <emphasis>/dev/ttyc2</emphasis> ought to be minor device 2. Calls to <command>mknod</command> fail unless the caller is the superuser.
      </para>
      <para>
	To read or write an existing file, the file must first be opened using <command>open</command>. This call specifies the file name to be opened, either as an absolute path name or relative to the working directory, and a code of <emphasis>O_RDONLY</emphasis>, <emphasis>O_WRONLY</emphasis>, or <emphasis>O_RDWR</emphasis>, meaning open for reading, writing, or both. The file descriptor returned can then be used for reading or writing. Afterward, the file can be closed by <command>close</command>, which makes the file descriptor available for reuse on a subsequent <command>creat</command> or <command>open</command>.
      </para>
      <para>
	The most heavily used calls are undoubtedly <command>read</command> and <command>write</command>. We saw <command>read</command> earlier; <command>write</command> has the same parameters.
      </para>
      <para>
	Although most programs read and write files sequentially, for some applications programs need to be able to access any part of a file at random. Associated with each file is a pointer that indicates the current position in the file. When reading (writing) sequentially, it normally points to the next byte to be read (written). The <command>lseek</command> call changes the value of the position pointer, so that subsequent calls to <command>read</command> or <command>write</command> can begin anywhere in the file, or even beyond the end.
      </para>
      <para>
	<command>lseek</command> has three parameters: the first is the file descriptor for the file, the second is a file position, and the third tells whether the file position is relative to the beginning of the file, the current position, or the end of the file. The value returned by <command>lseek</command> is the absolute position in the file after changing the pointer.
      </para>
      <para>
	For each file, MINIX 3 keeps track of the file mode (regular file, special file, directory, and so on), size, time of last modification, and other information. Programs can ask to see this information via the <command>stat</command> and <command>fstat</command> system calls. These differ only in that the former specifies the file by name, whereas the latter takes a file descriptor, making it useful for open files, especially standard input and standard output, whose names may not be known. Both calls provide as the second parameter a pointer to a structure where the information is to be put. The structure is shown in <tag><link linkend="1-12">Fig. 1-12</link></tag>.
      </para>

      <para id="1-12"><command>Figure 1-12. The structure used to return information for the stat and fstat system calls. In the actual code, symbolic names are used for some of the types.</command></para>
      <programlisting>struct stat{
  short st_dev;            /* device where i-node belongs */
  unsigned short st_ino;   /* i-node number */
  unsigned short st_mode;  /* mode word */
  short st_nlink;          /* number of links */
  short st_uid;            /* user id */
  short st_gid;            /* group id */
  short st_rdev;           /* major/minor device for special files */
  long st_size;            /* file size */
  long st_atime;           /* time of last access */
  long st_mtime;           /* time of last modification */
  long st_ctime;           /* time of last change to i-node */
};

      </programlisting>

      <para>
	When manipulating file descriptors, the <command>dup</command> call is occasionally helpful. Consider, for example, a program that needs to close standard output (file descriptor 1), substitute another file as standard output, call a function that writes some output onto standard output, and then restore the original situation. Just closing file descriptor 1 and then opening a new file will make the new file standard output (assuming standard input, file descriptor 0, is in use), but it will be impossible to restore the original situation later.
      </para>
      <para>
	The solution is first to execute the statement
      </para>
      <para>
	<command>fd = dup(1);</command>
      </para>
      <para>
	which uses the <command>dup</command> system call to allocate a new file descriptor, <emphasis>fd</emphasis>, and arrange for it to correspond to the same file as standard output. Then standard output can be closed and a new file opened and used. When it is time to restore the original situation, file descriptor 1 can be closed, and then
      </para>
      <para>
	<command>n = dup(fd);</command>
      </para>
      <para>
	executed to assign the lowest file descriptor, namely, 1, to the same file as <emphasis>fd</emphasis>. Finally, <emphasis>fd</emphasis> can be closed and we are back where we started.
      </para>
      <para>
	The <command>dup</command> call has a variant that allows an arbitrary unassigned file descriptor to be made to refer to a given open file. It is called by
      </para>
      <para>
	<command>dup2(fd, fd2);</command>
      </para>
      <para>
	where <emphasis>fd</emphasis> refers to an open file and <emphasis>fd2</emphasis> is the unassigned file descriptor that is to be made to refer to the same file as <emphasis>fd</emphasis>. Thus if <emphasis>fd</emphasis> refers to standard input (file descriptor 0) and <emphasis>fd2</emphasis> is 4, after the call, file descriptors 0 and 4 will both refer to standard input.
      </para>
      <para>
	Interprocess communication in MINIX 3 uses pipes, as described earlier. When a user types
      </para>
      <para>
	<command>cat file1 file2 | sort</command>
      </para>
      <para>
	the shell creates a pipe and arranges for standard output of the first process to write to the pipe, so standard input of the second process can read from it. The <command>pipe</command> system call creates a pipe and returns two file descriptors, one for writing and one for reading. The call is
      </para>
      <para>
	<command>pipe(&amp;fd[0]);</command>
      </para>
      <para>
	where <emphasis>fd</emphasis> is an array of two integers and <emphasis>fd</emphasis>[0] is the file descriptor for reading and <emphasis>fd</emphasis>[1] is the one for writing. Typically, a <command>fork</command> comes next, and the parent closes the file descriptor for reading and the child closes the file descriptor for writing (or vice versa), so when they are done, one process can read the pipe and the other can write on it.
      </para>
      <para>
	<tag><link linkend="1-13">Figure 1-13</link></tag> depicts a skeleton procedure that creates two processes, with the output of the first one piped into the second one. (A more realistic example would do error checking and handle arguments.) First a pipe is created, and then the procedure forks, with the parent eventually becoming the first process in the pipeline and the child process becoming the second one. Since the files to be executed, process1 and process2, do not know that they are part of a pipeline, it is essential that the file descriptors be manipulated so that the first process' standard output be the pipe and the second one's standard input be the pipe. The parent first closes off the file descriptor for reading from the pipe. Then it closes standard output and does a DUP call that allows file descriptor 1 to write on the pipe. It is important to realize that dup always returns the lowest available file descriptor, in this case, 1. Then the program closes the other pipe file descriptor.
      </para>

      <para id="1-13"><command>Figure 1-13. A skeleton for setting up a two-process pipeline.</command></para>
      <programlisting>#define STD_INPUT0        /* file descriptor for standard input */
#define STD_OUTPUT1       /* file descriptor for standard output */
pipeline(process1, process2)
char *process1, *process2;        /* pointers to program names */
{
  int fd[2];

  pipe(&amp;fd[0]);        /* create a pipe */
  if (fork() != 0) {
    /* The parent process executes these statements. */
    close(fd[0]);      /* process 1 does not need to read from pipe */
    close(STD_OUTPUT); /* prepare for new standard output */
    dup(fd[1]);        /* set standard output to fd[1] */
    close(fd[1]);      /* this file descriptor not needed any more */
    execl(process1, process1, 0);
  } else {
    /* The child process executes these statements. */
    close(fd[1]);      /* process 2 does not need to write to pipe */
    close(STD_INPUT);  /* prepare for new standard input */
    dup(fd[0]);        /* set standard input to fd[0] */
    close(fd[0]);      /* this file descriptor not needed any more */
    execl(process2, process2, 0);
  }
}

      </programlisting>

      <para>
	After the <command>exec</command> call, the process started will have file descriptors 0 and 2 be unchanged, and file descriptor 1 for writing on the pipe. The child code is analogous. The parameter to execl is repeated because the first one is the file to be executed and the second one is the first parameter, which most programs expect to be the file name.
      </para>
      <para>
	The next system call, <command>ioctl</command>, is potentially applicable to all special files. It is, for instance, used by block device drivers like the SCSI driver to control tape and CD-ROM devices. Its main use, however, is with special character files, primarily terminals. POSIX defines a number of functions which the library translates into ioctl calls. The <emphasis>tcgetattr</emphasis> and <emphasis>tcsetattr</emphasis> library functions use ioctl to change the characters used for correcting typing errors on the terminal, changing the <command><emphasis>terminal mode</emphasis></command>, and so forth.
      </para>
      <para>
	Traditionally, there are three terminal modes, cooked, raw, and cbreak. <command><emphasis>Cooked mode</emphasis></command> is the normal terminal mode, in which the erase and kill characters work normally, CTRL-S and CTRL-Q can be used for stopping and starting terminal output, CTRL-D means end of file, CTRL-C generates an interrupt signal, and CTRL-\ generates a quit signal to force a core dump.
      </para>
      <para>
	In <command><emphasis>raw mode</emphasis></command>, all of these functions are disabled; consequently, every character is passed directly to the program with no special processing. Furthermore, in raw mode, a read from the terminal will give the program any characters that have been typed, even a partial line, rather than waiting for a complete line to be typed, as in cooked mode. Screen editors often use this mode.
      </para>
      <para>
	<command><emphasis>Cbreak mode</emphasis></command> is in between. The erase and kill characters for editing are disabled, as is CTRL-D, but CTRL-S, CTRL-Q, CTRL-C, and CTRL-\ are enabled. Like raw mode, partial lines can be returned to programs (if intraline editing is turned off there is no need to wait until a whole line has been receivedthe user cannot change his mind and delete it, as he can in cooked mode).
      </para>
      <para>
	POSIX does not use the terms cooked, raw, and cbreak. In POSIX terminology <command><emphasis>canonical mode</emphasis></command> corresponds to cooked mode. In this mode there are eleven special characters defined, and input is by lines. In <command><emphasis>noncanonical mode</emphasis></command> a minimum number of characters to accept and a time, specified in units of 1/10th of a second, determine how a read will be satisfied. Under POSIX there is a great deal of flexibility, and various flags can be set to make noncanonical mode behave like either cbreak or raw mode. The older terms are more descriptive, and we will continue to use them informally.
      </para>
      <para>
	<command>Ioctl</command> has three parameters, for example a call to tcsetattr to set terminal parameters will result in
      </para>
      <para>
	<command>ioctl(fd, TCSETS, &amp;termios);</command>
      </para>
      <para>
	The first parameter specifies a file, the second one specifies an operation, and the third one is the address of the POSIX structure that contains flags and the array of control characters. Other operation codes instruct the system to postpone the changes until all output has been sent, cause unread input to be discarded, and return the current values.
      </para>
      <para>
	The <command>access</command> system call is used to determine whether a certain file access is permitted by the protection system. It is needed because some programs can run using a different user's UID. This SETUID mechanism will be described later.
      </para>
      <para>
	The <command>rename</command> system call is used to give a file a new name. The parameters specify the old and new names.
      </para>
      <para>
	Finally, the <command>fcntl</command> call is used to control files, somewhat analogous to <command>ioctl</command> (i.e., both of them are horrible hacks). It has several options, the most important of which is for advisory file locking. Using <command>fcntl</command>, it is possible for a process to lock and unlock parts of files and test part of a file to see if it is locked. The call does not enforce any lock semantics. Programs must do this themselves.
      </para>
    </sect2>

    <sect2 id="sect-1.4.4">
      <title>1.4.4. System Calls for Directory Management</title>
      <para>
	In this section we will look at some system calls that relate more to directories or the file system as a whole, rather than just to one specific file as in the previous section. The first two calls, <command>mkdir</command> and <command>rmdir</command>, create and remove empty directories, respectively. The next call is <command>link</command>. Its purpose is to allow the same file to appear under two or more names, often in different directories. A typical use is to allow several members of the same programming team to share a common file, with each of them having the file appear in his own directory, possibly under different names. Sharing a file is not the same as giving every team member a private copy, because having a shared file means that changes that any member of the team makes are instantly visible to the other membersthere is only one file. When copies are made of a file, subsequent changes made to one copy do not affect the other ones.
      </para>
      <para>
	To see how <command>link</command> works, consider the situation of <tag><link linkend="1-14">Fig. 1-14(a)</link></tag>. Here are two users, ast and <emphasis>jim</emphasis>, each having their own directories with some files. If <emphasis>ast</emphasis> now executes a program containing the system call
      </para>
      <para>
	<command>link("/usr/jim/memo", "/usr/ast/note");</command>
      </para>
      <para>
	the file <emphasis>memo</emphasis> in <emphasis>jim's</emphasis> directory is now entered into <emphasis>ast's</emphasis> directory under the name <emphasis>note</emphasis>. Thereafter, <emphasis>/usr/jim/memo</emphasis> and <emphasis>/usr/ast/note</emphasis> refer to the same file.
      </para>

      <para id="1-14"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/1-14.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 1-14. (a) Two directories before linking /usr/jim/memo to ast's directory. (b) The same directories after linking.</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	Understanding how <command>link</command> works will probably make it clearer what it does. Every file in UNIX has a unique number, its i-number, that identifies it. This inumber is an index into a table of <command><emphasis>i-nodes</emphasis></command>, one per file, telling who owns the file, where its disk blocks are, and so on. A directory is simply a file containing a set of (i-number, ASCII name) pairs. In the first versions of UNIX, each directory entry was 16 bytes2 bytes for the i-number and 14 bytes for the name. A more complicated structure is needed to support long file names, but conceptually a directory is still a set of (i-number, ASCII name) pairs. In <tag><link linkend="1-14">Fig. 1-14</link></tag>, <emphasis>mail</emphasis> has inumber 16, and so on. What <command>link</command> does is simply create a new directory entry with a (possibly new) name, using the i-number of an existing file. In <tag><link linkend="1-14">Fig. 1-14(b)</link></tag>, two entries have the same i-number (70) and thus refer to the same file. If either one is later removed, using the <command>unlink</command> system call, the other one remains. If both are removed, UNIX sees that no entries to the file exist (a field in the i-node keeps track of the number of directory entries pointing to the file), so the file is removed from the disk.
      </para>
      <para>
	As we have mentioned earlier, the <command>mount</command> system call allows two file systems to be merged into one. A common situation is to have the root file system containing the binary (executable) versions of the common commands and other heavily used files, on a hard disk. The user can then insert a CD-ROM with files to be read into the CD-ROM drive.
      </para>
      <para>
	By executing the <command>mount</command> system call, the CD-ROM file system can be attached to the root file system, as shown in <tag><link linkend="1-15">Fig. 1-15</link></tag>. A typical statement in C to perform the mount is
      </para>
      <para>
	<command>mount("/dev/cdrom0", "/mnt", 0);</command>
      </para>
      <para>
	where the first parameter is the name of a block special file for CD-ROM drive 0, the second parameter is the place in the tree where it is to be mounted, and the third one tells whether the file system is to be mounted read-write or read-only.
      </para>

      <para id="1-15"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/1-15.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 1-15. (a) File system before the mount. (b) File system after the mount.</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	After the <command>mount</command> call, a file on CD-ROM drive 0 can be accessed by just using its path from the root directory or the working directory, without regard to which drive it is on. In fact, second, third, and fourth drives can also be mounted anywhere in the tree. The <command>mount</command> call makes it possible to integrate removable media into a single integrated file hierarchy, without having to worry about which device a file is on. Although this example involves CD-ROMs, hard disks or portions of hard disks (often called <command><emphasis>partitions</emphasis></command> or <command><emphasis>minor devices</emphasis></command>) can also be mounted this way. When a file system is no longer needed, it can be unmounted with the <command>umount</command> system call.
      </para>
      <para>
	MINIX 3 maintains a <command><emphasis>block cache</emphasis></command> cache of recently used blocks in main memory to avoid having to read them from the disk if they are used again quickly. If a block in the cache is modified (by a <command>write</command> on a file) and the system crashes before the modified block is written out to disk, the file system will be damaged. To limit the potential damage, it is important to flush the cache periodically, so that the amount of data lost by a crash will be small. The system call <command>sync</command> tells MINIX 3 to write out all the cache blocks that have been modified since being read in. When MINIX 3 is started up, a program called <emphasis>update</emphasis> is started as a background process to do a <command>sync</command> every 30 seconds, to keep flushing the cache.
      </para>
      <para>
	Two other calls that relate to directories are <command>chdir</command> and <command>chroot</command>. The former changes the working directory and the latter changes the root directory. After the call
      </para>
      <para>
	<command>chdir("/usr/ast/test");</command>
      </para>
      <para>
	an open on the file <emphasis>xyz</emphasis> will open <emphasis>/usr/ast/test/xyz</emphasis>. <command>chroot</command> works in an analogous way. Once a process has told the system to change its root directory, all absolute path names (path names beginning with a "/") will start at the new root. Why would you want to do that? For securityserver programs for protocols such as <command>FTP</command> (File Transfer Protocol) and <command>HTTP</command> (HyperText Transfer Protocol) do this so remote users of these services can access only the portions of a file system below the new root. Only superusers may execute chroot, and even superusers do not do it very often.
      </para>
    </sect2>

    <sect2 id="sect-1.4.5">
      <title>1.4.5. System Calls for Protection</title>
      <para>
	In MINIX 3 every file has an 11-bit mode used for protection. Nine of these bits are the read-write-execute bits for the owner, group, and others. The <command>chmod</command> system call makes it possible to change the mode of a file. For example, to make a file read-only by everyone except the owner, one could execute
      </para>
      <para>
	<command>chmod("file", 0644);</command>
      </para>
      <para>
	The other two protection bits, 02000 and 04000, are the SETGID (set-group-id) and SETUID (set-user-id) bits, respectively. When any user executes a program with the SETUID bit on, for the duration of that process the user's effective UID is changed to that of the file's owner. This feature is heavily used to allow users to execute programs that perform superuser only functions, such as creating directories. Creating a directory uses <command>mknod</command>, which is for the superuser only. By arranging for the <emphasis>mkdir</emphasis> program to be owned by the superuser and have mode 04755, ordinary users can be given the power to execute <command>mknod</command> but in a highly restricted way.
      </para>
      <para>
	When a process executes a file that has the SETUID or SETGID bit on in its mode, it acquires an effective UID or GID different from its real UID or GID. It is sometimes important for a process to find out what its real and effective UID or GID is. The system calls <command>getuid</command> and <command>getgid</command> have been provided to supply this information. Each call returns both the real and effective UID or GID, so four library routines are needed to extract the proper information: <emphasis>getuid</emphasis>, <emphasis>getgid</emphasis>, <emphasis>geteuid</emphasis>, and <emphasis>getegid</emphasis>. The first two get the real UID/GID, and the last two the effective ones.
      </para>
      <para>
	Ordinary users cannot change their UID, except by executing programs with the SETUID bit on, but the superuser has another possibility: the <command>setuid</command> system call, which sets both the effective and real UIDs. <command>setgid</command> sets both GIDs. The superuser can also change the owner of a file with the <command>chown</command> system call. In short, the superuser has plenty of opportunity for violating all the protection rules, which explains why so many students devote so much of their time to trying to become superuser.
      </para>
      <para>
	The last two system calls in this category can be executed by ordinary user processes. The first one, <command>umask</command>, sets an internal bit mask within the system, which is used to mask off mode bits when a file is created. After the call
      </para>
      <para>
	<command>umask(022);</command>
      </para>
      <para>
	the mode supplied by <command>creat</command> and <command>mknod</command> will have the 022 bits masked off before being used. Thus the call
      </para>
      <para>
	<emphasis><command>creat("file", 0777);</command></emphasis>
      </para>
      <para>
	will set the mode to 0755 rather than 0777. Since the bit mask is inherited by child processes, if the shell does a <command>umask</command> just after login, none of the user's processes in that session will accidently create files that other people can write on.
      </para>
      <para>
	When a program owned by the root has the SETUID bit on, it can access any file, because its effective UID is the superuser. Frequently it is useful for the program to know if the person who called the program has permission to access a given file. If the program just tries the access, it will always succeed, and thus learn nothing.
      </para>
      <para>
	What is needed is a way to see if the access is permitted for the real UID. The <command>access</command> system call provides a way to find out. The <emphasis>mode</emphasis> parameter is 4 to check for read access, 2 for write access, and 1 for execute access. Combinations of these values are also allowed. For example, with <emphasis>mode</emphasis> equal to 6, the call returns 0 if both read and write access are allowed for the real ID; otherwise1 is returned. With <emphasis>mode</emphasis> equal to 0, a check is made to see if the file exists and the directories leading up to it can be searched.
      </para>
      <para>
	Although the protection mechanisms of all UNIX-like operating systems are generally similar, there are some differences and inconsistencies that lead to security vulnerabilities. See Chen et al. (2002) for a discussion.
      </para>
    </sect2>

    <sect2 id="sect-1.4.6">
      <title>1.4.6. System Calls for Time Management</title>
      <para>
	MINIX 3 has four system calls that involve the time-of-day clock. <command>Time</command> just returns the current time in seconds, with 0 corresponding to Jan. 1, 1970 at midnight (just as the day was starting, not ending). Of course, the system clock must be set at some point in order to allow it to be read later, so <command>stime</command> has been provided to let the clock be set (by the superuser). The third time call is <command>utime</command>, which allows the owner of a file (or the superuser) to change the time stored in a file's i-node. Application of this system call is fairly limited, but a few programs need it, for example, touch, which sets the file's time to the current time.
      </para>
      <para>
	Finally, we have <command>times</command>, which returns the accounting information to a process, so it can see how much CPU time it has used directly, and how much CPU time the system itself has expended on its behalf (handling its system calls). The total user and system times used by all of its children combined are also returned.
      </para>
    </sect2>
  </sect1>

  <sect1 id="sect-1.5">
    <title>1.5. Operating System Structure</title>
    <para>
      Now that we have seen what operating systems look like on the outside (i.e, the programmer's interface), it is time to take a look inside. In the following sections, we will examine five different structures that have been tried, in order to get some idea of the spectrum of possibilities. These are by no means exhaustive, but they give an idea of some designs that have been tried in practice. The five designs are monolithic systems, layered systems, virtual machines, exokernels, and client-server systems.
    </para>

    <sect2 id="sect-1.5.1">
      <title>1.5.1. Monolithic Systems</title>
      <para>
	By far the most common organization, this approach might well be subtitled "The Big Mess." The structure is that there is no structure. The operating system is written as a collection of procedures, each of which can call any of the other ones whenever it needs to. When this technique is used, each procedure in the system has a well-defined interface in terms of parameters and results, and each one is free to call any other one, if the latter provides some useful computation that the former needs.
      </para>
      <para>
	To construct the actual object program of the operating system when this approach is used, one first compiles all the individual procedures, or files containing the procedures, and then binds them all together into a single object file using the system linker. In terms of information hiding, there is essentially noneevery procedure is visible to every other procedure (as opposed to a structure containing modules or packages, in which much of the information is hidden away inside modules, and only the officially designated entry points can be called from outside the module).
      </para>
      <para>
	Even in monolithic systems, however, it is possible to have at least a little structure. The services (system calls) provided by the operating system are requested by putting the parameters in well-defined places, such as in registers or on the stack, and then executing a special trap instruction known as a <command><emphasis>kernel call</emphasis></command> or <command><emphasis>supervisor call</emphasis></command>.
      </para>
      <para>
	This instruction switches the machine from user mode to kernel mode and transfers control to the operating system. (Most CPUs have two modes: kernel mode, for the operating system, in which all instructions are allowed; and user mode, for user programs, in which I/O and certain other instructions are not allowed.)
      </para>
      <para>
	This is a good time to look at how system calls are performed. Recall that the read call is used like this:
      </para>
      <para>
	<command>count = read(fd, buffer, nbytes);</command>
      </para>
      <para>
	In preparation for calling the <emphasis>read</emphasis> library procedure, which actually makes the <command>read</command> system call, the calling program first pushes the parameters onto the stack, as shown in steps 13 in <tag><link linkend="1-16">Fig. 1-16</link></tag>. C and C++ compilers push the parameters onto the stack in reverse order for historical reasons (having to do with making the first parameter to <emphasis>printf</emphasis>, the format string, appear on top of the stack). The first and third parameters are called by value, but the second parameter is passed by reference, meaning that the address of the buffer (indicated by &amp;) is passed, not the contents of the buffer. Then comes the actual call to the library procedure (step 4). This instruction is the normal procedure call instruction used to call all procedures.
      </para>

      <para id="1-16"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/1-16.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 1-16. The 11 steps in making the system call read(fd, buffer, nbytes).</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	The library procedure, possibly written in assembly language, typically puts the system call number in a place where the operating system expects it, such as a register (step 5). Then it executes a <command>trAP</command> instruction to switch from user mode to kernel mode and start execution at a fixed address within the kernel (step 6). The kernel code that starts examines the system call number and then dispatches to the correct system call handler, usually via a table of pointers to system call handlers indexed on system call number (step 7). At that point the system call handler runs (step 8). Once the system call handler has completed its work, control may be returned to the user-space library procedure at the instruction following the <command>trAP</command> instruction (step 9). This procedure then returns to the user program in the usual way procedure calls return (step 10).
      </para>
      <para>
	To finish the job, the user program has to clean up the stack, as it does after any procedure call (step 11). Assuming the stack grows downward, as it often does, the compiled code increments the stack pointer exactly enough to remove the parameters pushed before the call to read. The program is now free to do whatever it wants to do next.
      </para>
      <para>
	In step 9 above, we said "may be returned to the user-space library procedure" for good reason. The system call may block the caller, preventing it from continuing. For example, if it is trying to read from the keyboard and nothing has been typed yet, the caller has to be blocked. In this case, the operating system will look around to see if some other process can be run next. Later, when the desired input is available, this process will get the attention of the system and steps 911 will occur.
      </para>
      <para>
	This organization suggests a basic structure for the operating system:
      </para>
      <orderedlist>
	<listitem>
	  <para>
	    A main program that invokes the requested service procedure.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    A set of service procedures that carry out the system calls.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    A set of utility procedures that help the service procedures
	  </para>
	</listitem>
      </orderedlist>
      <para>
	In this model, for each system call there is one service procedure that takes care of it. The utility procedures do things that are needed by several service procedures, such as fetching data from user programs. This division of the procedures into three layers is shown in <tag><link linkend="1-17">Fig. 1-17</link></tag>.
      </para>

      <para id="1-17"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/1-17.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 1-17. A simple structuring model for a monolithic system.</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>
    </sect2>

    <sect2 id="sect-1.5.2">
      <title>1.5.2. Layered Systems</title>
      <para>
	A generalization of the approach of <tag><link linkend="1-17">Fig. 1-17</link></tag> is to organize the operating system as a hierarchy of layers, each one constructed upon the one below it. The first system constructed in this way was the THE system built at the Technische Hogeschool Eindhoven in the Netherlands by E. W. Dijkstra (1968) and his students. The THE system was a simple batch system for a Dutch computer, the Electrologica X8, which had 32K of 27-bit words (bits were expensive back then).
      </para>
      <para>
	The system had 6 layers, as shown in <tag><link linkend="1-18">Fig. 1-18</link></tag>. Layer 0 dealt with allocation of the processor, switching between processes when interrupts occurred or timers expired. Above layer 0, the system consisted of sequential processes, each of which could be programmed without having to worry about the fact that multiple processes were running on a single processor. In other words, layer 0 provided the basic multiprogramming of the CPU.
      </para>

      <para id="1-18"/>
      <table xml:id="ex.calstable2" frame="all">
	<tgroup cols="2" align="left" colsep="1" rowsep="1">
	  <tbody>
	    <row>
	      <entry>Layer</entry>
	      <entry>Function</entry>
	    </row>
	    <row>
	      <entry>5</entry>
	      <entry>The operator</entry>
	    </row>
	    <row>
	      <entry>4</entry>
	      <entry>User programs</entry>
	    </row>
	    <row>
	      <entry>3</entry>
	      <entry>Input/output management</entry>
	    </row>
	    <row>
	      <entry>2</entry>
	      <entry>Operator-process communication</entry>
	    </row>
	    <row>
	      <entry>1</entry>
	      <entry>Memory and drum management</entry>
	    </row>
	    <row>
	      <entry>0</entry>
	      <entry>Processor allocation and multiprogramming</entry>
	    </row>
	  </tbody>
	</tgroup>
      </table>
      <para>
	<command>Figure 1-18. Structure of the THE operating system.</command>
      </para>

      <para>
	Layer 1 did the memory management. It allocated space for processes in main memory and on a 512K word drum used for holding parts of processes (pages) for which there was no room in main memory. Above layer 1, processes did not have to worry about whether they were in memory or on the drum; the layer 1 software took care of making sure pages were brought into memory whenever they were needed.
      </para>
      <para>
	Layer 2 handled communication between each process and the operator console. Above this layer each process effectively had its own operator console. Layer 3 took care of managing the I/O devices and buffering the information streams to and from them. Above layer 3 each process could deal with abstract I/O devices with nice properties, instead of real devices with many peculiarities. Layer 4 was where the user programs were found. They did not have to worry about process, memory, console, or I/O management. The system operator process was located in layer 5.
      </para>
      <para>
	A further generalization of the layering concept was present in the MULTICS system. Instead of layers, MULTICS was organized as a series of concentric rings, with the inner ones being more privileged than the outer ones. When a procedure in an outer ring wanted to call a procedure in an inner ring, it had to make the equivalent of a system call, that is, a TRAP instruction whose parameters were carefully checked for validity before allowing the call to proceed. Although the entire operating system was part of the address space of each user process in MULTICS, the hardware made it possible to designate individual procedures (memory segments, actually) as protected against reading, writing, or executing.
      </para>
      <para>
	Whereas the THE layering scheme was really only a design aid, because all the parts of the system were ultimately linked together into a single object program, in MULTICS, the ring mechanism was very much present at run time and enforced by the hardware. The advantage of the ring mechanism is that it can easily be extended to structure user subsystems. For example, a professor could write a program to test and grade student programs and run this program in ring n, with the student programs running in ring n + 1 so that they could not change their grades. The Pentium hardware supports the MULTICS ring structure, but no major operating system uses it at present.
      </para>
    </sect2>

    <sect2 id="sect-1.5.3">
      <title>1.5.3. Virtual Machines</title>
      <para>
	The initial releases of OS/360 were strictly batch systems. Nevertheless, many 360 users wanted to have timesharing, so various groups, both inside and outside IBM decided to write timesharing systems for it. The official IBM timesharing system, TSS/360, was delivered late, and when it finally arrived it was so big and slow that few sites converted over to it. It was eventually abandoned after its development had consumed some $50 million (Graham, 1970). But a group at IBM's Scientific Center in Cambridge, Massachusetts, produced a radically different system that IBM eventually accepted as a product, and which is now widely used on its mainframes.
      </para>
      <para>
	This system, originally called CP/CMS and later renamed VM/370 (Seawright and MacKinnon, 1979), was based on a very astute observation: a timesharing system provides (1) multiprogramming and (2) an extended machine with a more convenient interface than the bare hardware. The essence of VM/370 is to completely separate these two functions.
      </para>
      <para>
The heart of the system, known as the <command><emphasis>virtual machine monitor</emphasis></command>, runs on the bare hardware and does the multiprogramming, providing not one, but several virtual machines to the next layer up, as shown in <tag><link linkend="1-19">Fig. 1-19</link></tag>. However, unlike all other operating systems, these virtual machines are not extended machines, with files and other nice features. Instead, they are <emphasis>exact</emphasis> copies of the bare hardware, including kernel/user mode, I/O, interrupts, and everything else the real machine has.
      </para>

      <para id="1-19"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/1-19.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 1-19. The structure of VM/370 with CMS.</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	Because each virtual machine is identical to the true hardware, each one can run any operating system that will run directly on the bare hardware. Different virtual machines can, and frequently do, run different operating systems. Some run one of the descendants of OS/360 for batch or transaction processing, while others run a single-user, interactive system called <command><emphasis>CMS</emphasis></command> (Conversational Monitor System) for timesharing users.
      </para>
      <para>
	When a CMS program executes a system call, the call is trapped to the operating-system in its own virtual machine, not to VM/370, just as it would if it were running on a real machine instead of a virtual one. CMS then issues the normal hardware I/O instructions for reading its virtual disk or whatever is needed to carry out the call. These I/O instructions are trapped by VM/370, which then performs them as part of its simulation of the real hardware. By making a complete separation of the functions of multiprogramming and providing an extended machine, each of the pieces can be much simpler, more flexible, and easier to maintain.
      </para>
      <para>
	The idea of a virtual machine is used nowadays in a different context: running old MS-DOS programs on a Pentium. When designing the Pentium and its software, both Intel and Microsoft realized that there would be a big demand for running old software on new hardware. For this reason, Intel provided a virtual 8086 mode on the Pentium. In this mode, the machine acts like an 8086 (which is identical to an 8088 from a software point of view), including 16-bit addressing with a 1-MB limit.
      </para>
      <para>
	This mode is used by Windows, and other operating systems for running old MS-DOS programs. These programs are started up in virtual 8086 mode. As long as they execute normal instructions, they run on the bare hardware. However, when a program tries to trap to the operating system to make a system call, or tries to do protected I/O directly, a trap to the virtual machine monitor occurs.
      </para>
      <para>
	Two variants on this design are possible. In the first one, MS-DOS itself is loaded into the virtual 8086's address space, so the virtual machine monitor just reflects the trap back to MS-DOS, just as would happen on a real 8086. When MS-DOS later tries to do the I/O itself, that operation is caught and carried out by the virtual machine monitor.
      </para>
      <para>
	In the other variant, the virtual machine monitor just catches the first trap and does the I/O itself, since it knows what all the MS-DOS system calls are and thus knows what each trap is supposed to do. This variant is less pure than the first one, since it emulates only MS-DOS correctly, and not other operating systems, as the first one does. On the other hand, it is much faster, since it saves the trouble of starting up MS-DOS to do the I/O. A further disadvantage of actually running MS-DOS in virtual 8086 mode is that MS-DOS fiddles around with the interrupt enable/disable bit quite a lot, all of which must be emulated at considerable cost.
      </para>
      <para>
	It is worth noting that neither of these approaches are really the same as VM/370, since the machine being emulated is not a full Pentium, but only an 8086. With the VM/370 system, it is possible to run VM/370, itself, in the virtual machine. Even the earliest versions of Windows require at least a 286 and cannot be run on a virtual 8086.
      </para>
      <para>
	Several virtual machine implementations are marketed commercially. For companies that provide web-hosting services, it can be more economical to run multiple virtual machines on a single fast server (perhaps one with multiple CPUs) than to run many small computers, each hosting a single Web site. VMWare and Microsoft's Virtual PC are marketed for such installations. These programs use large files on a host system as simulated disks for their guest systems. To achieve efficiency they analyze guest system program binaries and allow safe code to run directly on the host hardware, trapping instructions that make operating system calls. Such systems are also useful in education. For instance, students working on MINIX 3 lab assignments can work using MINIX 3 as a guest operating system on VMWare on a Windows, Linux or UNIX host with no risk of damaging other software installed on the same PC. Most professors teaching other subjects would be very nervous about sharing laboratory computers with an operating systems course where student mistakes could corrupt or erase disk data.
      </para>
      <para>
	Another are a where virtual machines are used, but in a somewhat different way, is for running Java programs. When Sun Microsystems invented the Java programming language, it also invented a virtual machine (i.e., a computer architecture) called the <command><emphasis>JVM</emphasis></command> (<command><emphasis>Java Virtual Machine</emphasis></command>). The Java compiler produces code for JVM, which then typically is executed by a software JVM interpreter. The advantage of this approach is that the JVM code can be shipped over the Internet to any computer that has a JVM interpreter and run there. If the compiler had produced SPARC or Pentium binary programs, for example, they could not have been shipped and run anywhere as easily. (Of course, Sun could have produced a compiler that produced SPARC binaries and then distributed a SPARC interpreter, but JVM is a much simpler architecture to interpret.) Another advantage of using JVM is that if the interpreter is implemented properly, which is not completely trivial, incoming JVM programs can be checked for safety and then executed in a protected environment so they cannot steal data or do any damage.
      </para>
    </sect2>

    <sect2 id="sect-1.5.4">
      <title>1.5.4. Exokernels</title>
      <para>
	With VM/370, each user process gets an exact copy of the actual computer. With virtual 8086 mode on the Pentium, each user process gets an exact copy of a different computer. Going one step further, researchers at M.I.T. built a system that gives each user a clone of the actual computer, but with a subset of the resources (Engler et al., 1995; and Leschke, 2004). Thus one virtual machine might get disk blocks 0 to 1023, the next one might get blocks 1024 to 2047, and so on.
      </para>
      <para>
	At the bottom layer, running in kernel mode, is a program called the <command><emphasis>exokernel</emphasis></command>. Its job is to allocate resources to virtual machines and then check attempts to use them to make sure no machine is trying to use somebody else's resources. Each user-level virtual machine can run its own operating system, as on VM/370 and the Pentium virtual 8086s, except that each one is restricted to using only the resources it has asked for and been allocated.
      </para>
      <para>
	The advantage of the exokernel scheme is that it saves a layer of mapping. In the other designs, each virtual machine thinks it has its own disk, with blocks running from 0 to some maximum, so the virtual machine monitor must maintain tables to remap disk addresses (and all other resources). With the exokernel, this remapping is not needed. The exokernel need only keep track of which virtual machine has been assigned which resource. This method still has the advantage of separating the multiprogramming (in the exokernel) from the user operating system code (in user space), but with less overhead, since all the exokernel has to do is keep the virtual machines out of each other's hair.
      </para>
    </sect2>

    <sect2 id="sect-1.5.5">
      <title>1.5.5. Client-Server Model</title>
      <para>
	VM/370 gains much in simplicity by moving a large part of the traditional operating system code (implementing the extended machine) into a higher layer, CMS. Nevertheless, VM/370 itself is still a complex program because simulating a number of virtual 370s is not that simple (especially if you want to do it reasonably efficiently).
      </para>
      <para>
	A trend in modern operating systems is to take this idea of moving code up into higher layers even further and remove as much as possible from the operating system, leaving a minimal <command><emphasis>kernel</emphasis></command>. The usual approach is to implement most of the operating system functions in user processes. To request a service, such as reading a block of a file, a user process (now known as the <command><emphasis>client process</emphasis></command>) sends the request to a <command><emphasis>server process</emphasis></command>, which then does the work and sends back the answer.
      </para>
      <para>
	In this model, shown in <tag><link linkend="1-20">Fig. 1-20</link></tag>, all the kernel does is handle the communication between clients and servers. By splitting the operating system up into parts, each of which only handles one facet of the system, such as file service, process service, terminal service, or memory service, each part becomes small and manageable. Furthermore, because all the servers run as user-mode processes, and not in kernel mode, they do not have direct access to the hardware. As a consequence, if a bug in the file server is triggered, the file service may crash, but this will not usually bring the whole machine down.
      </para>

      <para id="1-20"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/1-20.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 1-20. The client-server model.</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	Another advantage of the client-server model is its adaptability to use in distributed systems (see <tag><link linkend="1-21">Fig. 1-21</link></tag>). If a client communicates with a server by sending it messages, the client need not know whether the message is handled locally in its own machine, or whether it was sent across a network to a server on a remote machine. As far as the client is concerned, the same thing happens in both cases: a request was sent and a reply came back.
      </para>

      <para id="1-21"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/1-21.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 1-21. The client-server model in a distributed system.</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	The picture painted above of a kernel that handles only the transport of messages from clients to servers and back is not completely realistic. Some operating system functions (such as loading commands into the physical I/O device registers) are difficult, if not impossible, to do from user-space programs. There are two ways of dealing with this problem. One way is to have some critical server processes (e.g., I/O device drivers) actually run in kernel mode, with complete access to all the hardware, but still communicate with other processes using the normal message mechanism. A variant of this mechanism was used in earlier versions of MINIX where drivers were compiled into the kernel but ran as separate processes.
      </para>
      <para>
	The other way is to build a minimal amount of <command><emphasis>mechanism</emphasis></command> into the kernel but leave the <command><emphasis>policy</emphasis></command> decisions up to servers in user space. For example, the kernel might recognize that a message sent to a certain special address means to take the contents of that message and load it into the I/O device registers for some disk, to start a disk read. In this example, the kernel would not even inspect the bytes in the message to see if they were valid or meaningful; it would just blindly copy them into the disk's device registers. (Obviously, some scheme for limiting such messages to authorized processes only must be used.) This is how MINIX 3 works, drivers are in user space and use special kernel calls to request reads and writes of I/O registers or to access kernel information. The split between mechanism and policy is an important concept; it occurs again and again in operating systems in various contexts.
      </para>
    </sect2>
  </sect1>

  <sect1 id="sect-1.6">
    <title>1.6. Outline of the Rest of This Book</title>
    <para>
      Operating systems typically have four major components: process management, I/O device management, memory management, and file management. MINIX 3 is also divided into these four parts. The next four chapters deal with these four topics, one topic per chapter. <tag><xref linkend="Chapter6"/></tag> is a list of suggested readings and a bibliography.
    </para>
    <para>
      The chapters on processes, I/O, memory management, and file systems have the same general structure. First the general principles of the subject are laid out. Then comes an overview of the corresponding area of MINIX 3 (which also applies to UNIX). Finally, the MINIX 3 implementation is discussed in detail. The implementation section may be skimmed or skipped without loss of continuity by readers just interested in the principles of operating systems and not interested in the MINIX 3 code. Readers who are interested in finding out how a real operating system (MINIX 3) works should read all the sections.
    </para>
  </sect1>

  <sect1 id="sect-1.7">
    <title>1.7. Summary</title>
    <para>
      Operating systems can be viewed from two viewpoints: resource managers and extended machines. In the resource manager view, the operating system's job is to efficiently manage the different parts of the system. In the extended machine view, the job of the system is to provide the users with a virtual machine that is more convenient to use than the actual machine.
    </para>
    <para>
      Operating systems have a long history, starting from the days when they replaced the operator, to modern multiprogramming systems.
    </para>
    <para>
      The heart of any operating system is the set of system calls that it can handle. These tell what the operating system really does. For MINIX 3, these calls can be divided into six groups. The first group of system calls relates to process creation and termination. The second group handles signals. The third group is for reading and writing files. A fourth group is for directory management. The fifth group protects information, and the sixth group is about keeping track of time.
    </para>
    <para>
      Operating systems can be structured in several ways. The most common ones are as a monolithic system, as a hierarchy of layers, as a virtual machine system, using an exokernel, and using the client-server model.
    </para>
  </sect1>

  <sect1 id="sect-1-problems">
    <title>Problems</title>
    <orderedlist>
      <listitem>
	<para>
	  What are the two main functions of an operating system?
	</para>
      </listitem>
      <listitem>
	<para>
	  What is the difference between kernel mode and user mode? Why is the difference important to an operating system?
	</para>
      </listitem>
      <listitem>
	<para>
	  What is multiprogramming?
	</para>
      </listitem>
      <listitem>
	<para>
	  What is spooling? Do you think that advanced personal computers will have spooling as a standard feature in the future?
	</para>
      </listitem>
      <listitem>
	<para>
	  On early computers, every byte of data read or written was directly handled by the CPU (i.e., there was no DMADirect Memory Access). What implications does this organization have for multiprogramming?
	</para>
      </listitem>
      <listitem>
	<para>
	  Why was timesharing not widespread on second-generation computers?
	</para>
      </listitem>
      <listitem>
	<para>
	  Which of the following instructions should be allowed only in kernel mode?
	</para>
	<orderedlist inheritnum="ignore" numeration="loweralpha">
	  <listitem>
	    <para>
	      Disable all interrupts.
	    </para>
	  </listitem>
	  <listitem>
	    <para>
	      Read the time-of-day clock.
	    </para>
	  </listitem>
	  <listitem>
	    <para>
	      Set the time-of-day clock.
	    </para>
	  </listitem>
	  <listitem>
	    <para>
	      Change the memory map.
	    </para>
	  </listitem>
	</orderedlist>
      </listitem>
      <listitem>
	<para>
	  List some differences between personal computer operating systems and mainframe operating systems.
	</para>
      </listitem>
      <listitem>
	<para>
	  Give one reason why a closed-source proprietary operating system like Windows should have better quality than an open-source operating system like Linux. Now give one reason why an open-source operating system like Linux should have better quality than a closed-source proprietary operating system like Windows.
	</para>
      </listitem>
      <listitem>
	<para>
	  A MINIX file whose owner has UID = 12 and GID = 1 has mode <emphasis>rwxr-x---</emphasis>. Another user with UID = 6, GID = 1 tries to execute the file. What will happen?
	</para>
      </listitem>
      <listitem>
	<para>
	  In view of the fact that the mere existence of a superuser can lead to all kinds of security problems, why does such a concept exist?
	</para>
      </listitem>
      <listitem>
	<para>
	  All versions of UNIX support file naming using both absolute paths (relative to the root) and relative paths (relative to the working directory). Would it be possible to dispose of one of these and just use the other? If so, which would you suggest keeping?
	</para>
      </listitem>
      <listitem>
	<para>
	  Why is the process table needed in a timesharing system? Is it also needed in personal computer systems in which only one process exists, that process taking over the entire machine until it is finished?
	</para>
      </listitem>
      <listitem>
	<para>
	  What is the essential difference between a block special file and a character special file?
	</para>
      </listitem>
      <listitem>
	<para>
	  In MINIX 3 if user 2 links to a file owned by user 1, then user 1 removes the file, what happens when user 2 tries to read the file?
	</para>
      </listitem>
      <listitem>
	<para>
	  Are pipes an essential facility? Would major functionality be lost if they were not available?
	</para>
      </listitem>
      <listitem>
	<para>
	  Modern consumer appliances such as stereos and digital cameras often have a display where commands can be entered and the results of entering those commands can be viewed. These devices often have a primitive operating system inside. To what part of a personal computer software is the command processing via the stereo or camera's display similar to?
	</para>
      </listitem>
      <listitem>
	<para>
	  Windows does not have a <command>fork</command> system call, yet it is able to create new processes. Make an educated guess about the semantics of the system call Windows uses to create new processes.
	</para>
      </listitem>
      <listitem>
	<para>
	   Why is the <command>chroot</command> system call limited to the superuser?(Hint: Think about protection problems.)
	</para>
      </listitem>
      <listitem>
	<para>
	  Examine the list of system calls in <tag><link linkend="1-9">Fig. 1-9</link></tag>. Which call do you think is likely to execute most quickly. Explain your answer.
	</para>
      </listitem>
      <listitem>
	<para>
	  Suppose that a computer can execute 1 billion instructions/sec and that a system call takes 1000 instructions, including the trap and all the context switching. How many system calls can the computer execute per second and still have half the CPU capacity for running application code?
	</para>
      </listitem>
      <listitem>
	<para>
	  There is a <command>mknod</command> system call in <tag><link linkend="1-16">Fig. 1-16</link></tag> but there is no <command>rmnod</command> call. Does this mean that you have to be very, very careful about making nodes this way because there is no way to every remove them?
	</para>
      </listitem>
      <listitem>
	<para>
	   Why does MINIX 3 have the program update running in the background all the time?
	</para>
      </listitem>
      <listitem>
	<para>
	  Does it ever make any sense to ignore the SIGALRM signal?
	</para>
      </listitem>
      <listitem>
	<para>
	  The client-server model is popular in distributed systems. Can it also be used in a single-computer system?
	</para>
      </listitem>
      <listitem>
	<para>
	  The initial versions of the Pentium could not support a virtual machine monitor. What essential characteristic is needed to allow a machine to be virtualizable?
	</para>
      </listitem>
      <listitem>
	<para>
	  Write a program (or series of programs) to test all the MINIX 3 system calls. For each call, try various sets of parameters, including some incorrect ones, to see if they are detected.
	</para>
      </listitem>
      <listitem>
	<para>
	  Write a shell that is similar to <tag><link linkend="1-10">Fig. 1-10</link></tag> but contains enough code that it actually works so you can test it. You might also add some features such as redirection of input and output, pipes, and background jobs.
	</para>
      </listitem>
    </orderedlist>
  </sect1>

</chapter>
  <chapter xmlns:xl="http://www.w3.org/1999/xlink" id="Chapter2">
  <title>Processes</title>
  <para>
    We are now about to embark on a detailed study of how operating systems, in general, and MINIX 3, in particular, are designed and constructed. The most central concept in any operating system is the process: an abstraction of a running program. Everything else hinges on this concept, and it is important that the operating system designer (and student) understand this concept well.
  </para>

  <sect1 id="sect-2.1">
    <title>2.1. Introduction to Processes</title>
    <para>
      All modern computers can do several things at the same time. While running a user program, a computer can also be reading from a disk and outputting text to a screen or printer. In a multiprogramming system, the CPU also switches from program to program, running each for tens or hundreds of milliseconds. While, strictly speaking, at any instant of time, the CPU is running only one program, in the course of 1 second, it may work on several programs, thus giving the users the illusion of parallelism. Sometimes people speak of <command><emphasis>pseudoparallelism</emphasis></command> in this context, to contrast it with the true hardware parallelism of <command><emphasis>multiprocessor</emphasis></command> systems (which have two or more CPUs sharing the same physical memory). Keeping track of multiple, parallel activities is hard for people to do. Therefore, operating system designers over the years have evolved a conceptual model (sequential processes) that makes parallelism easier to deal with. That model, its uses, and some of its consequences form the subject of this chapter.
    </para>

    <sect2 id="sect-2.1.1">
      <title>2.1.1. The Process Model</title>
      <para>
	In this model, all the runnable software on the computer, sometimes including the operating system, is organized into a number of <command><emphasis>sequential processes</emphasis></command>, or just <command><emphasis>processes</emphasis></command> for short. A process is just an executing program, including the current values of the program counter, registers, and variables. Conceptually, each process has its own virtual CPU. In reality, of course, the real CPU switches back and forth from process to process, but to understand the system, it is much easier to think about a collection of processes running in (pseudo) parallel, than to try to keep track of how the CPU switches from program to program. This rapid switching back and forth is called <command><emphasis>multiprogramming</emphasis></command>, as we saw in <tag><xref linkend="Chapter1"/></tag>.
      </para>
      <para>
	In <tag><link linkend="2-1">Fig. 2-1(a)</link></tag> we see a computer multiprogramming four programs in memory. In <tag><link linkend="2-1">Fig. 2-1(b)</link></tag> we see four processes, each with its own flow of control (i.e., its own program counter), and each one running independently of the other ones. Of course, there is only one physical program counter, so when each process runs, its logical program counter is loaded into the real program counter. When it is finished for the time being, the physical program counter is saved in the process' logical program counter in memory. In <tag><link linkend="2-1">Fig. 2-1(c)</link></tag> we see that viewed over a long enough time interval, all the processes have made progress, but at any given instant only one process is actually running.
      </para>

      <para id="2-1"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/2-1.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 2-1. (a) Multiprogramming of four programs. (b) Conceptual model of four independent, sequential processes. (c) Only one program is active at any instant.</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	With the CPU switching back and forth among the processes, the rate at which a process performs its computation will not be uniform, and probably not even reproducible if the same processes are run again. Thus, processes must not be programmed with built-in assumptions about timing. Consider, for example, an I/O process that starts a streamer tape to restore backed up files, executes an idle loop 10,000 times to let it get up to speed, and then issues a command to read the first record. If the CPU decides to switch to another process during the idle loop, the tape process might not run again until after the first record was already past the read head. When a process has critical real-time requirements like this, that is, particular events must occur within a specified number of milliseconds, special measures must be taken to ensure that they do occur. Normally, however, most processes are not affected by the underlying multiprogramming of the CPU or the relative speeds of different processes.
      </para>
      <para>
	The difference between a process and a program is subtle, but crucial. An analogy may help make this point clearer. Consider a culinary-minded computer scientist who is baking a birthday cake for his daughter. He has a birthday cake recipe and a kitchen well stocked with the necessary input: flour, eggs, sugar, extract of vanilla, and so on. In this analogy, the recipe is the program (i.e., an algorithm expressed in some suitable notation), the computer scientist is the processor (CPU), and the cake ingredients are the input data. The process is the activity consisting of our baker reading the recipe, fetching the ingredients, and baking the cake.
      </para>
      <para>
	Now imagine that the computer scientist's son comes running in crying, saying that he has been stung by a bee. The computer scientist records where he was in the recipe (the state of the current process is saved), gets out a first aid book, and begins following the directions in it. Here we see the processor being switched from one process (baking) to a higher priority process (administering medical care), each having a different program (recipe vs. first aid book). When the bee sting has been taken care of, the computer scientist goes back to his cake, continuing at the point where he left off.
      </para>
      <para>
	The key idea here is that a process is an activity of some kind. It has a program, input, output, and a state. A single processor may be shared among several processes, with some scheduling algorithm being used to determine when to stop work on one process and service a different one.
      </para>
    </sect2>

    <sect2 id="sect-2.1.2">
      <title>2.1.2. Process Creation</title>
      <para>
	Operating systems need some way to make sure all the necessary processes exist. In very simple systems, or in systems designed for running only a single application (e.g., controlling a device in real time), it may be possible to have all the processes that will ever be needed be present when the system comes up. In general-purpose systems, however, some way is needed to create and terminate processes as needed during operation. We will now look at some of the issues.
      </para>
      <para>
	There are four principal events that cause processes to be created:
      </para>
      <orderedlist>
	<listitem>
	  <para>
	    System initialization.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    Execution of a process creation system call by a running process.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    A user request to create a new process.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    Initiation of a batch job.
	  </para>
	</listitem>
      </orderedlist>

      <para>
	When an operating system is booted, often several processes are created. Some of these are foreground processes, that is, processes that interact with (human) users and perform work for them. Others are background processes, which are not associated with particular users, but instead have some specific function. For example, a background process may be designed to accept incoming requests for web pages hosted on that machine, waking up when a request arrives to service the request. Processes that stay in the background to handle some activity such as web pages, printing, and so on are called <command><emphasis>daemons</emphasis></command>. Large systems commonly have dozens of them. In MINIX 3, the <emphasis>ps</emphasis> program can be used to list the running processes.
      </para>
      <para>
	In addition to the processes created at boot time, new processes can be created afterward as well. Often a running process will issue system calls to create one or more new processes to help it do its job. Creating new processes is particularly useful when the work to be done can easily be formulated in terms of several related, but otherwise independent interacting processes. For example, when compiling a large program, the <emphasis>make</emphasis> program invokes the C compiler to convert source files to object code, and then it invokes the <command><emphasis>install</emphasis></command> program to copy the program to its destination, set ownership and permissions, etc. In MINIX 3, the C compiler itself is actually several different programs, which work together. These include a preprocessor, a C language parser, an assembly language code generator, an assembler, and a linker.
      </para>
      <para>
	In interactive systems, users can start a program by typing a command. In MINIX 3, virtual consoles allow a user to start a program, say a compiler, and then switch to an alternate console and start another program, perhaps to edit documentation while the compiler is running.
      </para>
      <para>
	The last situation in which processes are created applies only to the batch systems found on large mainframes. Here users can submit batch jobs to the system (possibly remotely). When the operating system decides that it has the resources to run another job, it creates a new process and runs the next job from the input queue in it.
      </para>
      <para>
	Technically, in all these cases, a new process is created by having an existing process execute a process creation system call. That process may be a running user process, a system process invoked from the keyboard or mouse, or a batch manager process. What that process does is execute a system call to create the new process. This system call tells the operating system to create a new process and indicates, directly or indirectly, which program to run in it.
      </para>
      <para>
	In MINIX 3, there is only one system call to create a new process: <command>fork</command>. This call creates an exact clone of the calling process. After the <command>fork</command>, the two processes, the parent and the child, have the same memory image, the same environment strings, and the same open files. That is all there is. Usually, the child process then executes <command>execve</command> or a similar system call to change its memory image and run a new program. For example, when a user types a command, say, <emphasis>sort</emphasis>, to the shell, the shell forks off a child process and the child executes <emphasis>sort</emphasis>. The reason for this two-step process is to allow the child to manipulate its file descriptors after the <command>fork</command> but before the <command>execve</command> to accomplish redirection of standard input, standard output, and standard error.
      </para>
      <para>
In both MINIX 3 and UNIX, after a process is created both the parent and child have their own distinct address spaces. If either process changes a word in its address space, the change is not visible to the other process. The child's initial address space is a copy of the parent's, but there are two distinct address spaces involved; no writable memory is shared (like some UNIX implementations, MINIX 3 can share the program text between the two since that cannot be modified). It is, however, possible for a newly created process to share some of its creator's other resources, such as open files.
      </para>
    </sect2>

    <sect2 id="sect-2.1.3">
      <title>2.1.3. Process Termination</title>
      <para>
After a process has been created, it starts running and does whatever its job is. However, nothing lasts forever, not even processes. Sooner or later the new process will terminate, usually due to one of the following conditions:
      </para>
      <orderedlist>
	<listitem>
	  <para>
	    Normal exit (voluntary).
	  </para>
	</listitem>
	<listitem>
	  <para>
	    Error exit (voluntary).
	  </para>
	</listitem>
	<listitem>
	  <para>
	    Fatal error (involuntary).
	  </para>
	</listitem>
	<listitem>
	  <para>
	    Killed by another process (involuntary). 
	  </para>
	</listitem>
      </orderedlist>

      <para>
Most processes terminate because they have done their work. When a compiler has compiled the program given to it, the compiler executes a system call to tell the operating system that it is finished. This call is <command>exit</command> in MINIX 3. Screen-oriented programs also support voluntary termination. For instance, editors always have a key combination that the user can invoke to tell the process to save the working file, remove any temporary files that are open and terminate.
      </para>
      <para>
	The second reason for termination is that the process discovers a fatal error. For example, if a user types the command
      </para>
      <para>
	<command>cc foo.c</command>
      </para>
      <para>
	to compile the program <emphasis>foo.c</emphasis> and no such file exists, the compiler simply exits.
      </para>
      <para>
	The third reason for termination is an error caused by the process, perhaps due to a program bug. Examples include executing an illegal instruction, referencing nonexistent memory, or dividing by zero. In MINIX 3, a process can tell the operating system that it wishes to handle certain errors itself, in which case the process is signaled (interrupted) instead of terminated when one of the errors occurs.
      </para>
      <para>
	The fourth reason a process might terminate is that one process executes a system call telling the operating system to kill some other process. In MINIX 3, this call is <command>kill</command>. Of course, the killer must have the necessary authorization to do in the killee. In some systems, when a process terminates, either voluntarily or otherwise, all processes it created are immediately killed as well. MINIX 3 does not work this way, however.
      </para>
    </sect2>

    <sect2 id="sect-2.1.4">
      <title>2.1.4. Process Hierarchies</title>
      <para>
	In some systems, when a process creates another process, the parent and child continue to be associated in certain ways. The child can itself create more processes, forming a process hierarchy. Unlike plants and animals that use sexual reproduction, a process has only one parent (but zero, one, two, or more children).
      </para>
      <para>
	In MINIX 3, a process, its children, and further descendants together may form a process group. When a user sends a signal from the keyboard, the signal may be delivered to all members of the process group currently associated with the keyboard (usually all processes that were created in the current window). This is signal-dependent. If a signal is sent to a group, each process can catch the signal, ignore the signal, or take the default action, which is to be killed by the signal.
      </para>
      <para>
	As a simple example of how process trees are used, let us look at how MINIX 3 initializes itself. Two special processes, the <command><emphasis>reincarnation server</emphasis></command> and <command><emphasis>init</emphasis></command> are present in the boot image. The reincarnation server's job is to (re)start drivers and servers. It begins by blocking, waiting for a message telling it what to create.
      </para>
      <para>
	In contrast, <emphasis>init</emphasis> executes the <emphasis>/etc/rc</emphasis> script that causes it to issue commands to the reincarnation server to start the drivers and servers not present in the boot image. This procedure makes the drivers and servers so started children of the reincarnation server, so if any of them ever terminate, the reincarnation server will be informed and can restart (i.e., reincarnate) them again. This mechanism is intended to allow MINIX 3 to tolerate a driver or server crash because a new one will be started automatically. In practice, replacing a driver is much easier than replacing a server, however, since there fewer repercussions elsewhere in the system. (And, we do not say this always works perfectly; it is still work in progress.)
      </para>
      <para>
	When <emphasis>init</emphasis> has finished this, it reads a configuration file <emphasis>/etc/ttytab</emphasis>) to see which terminals and virtual terminals exist. <emphasis>Init</emphasis> <command>fork</command>s a <emphasis>getty</emphasis> process for each one, displays a login prompt on it, and then waits for input. When a name is typed, <emphasis>getty</emphasis> <command>exec</command>s a <emphasis>login</emphasis> process with the name as its argument. If the user succeeds in logging in, <emphasis>login</emphasis> will <command>exec</command> the user's shell. So the shell is a child of <emphasis>init</emphasis>. User commands create children of the shell, which are grandchildren of <emphasis>init</emphasis>. This sequence of events is an example of how process trees are used. As an aside, the code for the reincarnation server and <emphasis>init</emphasis> is not listed in this book; neither is the shell. The line had to be drawn somewhere. But now you have the basic idea.
      </para>
    </sect2>

    <sect2 id="sect-2.1.5">
      <title>2.1.5. Process States</title>
      <para>
	Although each process is an independent entity, with its own program counter registers, stack, open files, alarms, and other internal state, processes often need to interact, communicate, and synchronize with other processes. One process may generate some output that another process uses as input, for example. In that case, the data needs to be moved between processes. In the shell command
      </para>
      <para>
	<command>cat chapter1 chapter2 chapter3 | grep tree</command>
      </para>
      <para>
	the first process, running <emphasis>cat</emphasis>, concatenates and outputs three files. The second process, running <emphasis>grep</emphasis>, selects all lines containing the word "tree." Depending on the relative speeds of the two processes (which depends on both the relative complexity of the programs and how much CPU time each one has had), it may happen that <emphasis>grep</emphasis> is ready to run, but there is no input waiting for it. It must then <command><emphasis>block</emphasis></command> until some input is available.
      </para>
      <para>
	When a process blocks, it does so because logically it cannot continue, typically because it is waiting for input that is not yet available. It is also possible for a process that is conceptually ready and able to run to be stopped because the operating system has decided to allocate the CPU to another process for a while. These two conditions are completely different. In the first case, the suspension is inherent in the problem (you cannot process the user's command line until it has been typed). In the second case, it is a technicality of the system (not enough CPUs to give each process its own private processor). In <tag><link linkend="2-2">Fig. 2-2</link></tag> we see a state diagram showing the three states a process may be in:
      </para>
      <orderedlist>
	<listitem>
	  <para>
	    Running (actually using the CPU at that instant).
	  </para>
	</listitem>
	<listitem>
	  <para>
	    Ready (runnable; temporarily stopped to let another process run).
	  </para>
	</listitem>
	<listitem>
	  <para>
	    Blocked (unable to run until some external event happens).
	  </para>
	</listitem>
      </orderedlist>

      <para id="2-2"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/2-2.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 2-2. A process can be in running, blocked, or ready state. Transitions between these states are as shown.</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	Logically, the first two states are similar. In both cases the process is willing to run, only in the second one, there is temporarily no CPU available for it. The third state is different from the first two in that the process cannot run, even if the CPU has nothing else to do.
      </para>
      <para>
	Four transitions are possible among these three states, as shown. Transition 1 occurs when a process discovers that it cannot continue. In some systems the process must execute a system call, <command>block</command> or <command>pause</command> to get into blocked state. In other systems, including MINIX 3, when a process reads from a pipe or special file (e.g., a terminal) and there is no input available, the process is automatically moved from the running state to the blocked state.
      </para>
      <para>
	Transitions 2 and 3 are caused by the process scheduler, a part of the operating-system, without the process even knowing about them. Transition 2 occurs when the scheduler decides that the running process has run long enough, and it is time to let another process have some CPU time. Transition 3 occurs when all the other processes have had their fair share and it is time for the first process to get the CPU to run again. The subject of schedulingdeciding which process should run when and for how longis an important one. Many algorithms have been devised to try to balance the competing demands of efficiency for the system as a whole and fairness to individual processes. We will look at scheduling and study some of these algorithms later in this chapter.
      </para>
      <para>
	Transition 4 occurs when the external event for which a process was waiting (e.g., the arrival of some input) happens. If no other process is running then, transition 3 will be triggered immediately, and the process will start running. Otherwise it may have to wait in <emphasis>ready</emphasis> state for a little while until the CPU is available.
      </para>
      <para>
	Using the process model, it becomes much easier to think about what is going on inside the system. Some of the processes run programs that carry out commands typed in by a user. Other processes are part of the system and handle tasks such as carrying out requests for file services or managing the details of running a disk or a tape drive. When a disk interrupt occurs, the system may make a decision to stop running the current process and run the disk process, which was blocked waiting for that interrupt. We say "may" because it depends upon relative priorities of the running process and the disk driver process. But the point is that instead of thinking about interrupts, we can think about user processes, disk processes, terminal processes, and so on, which block when they are waiting for something to happen. When the disk block has been read or the character typed, the process waiting for it is unblocked and is eligible to run again.
      </para>
      <para>
	This view gives rise to the model shown in <tag><link linkend="2-3">Fig. 2-3</link></tag>. Here the lowest level of the operating system is the scheduler, with a variety of processes on top of it. All the interrupt handling and details of actually starting and stopping processes are hidden away in the scheduler, which is actually quite small. The rest of the operating system is nicely structured in process form. The model of <tag><link linkend="2-3">Fig. 2-3</link></tag> is used in MINIX 3. Of course, the "scheduler" is not the only thing in the lowest layer, there is also support for interrupt handling and interprocess communication. Nevertheless, to a first approximation, it does show the basic structure.
      </para>

      <para id="2-3"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/2-3.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 2-3. The lowest layer of a process-structured operating system handles interrupts and scheduling. Above that layer are sequential processes.</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>
    </sect2>

    <sect2 id="sect-2.1.6">
      <title>2.1.6. Implementation of Processes</title>
      <para>
To implement the process model, the operating system maintains a table (an array of structures), called the <command><emphasis>process table</emphasis></command>, with one entry per process. (Some authors call these entries <command><emphasis>process control blocks</emphasis></command>.) This entry contains information about the process' state, its program counter, stack pointer, memory allocation, the status of its open files, its accounting and scheduling information, alarms and other signals, and everything else about the process that must be saved when the process is switched from <emphasis>running</emphasis> to <emphasis>ready</emphasis> state so that it can be restarted later as if it had never been stopped.
      </para>
      <para>
In MINIX 3, interprocess communication, memory management, and file management are each handled by separate modules within the system, so the process table is partitioned, with each module maintaining the fields that it needs. <tag><link linkend="2-4">Figure 2-4</link></tag> shows some of the more important fields. The fields in the first column are the only ones relevant to this chapter. The other two columns are provided just to give an idea of what information is needed elsewhere in the system.
      </para>

      <para/>
      <table xml:id="ex.calstable3" frame="all">
	<tgroup cols="3" align="left" colsep="1" rowsep="1">
	  <tbody>
	    <row>
	      <entry>
		<para><command>Kernel</command></para>
		<para>Registers</para>
		<para>Program counter</para>
		<para>Program status word</para>
		<para>Stack pointer</para>
		<para>Process state</para>
		<para>Current scheduling priority</para>
		<para>Maximum scheduling priority</para>
		<para>Scheduling ticks left</para>
		<para>Quantum size</para>
		<para>CPU time used</para>
		<para>Message queue pointers</para>
		<para>Pending signal bits</para>
		<para>Various flag bits</para>
		<para>Process name</para>
	      </entry>
	      <entry>
		<para><command>Process management</command></para>
		<para>Pointer to text segment</para>
		<para>Pointer to data segment</para>
		<para>Pointer to bss segment</para>
		<para>Exit status</para>
		<para>Signal status</para>
		<para>Process ID</para>
		<para>Parent process</para>
		<para>Process group</para>
		<para>Children's CPU time</para>
		<para>Real UID</para>
		<para>Effective UID</para>
		<para>Real GID</para>
		<para>Effective GID</para>
		<para>File info for sharing text</para>
		<para>Bitmaps for signals</para>
		<para>Various flag bits</para>
		<para>Process name</para>
	      </entry>
	      <entry>
		<para><command>File management</command></para>
		<para>UMASK mask</para>
		<para>Root directory</para>
		<para>Working directory</para>
		<para>File descriptors</para>
		<para>Real id</para>
		<para>Effective UID</para>
		<para>Real GID</para>
		<para>Effective GID</para>
		<para>Controlling tty</para>
		<para>Save area for read/write</para>
		<para>System call parameters</para>
		<para>Various flag bits</para>
		<para/>
		<para/>
		<para/>
		<para/>
		<para/>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </table>
      <para id="2-4">
	<command>Figure 2-4. Some of the fields of the MINIX 3 process table. The fields are distributed over the kernel, the process manager, and the file system.</command>
      </para>

      <para>
	Now that we have looked at the process table, it is possible to explain a little more about how the illusion of multiple sequential processes is maintained on a machine with one CPU and many I/O devices. What follows is technically a description of how the "scheduler" of <tag><link linkend="2-3">Fig. 2-3</link></tag> works in MINIX 3 but most modern operating systems work essentially the same way. Associated with each I/O device class (e.g., floppy disks, hard disks, timers, terminals) is a data structure in a table called the <command><emphasis>interrupt descriptor table</emphasis></command>. The most important part of each entry in this table is called the <command><emphasis>interrupt vector</emphasis></command>. It contains the address of the interrupt service procedure. Suppose that user process 23 is running when a disk interrupt occurs. The program counter, program status word, and possibly one or more registers are pushed onto the (current) stack by the interrupt hardware. The computer then jumps to the address specified in the disk interrupt vector. That is all the hardware does. From here on, it is up to the software.
      </para>
      <para>
	The interrupt service procedure starts out by saving all the registers in the process table entry for the current process. The current process number and a pointer to its entry are kept in global variables so they can be found quickly. Then the information deposited by the interrupt is removed from the stack, and the stack pointer is set to a temporary stack used by the process handler. Actions such as saving the registers and setting the stack pointer cannot even be expressed in high-level languages such as C, so they are performed by a small assembly language routine. When this routine is finished, it calls a C procedure to do the rest of the work for this specific interrupt type.
      </para>
      <para>
	Interprocess communication in MINIX 3 is via messages, so the next step is to build a message to be sent to the disk process, which will be blocked waiting for it. The message says that an interrupt occurred, to distinguish it from messages from user processes requesting disk blocks to be read and things like that. The state of the disk process is now changed from <emphasis>blocked</emphasis> to <emphasis>ready</emphasis> and the scheduler is called. In MINIX 3, different processes have different priorities, to give better service to I/O device handlers than to user processes, for example. If the disk process is now the highest priority runnable process, it will be scheduled to run. If the process that was interrupted is just as important or more so, then it will be scheduled to run again, and the disk process will have to wait a little while.
      </para>
      <para>
	Either way, the C procedure called by the assembly language interrupt code now returns, and the assembly language code loads up the registers and memory map for the now-current process and starts it running. Interrupt handling and scheduling are summarized in <tag><link linkend="2-5">Fig. 2-5</link></tag>. It is worth noting that the details vary slightly from system to system.
      </para>

      <para id="2-5"/>
      <orderedlist>
	<listitem>
	  <para>
	    Hardware stacks program counter, etc.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    Hardware loads new program counter from interrupt vector.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    Assembly language procedure saves registers.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    Assembly language procedure sets up new stack.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    C interrupt service constructs and sends message.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    Message passing code marks waiting message recipient ready.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    Scheduler decides which process is to run next.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    C procedure returns to the assembly code.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    Assembly language procedure starts up new current process.
	  </para>
	</listitem>
      </orderedlist>
      <para>
	<command>Figure 2-5. Skeleton of what the lowest level of the operating system does when an interrupt occurs.</command>
      </para>
    </sect2>

    <sect2 id="sect-2.1.7">
      <title>2.1.7. Threads</title>
      <para>
	In traditional operating systems, each process has an address space and a single thread of control. In fact, that is almost the definition of a process. Nevertheless, there are often situations in which it is desirable to have multiple threads of control in the same address space running in quasi-parallel, as though they were separate processes (except for the shared address space). These threads of control are usually just called <command><emphasis>threads</emphasis></command>, although some people call them <command><emphasis>lightweight processes</emphasis></command>.
      </para>
      <para>
	One way of looking at a process is that it is a way to group related resources together. A process has an address space containing program text and data, as well as other resources. These resources may include open files, child processes, pending alarms, signal handlers, accounting information, and more. By putting them together in the form of a process, they can be managed more easily.
      </para>
      <para>
	The other concept a process has is a thread of execution, usually shortened to just <command><emphasis>thread</emphasis></command>. The thread has a program counter that keeps track of which instruction to execute next. It has registers, which hold its current working variables. It has a stack, which contains the execution history, with one frame for each procedure called but not yet returned from. Although a thread must execute in some process, the thread and its process are different concepts and can be treated separately. Processes are used to group resources together; threads are the entities scheduled for execution on the CPU.
      </para>
      <para>
What threads add to the process model is to allow multiple executions to take place in the same process environment, to a large degree independent of one another. In <tag><link linkend="2-6">Fig. 2-6(a)</link></tag> we see three traditional processes. Each process has its own address space and a single thread of control. In contrast, in <tag><link linkend="2-6">Fig. 2-6(b)</link></tag> we see a single process with three threads of control. Although in both cases we have three threads, in <tag><link linkend="2-6">Fig. 2-6(a)</link></tag> each of them operates in a different address space, whereas in <tag><link linkend="2-6">Fig. 2-6(b)</link></tag> all three of them share the same address space.
      </para>

      <para id="2-6"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/2-6.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 2-6. (a) Three processes each with one thread. (b) One process with three threads.</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	As an example of where multiple threads might be used, consider a web browser process. Many web pages contain multiple small images. For each image on a web page, the browser must set up a separate connection to the page's home site and request the image. A great deal of time is spent establishing and releasing all these connections. By having multiple threads within the browser, many images can be requested at the same time, greatly speeding up performance in most cases since with small images, the set-up time is the limiting factor, not the speed of the transmission line.
      </para>
      <para>
	When multiple threads are present in the same address space, a few of the fields of <tag><link linkend="2-4">Fig. 2-4</link></tag> are not per process, but per thread, so a separate thread table is needed, with one entry per thread. Among the per-thread items are the program counter, registers, and state. The program counter is needed because threads, like processes, can be suspended and resumed. The registers are needed because when threads are suspended, their registers must be saved. Finally, threads, like processes, can be in <emphasis>running</emphasis>, <emphasis>ready</emphasis>, or <emphasis>blocked</emphasis> state. <tag><link linkend="2-7">Fig. 2-7</link></tag> lists some per-process and per-thread items.
      </para>

      <para/>
      <table xml:id="ex.calstable4" frame="all">
	<tgroup cols="2" align="left" colsep="1" rowsep="1">
	  <tbody>
	    <row>
	      <entry>
		<para>
		  <command>Per process items</command>
		</para>
		<para>
		  Address space
		</para>
		<para>
		  Global variables
		</para>
		<para>
		  Open files
		</para>
		<para>
		  Child processes
		</para>
		<para>
		  Pending alarms
		</para>
		<para>
		  Signals and signal handlers
		</para>
		<para>
		  Accounting information
		</para>
	      </entry>
	      <entry>
		<para>
		  <command>Per thread items</command>
		</para>
		<para>
		  Program counter
		</para>
		<para>
		  Registers
		</para>
		<para>
		  Stack
		</para>
		<para>
		  State
		</para>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </table>
      <para id="2-7">
	<command>Figure 2-7. The first column lists some items shared by all threads in a process. The second one lists some items private to each thread.</command>
      </para>

      <para>
	In some systems, the operating system is not aware of the threads. In other words, they are managed entirely in user space. When a thread is about to block, for example, it chooses and starts its successor before stopping. Several userlevel threads packages are in common use, including the POSIX <command><emphasis>P-threads</emphasis></command> and Mach <command><emphasis>C-threads</emphasis></command> packages.
      </para>
      <para>
	In other systems, the operating system is aware of the existence of multiple threads per process, so when a thread blocks, the operating system chooses the next one to run, either from the same process or a different one. To do scheduling, the kernel must have a thread table that lists all the threads in the system, analogous to the process table.
      </para>
      <para>
	Although these two alternatives may seem equivalent, they differ considerably in performance. Switching threads is much faster when thread management is done in user space than when a system call is needed. This fact argues strongly for doing thread management in user space. On the other hand, when threads are managed entirely in user space and one thread blocks (e.g., waiting for I/O or a page fault to be handled), the kernel blocks the entire process, since it is not even aware that other threads exist. This fact as well as others argue for doing thread management in the kernel (Boehm, 2005). As a consequence, both systems are in use, and various hybrid schemes have been proposed as well (Anderson et al., 1992).
      </para>
      <para>
	No matter whether threads are managed by the kernel or in user space, they introduce a raft of problems that must be solved and which change the programming model appreciably. To start with, consider the effects of the <command>fork</command> system call. If the parent process has multiple threads, should the child also have them? If not, the process may not function properly, since all of them may be essential.
      </para>
      <para>
	However, if the child process gets as many threads as the parent, what happens if a thread was blocked on a <command>read</command> call, say, from the keyboard? Are two threads now blocked on the keyboard? When a line is typed, do both threads get a copy of it? Only the parent? Only the child? The same problem exists with open network connections.
      </para>
      <para>
	Another class of problems is related to the fact that threads share many data structures. What happens if one thread closes a file while another one is still reading from it? Suppose that one thread notices that there is too little memory and starts allocating more memory. Then, part way through, a thread switch occurs, and the new thread also notices that there is too little memory and also starts allocating more memory. Does the allocation happen once or twice? In nearly all systems that were not designed with threads in mind, the libraries (such as the memory allocation procedure) are not reentrant, and will crash if a second call is made while the first one is still active.
      </para>
      <para>
	Another problem relates to error reporting. In UNIX, after a system call, the status of the call is put into a global variable, <emphasis>errno</emphasis>. What happens if a thread makes a system call, and before it is able to read <emphasis>errno</emphasis>, another thread makes a system call, wiping out the original value?
      </para>
      <para>
	Next, consider signals. Some signals are logically thread specific; others are not. For example, if a thread calls <command>alarm</command>, it makes sense for the resulting signal to go to the thread that made the call. When the kernel is aware of threads, it can usually make sure the right thread gets the signal. When the kernel is not aware of threads, the threads package must keep track of alarms by itself. An additional complication for user-level threads exists when (as in UNIX) a process may only have one alarm at a time pending and several threads call <command>alarm</command> independently.
      </para>
      <para>
	Other signals, such as a keyboard-initiated <emphasis>SIGINT</emphasis>, are not thread specific. Who should catch them? One designated thread? All the threads? A newly created thread? Each of these solutions has problems. Furthermore, what happens if one thread changes the signal handlers without telling other threads?
      </para>
      <para>
	One last problem introduced by threads is stack management. In many systems, when stack overflow occurs, the kernel just provides more stack, automatically. When a process has multiple threads, it must also have multiple stacks. If the kernel is not aware of all these stacks, it cannot grow them automatically upon stack fault. In fact, it may not even realize that a memory fault is related to stack growth.
      </para>
      <para>
	These problems are certainly not insurmountable, but they do show that just introducing threads into an existing system without a fairly substantial system redesign is not going to work at all. The semantics of system calls have to be redefined and libraries have to be rewritten, at the very least. And all of these things must be done in such a way as to remain backward compatible with existing programs for the limiting case of a process with only one thread. For additional information about threads, see Hauser et al. (1993) and Marsh et al. (1991).
      </para>
    </sect2>
  </sect1>

  <sect1 id="sect-2.2">
    <title>2.2. Interprocess Communication</title>
    <para>
      Processes frequently need to communicate with other processes. For example, in a shell pipeline, the output of the first process must be passed to the second process, and so on down the line. Thus there is a need for communication between processes, preferably in a well-structured way not using interrupts. In the following sections we will look at some of the issues related to this <command><emphasis>InterProcess Communication</emphasis></command> or <command><emphasis>IPC</emphasis></command>.
    </para>
    <para>
      There are three issues here. The first was alluded to above: how one process can pass information to another. The second has to do with making sure two or more processes do not get into each other's way when engaging in critical activities (suppose two processes each try to grab the last 1 MB of memory). The third concerns proper sequencing when dependencies are present: if process A produces data and process B prints it, B has to wait until A has produced some data before starting to print. We will examine all three of these issues in some detail in this section.
    </para>
    <para>
      It is also important to mention that two of these issues apply equally well to threads. The first onepassing informationis easy for threads since they share a common address space (threads in different address spaces that need to communicate fall under the heading of communicating processes). However, the other twokeeping out of each other's hair and proper sequencingapply as well to threads. The same problems exist and the same solutions apply. Below we will discuss the problem in the context of processes, but please keep in mind that the same problems and solutions also apply to threads.
    </para>

    <sect2 id="sect-2.2.1">
      <title>2.2.1. Race Conditions</title>
      <para>
	In some operating systems, processes that are working together may share some common storage that each one can read and write. The shared storage may be in main memory (possibly in a kernel data structure) or it may be a shared file; the location of the shared memory does not change the nature of the communication or the problems that arise. To see how interprocess communication works in practice, let us consider a simple but common example, a print spooler. When a process wants to print a file, it enters the file name in a special <command><emphasis>spooler directory</emphasis></command>. Another process, the <command><emphasis>printer daemon</emphasis></command>, periodically checks to see if so are any files to be printed, and if so removes their names from the directory.
      </para>
      <para>
	Imagine that our spooler directory has a large number of slots, numbered 0, 1, 2, ..., each one capable of holding a file name. Also imagine that there are two shared variables, <emphasis>out</emphasis>, which points to the next file to be printed, and <emphasis>in</emphasis>, which points to the next free slot in the directory. These two variables might well be kept in a two-word file available to all processes. At a certain instant, slots 0 to 3 are empty (the files have already been printed) and slots 4 to 6 are full (with the names of files to be printed). More or less simultaneously, processes A and B decide they want to queue a file for printing. This situation is shown in <tag><link linkend="2-8">Fig. 2-8</link></tag>.
      </para>

      <para id="2-8"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/2-8.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 2-8. Two processes want to access shared memory at the same time.</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	In jurisdictions where Murphy's law[] is applicable, the following might well happen. Process <emphasis>A</emphasis> reads <emphasis>in</emphasis> and stores the value, 7, in a local variable called <emphasis>next_free_slot</emphasis>. Just then a clock interrupt occurs and the CPU decides that process <emphasis>A</emphasis> has run long enough, so it switches to process <emphasis>B</emphasis>. Process <emphasis>B</emphasis> also reads <emphasis>in</emphasis>, and also gets a 7, so it stores the name of its file in slot 7 and updates <emphasis>in</emphasis> to be an 8. Then it goes off and does other things.
      </para>

      <para/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/u2020.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      If something can go wrong, it will.
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	Eventually, process <emphasis>A</emphasis> runs again, starting from the place it left off last time. It looks at <emphasis>next_free_slot</emphasis>, finds a 7 there, and writes its file name in slot 7, erasing the name that process <emphasis>B</emphasis> just put there. Then it computes <emphasis>next_free_slot</emphasis> + 1, which is 8, and sets <emphasis>in</emphasis> to 8. The spooler directory is now internally consistent, so the printer daemon will not notice anything wrong, but process <emphasis>B</emphasis> will never receive any output. User <emphasis>B</emphasis> will hang around the printer room for years, wistfully hoping for output that never comes. Situations like this, where two or more processes are reading or writing some shared data and the final result depends on who runs precisely when, are called <command><emphasis>race conditions</emphasis></command>. Debugging programs containing race conditions is no fun at all. The results of most test runs are fine, but once in a blue moon something weird and unexplained happens.
      </para>
    </sect2>

    <sect2 id="sect-2.2.2">
      <title>2.2.2. Critical Sections</title>
      <para>
	How do we avoid race conditions? The key to preventing trouble here and in many other situations involving shared memory, shared files, and shared everything else is to find some way to prohibit more than one process from reading and writing the shared data at the same time. Put in other words, what we need is <command><emphasis>mutual exclusion</emphasis></command> some way of making sure that if one process is using a shared variable or file, the other processes will be excluded from doing the same thing. The difficulty above occurred because process B started using one of the shared variables before process A was finished with it. The choice of appropriate primitive operations for achieving mutual exclusion is a major design issue in any operating system, and a subject that we will now examine in great detail.
      </para>
      <para>
	The problem of avoiding race conditions can also be formulated in an abstract way. Part of the time, a process is busy doing internal computations and other things that do not lead to race conditions. However, sometimes a process may be accessing shared memory or files. That part of the program where the shared memory is accessed is called the <command><emphasis>critical region</emphasis></command> or <command><emphasis>critical section</emphasis></command>. If we could arrange matters such that no two processes were ever in their critical regions at the same time, we could avoid race conditions.
      </para>
      <para>
	Although this requirement avoids race conditions, this is not sufficient for having parallel processes cooperate correctly and efficiently using shared data. We need four conditions to hold to have a good solution:
      </para>
      <orderedlist>
	<listitem>
	  <para>
	     No two processes may be simultaneously inside their critical regions.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    No assumptions may be made about speeds or the number of CPUs.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    No process running outside its critical region may block other processes.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    No process should have to wait forever to enter its critical region. 
	  </para>
	</listitem>
      </orderedlist>
      <para>
	The behavior that we want is shown in <tag><link linkend="2-9">Fig. 2-9</link></tag>. Here process <emphasis>A</emphasis> enters its critical region at time <emphasis>T</emphasis><subscript>1</subscript>. A little later, at time <emphasis>T</emphasis><subscript>2</subscript> process <emphasis>B</emphasis> attempts to enter its critical region but fails because another process is already in its critical region and we allow only one at a time. Consequently, <emphasis>B</emphasis> is temporarily suspended until time <emphasis>T</emphasis><subscript>3</subscript> when <emphasis>A</emphasis> leaves its critical region, allowing <emphasis>B</emphasis> to enter immediately. Eventually <emphasis>B</emphasis> leaves (at <emphasis>T</emphasis><subscript>4</subscript>) and we are back to the original situation with no processes in their critical regions.
      </para>

      <para id="2-9"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/2-9.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 2-9. Mutual exclusion using critical regions.</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>
    </sect2>

    <sect2 id="sect-2.2.3">
      <title>2.2.3. Mutual Exclusion with Busy Waiting</title>
      <para>
	In this section we will examine various proposals for achieving mutual exclusion, so that while one process is busy updating shared memory in its critical region, no other process will enter its critical region and cause trouble.
      </para>
      <para>      
	<bridgehead renderas="sect4"> <literallayout>
	              Disabling Interrupts
	</literallayout></bridgehead>
      </para>
      <para>
	The simplest solution is to have each process disable all interrupts just after entering its critical region and reenable them just before leaving it. With interrupts disabled, no clock interrupts can occur. The CPU is only switched from process to process as a result of clock or other interrupts, after all, and with interrupts turned off the CPU will not be switched to another process. Thus, once a process has disabled interrupts, it can examine and update the shared memory without fear that any other process will intervene.
      </para>
      <para>
	This approach is generally unattractive because it is unwise to give user processes the power to turn off interrupts. Suppose that one of them did, and then never turned them on again? That could be the end of the system. Furthermore, if the system is a multiprocessor, with two or more CPUs, disabling interrupts affects only the CPU that executed the disable instruction. The other ones will continue running and can access the shared memory.
      </para>
      <para>
	On the other hand, it is frequently convenient for the kernel itself to disable interrupts for a few instructions while it is updating variables or lists. If an interrupt occurred while the list of ready processes, for example, was in an inconsistent state, race conditions could occur. The conclusion is: disabling interrupts is often a useful technique within the operating system itself but is not appropriate as a general mutual exclusion mechanism for user processes.
      </para>
      <para>      
	<bridgehead renderas="sect4"> <literallayout>
	              Lock Variables
	</literallayout></bridgehead>
      </para>
      <para>
	As a second attempt, let us look for a software solution. Consider having a single, shared, (lock) variable, initially 0. When a process wants to enter its critical region, it first tests the lock. If the lock is 0, the process sets it to 1 and enters the critical region. If the lock is already 1, the process just waits until it becomes 0. Thus, a 0 means that no process is in its critical region, and a 1 means that some process is in its critical region.
      </para>
      <para>
	Unfortunately, this idea contains exactly the same fatal flaw that we saw in the spooler directory. Suppose that one process reads the lock and sees that it is 0. Before it can set the lock to 1, another process is scheduled, runs, and sets the lock to 1. When the first process runs again, it will also set the lock to 1, and two processes will be in their critical regions at the same time.
      </para>
      <para>
	Now you might think that we could get around this problem by first reading out the lock value, then checking it again just before storing into it, but that really does not help. The race now occurs if the second process modifies the lock just after the first process has finished its second check.
      </para>
      <para>      
	<bridgehead renderas="sect4"> <literallayout>
	              Strict Alternation
	</literallayout></bridgehead>
      </para>
      <para>
	A third approach to the mutual exclusion problem is shown in <tag><link linkend="2-10">Fig. 2-10</link></tag>. This program fragment, like most others in this book, is written in C. C was chosen here because real operating systems are commonly written in C (or occasionally C++), but hardly ever in languages like Java. C is powerful, efficient, and predictable, characteristics critical for writing operating systems. Java, for example, is not predictable because it might run out of storage at a critical moment and need to invoke the garbage collector at a most inopportune time. This cannot happen in C because there is no garbage collection in C. A quantitative comparison of C, C++, Java, and four other languages is given by Prechelt (2000).
      </para>

      <para id="2-10"><command>Figure 2-10. A proposed solution to the critical region problem. (a) Process 0. (b) Process 1. In both cases, be sure to note the semicolons terminating the while statements.</command></para>
      <programlisting>while (TRUE){                              while (TRUE) {
    while(turn != 0) /* loop* /;               while(turn != 1) /* loop* /;
    critical_region();                         critical_region();
    turn  = 1;                                 turn  = 0;
    noncritical_region();                      noncritical_region();
}                                          }
                 (a)                                        (b)


      </programlisting>

      <para>
	In <tag><link linkend="2-10">Fig. 2-10</link></tag>, the integer variable <emphasis>turn</emphasis>, initially 0, keeps track of whose turn it is to enter the critical region and examine or update the shared memory. Initially, process 0 inspects <emphasis>turn</emphasis>, finds it to be 0, and enters its critical region. Process 1 also finds it to be 0 and therefore sits in a tight loop continually testing turn to see when it becomes 1. Continuously testing a variable until some value appears is called <command><emphasis>busy waiting</emphasis></command>. It should usually be avoided, since it wastes CPU time. Only when there is a reasonable expectation that the wait will be short is busy waiting used. A lock that uses busy waiting is called a <command><emphasis>spin lock</emphasis></command>.
      </para>
      <para>
	When process 0 leaves the critical region, it sets <emphasis>turn</emphasis> to 1, to allow process 1 to enter its critical region. Suppose that process 1 finishes its critical region quickly, so both processes are in their noncritical regions, with <emphasis>turn</emphasis> set to 0. Now process 0 executes its whole loop quickly, exiting its critical region and setting <emphasis>turn</emphasis> to 1. At this point <emphasis>turn</emphasis> is 1 and both processes are executing in their noncritical regions.
      </para>
      <para>
	Suddenly, process 0 finishes its noncritical region and goes back to the top of its loop. Unfortunately, it is not permitted to enter its critical region now, because <emphasis>turn</emphasis> is 1 and process 1 is busy with its noncritical region. It hangs in its <command>while</command> loop until process 1 sets <emphasis>turn</emphasis> to 0. Put differently, taking turns is not a good idea when one of the processes is much slower than the other.
      </para>
      <para>
	This situation violates condition 3 set out above: process 0 is being blocked by a process not in its critical region. Going back to the spooler directory discussed above, if we now associate the critical region with reading and writing the spooler directory, process 0 would not be allowed to print another file because process 1 was doing something else.
      </para>
      <para>
	In fact, this solution requires that the two processes strictly alternate in entering their critical regions, for example, in spooling files. Neither one would be permitted to spool two in a row. While this algorithm does avoid all races, it is not really a serious candidate as a solution because it violates condition 3.
      </para>

      <para>      
	<bridgehead renderas="sect4"> <literallayout>
	              Peterson's Solution
	</literallayout></bridgehead>
      </para>

      <para>
	By combining the idea of taking turns with the idea of lock variables and warning variables, a Dutch mathematician, T. Dekker, was the first one to devise a software solution to the mutual exclusion problem that does not require strict alternation. For a discussion of Dekker's algorithm, see Dijkstra (1965).
      </para>
      <para>
	In 1981, G.L. Peterson discovered a much simpler way to achieve mutual exclusion, thus rendering Dekker's solution obsolete. Peterson's algorithm is shown in <tag><link linkend="2-11">Fig. 2-11</link></tag>. This algorithm consists of two procedures written in ANSI C, which means that function prototypes should be supplied for all the functions defined and used. However, to save space, we will not show the prototypes in this or subsequent examples.
      </para>

      <para id="2-11"><command>Figure 2-11. Peterson's solution for achieving mutual exclusion.</command></para>
      <programlisting>#define FALSE 0
#define TRUE  1
#define N     2                /* number of processes */

int turn;                      /* whose turn is it? */
int interested[N];             /* all values initially 0 (FALSE)*/
void enter_region(int process) /* process is 0 or 1 */
{
  int other;                   /* number of the other process */
  other = 1 - process;         /* the opposite of process */
  interested[process] = TRUE;  /* show that you are interested */
  turn = process;              /* set flag */
  while (turn == process &amp;&amp; interested[other] == TRUE) /* null statement */;
}
void leave_region(int process)  /* process: who is leaving */
{
  interested[process] = FALSE;  /* indicate departure from critical region */
}


      </programlisting>

      <para>
	Before using the shared variables (i.e., before entering its critical region), each process calls <emphasis>enter_region</emphasis> with its own process number, 0 or 1, as the parameter. This call will cause it to wait, if need be, until it is safe to enter. After it has finished with the shared variables, the process calls <emphasis>leave_region</emphasis> to indicate that it is done and to allow the other process to enter, if it so desires.
      </para>
      <para>
	Let us see how this solution works. Initially, neither process is in its critical region. Now process 0 calls <emphasis>enter_region</emphasis>. It indicates its interest by setting its array element and sets <emphasis>turn</emphasis> to 0. Since process 1 is not interested, <emphasis>enter_region</emphasis> returns immediately. If process 1 now calls <emphasis>enter_region</emphasis>, it will hang there until <emphasis>interested</emphasis>[0] goes to <emphasis>FALSE</emphasis>, an event that only happens when process 0 calls <emphasis>leave_region</emphasis> to exit the critical region.
      </para>
      <para>
	Now consider the case that both processes call <emphasis>enter_region</emphasis> almost simultaneously. Both will store their process number in <emphasis>turn</emphasis>. Whichever store is done last is the one that counts; the first one is lost. Suppose that process 1 stores last, so <emphasis>turn</emphasis> is 1. When both processes come to the <command>while</command> statement, process 0 executes it zero times and enters its critical region. Process 1 loops and does not enter its critical region.
      </para>

      <para>      
	<bridgehead renderas="sect4"> <literallayout>
	              The TSL Instruction
	</literallayout></bridgehead>
      </para>

      <para>
	Now let us look at a proposal that requires a little help from the hardware. Many computers, especially those designed with multiple processors in mind, have an instruction
      </para>
      <para>
	<command>TSL RX,LOCK</command>
      </para>
      <para>
	(Test and Set Lock) that works as follows: it reads the contents of the memory word <emphasis>LOCK</emphasis> into register <command>RX</command> and then stores a nonzero value at the memory address <emphasis>LOCK</emphasis>. The operations of reading the word and storing into it are guaranteed to be indivisibleno other processor can access the memory word until the instruction is finished. The CPU executing the <command>TSL</command> instruction locks the memory bus to prohibit other CPUs from accessing memory until it is done.
      </para>
      <para>
	To use the <command>TSL</command> instruction, we will use a shared variable, <emphasis>LOCK</emphasis>, to coordinate access to shared memory. When <emphasis>LOCK</emphasis> is 0, any process may set it to 1 using the <command>TSL</command> instruction and then read or write the shared memory. When it is done, the process sets <emphasis>LOCK</emphasis> back to 0 using an ordinary <command>move</command> instruction.
      </para>
      <para>
	How can this instruction be used to prevent two processes from simultaneously entering their critical regions? The solution is given in <tag><link linkend="2-12">Fig. 2-12</link></tag>. There a four-instruction subroutine in a fictitious (but typical) assembly language is shown. The first instruction copies the old value of <emphasis>LOCK</emphasis> to the register and then sets <emphasis>LOCK</emphasis> to 1. Then the old value is compared with 0. If it is nonzero, the lock was already set, so the program just goes back to the beginning and tests it again. Sooner or later it will become 0 (when the process currently in its critical region is done with its critical region), and the subroutine returns, with the lock set. Clearing the lock is simple. The program just stores a 0 in <emphasis>LOCK</emphasis>. No special instructions are needed.
      </para>

      <para id="2-12"><command>Figure 2-12. Entering and leaving a critical region using the TSL instruction.</command></para>
      <programlisting>enter_region:
TSL REGISTER,LOCK       |copy LOCK to register and set LOCK to 1
  CMP REGISTER,#0         |was LOCK zero?
  JNE ENTER_REGION        |if it was non zero, LOCK was set, so loop
  RET                     |return to caller; critical region entered


  leave_region:
MOVE LOCK,#0             |store a 0 in LOCK
       RET                      |return to caller

      </programlisting>

      <para>
	One solution to the critical region problem is now straightforward. Before entering its critical region, a process calls <emphasis>enter_region</emphasis>, which does busy waiting until the lock is free; then it acquires the lock and returns. After the critical region the process calls <emphasis>leave_region</emphasis>, which stores a 0 in <emphasis>LOCK</emphasis>. As with all solutions based on critical regions, the processes must call <emphasis>enter_region</emphasis> and <emphasis>leave_region</emphasis> at the correct times for the method to work. If a process cheats, the mutual exclusion will fail.
      </para>
    </sect2>

    <sect2 id="sect-2.2.4">
      <title>2.2.4. Sleep and Wakeup</title>
      <para>
	Both Peterson's solution and the solution using <command>TSL</command> are correct, but both have the defect of requiring busy waiting. In essence, what these solutions do is this: when a process wants to enter its critical region, it checks to see if the entry is allowed. If it is not, the process just sits in a tight loop waiting until it is.
      </para>
      <para>
	Not only does this approach waste CPU time, but it can also have unexpected effects. Consider a computer with two processes, <emphasis>H</emphasis>, with high priority and <emphasis>L</emphasis>, with low priority, which share a critical region. The scheduling rules are such that <emphasis>H</emphasis> is run whenever it is in ready state. At a certain moment, with <emphasis>L</emphasis> in its critical region, <emphasis>H</emphasis> becomes ready to run (e.g., an I/O operation completes). <emphasis>H</emphasis> now begins busy waiting, but since <emphasis>L</emphasis> is never scheduled while <emphasis>H</emphasis> is running, <emphasis>L</emphasis> never gets the chance to leave its critical region, so <emphasis>H</emphasis> loops forever. This situation is sometimes referred to as the <command><emphasis>priority inversion problem</emphasis></command>.
      </para>
      <para>
	Now let us look at some interprocess communication primitives that block instead of wasting CPU time when they are not allowed to enter their critical regions. One of the simplest is the pair <command>sleep</command> and <command>wakeup</command>. <command>sleep</command> is a system call that causes the caller to block, that is, be suspended until another process wakes it up. The <command>wakeup</command> call has one parameter, the process to be awakened. Alternatively, both <command>sleep</command> and <command>wakeup</command> each have one parameter, a memory address used to match up <command>sleep</command>s with <command>wakeup</command>s.
      </para>

      <para>      
	<bridgehead renderas="sect4"> <literallayout>
	              The Producer-Consumer Problem
	</literallayout></bridgehead>
      </para>

      <para>
	As an example of how these primitives can be used in practice, let us consider the <command><emphasis>producer-consumer</emphasis></command> problem (also known as the <command><emphasis>bounded buffer</emphasis></command> problem). Two processes share a common, fixed-size buffer. One of them, the producer, puts information into the buffer, and the other one, the consumer, takes it out. (It is also possible to generalize the problem to have m producers and n consumers, but we will only consider the case of one producer and one consumer because this assumption simplifies the solutions).
      </para>
      <para>
	Trouble arises when the producer wants to put a new item in the buffer, but it is already full. The solution is for the producer to go to sleep, to be awakened when the consumer has removed one or more items. Similarly, if the consumer wants to remove an item from the buffer and sees that the buffer is empty, it goes to sleep until the producer puts something in the buffer and wakes it up.
      </para>
      <para>
	This approach sounds simple enough, but it leads to the same kinds of race conditions we saw earlier with the spooler directory. To keep track of the number of items in the buffer, we will need a variable, <emphasis>count</emphasis>. If the maximum number of items the buffer can hold is <emphasis>N</emphasis>, the producer's code will first test to see if <emphasis>count</emphasis> is <emphasis>N</emphasis>. If it is, the producer will go to sleep; if it is not, the producer will add an item and increment <emphasis>count</emphasis>.
      </para>
      <para>
	The consumer's code is similar: first test count to see if it is 0. If it is, go to sleep; if it is nonzero, remove an item and decrement the counter. Each of the processes also tests to see if the other should be sleeping, and if not, wakes it up. The code for both producer and consumer is shown in <tag><link linkend="2-13">Fig. 2-13</link></tag>.
      </para>

      <para id="2-13"><command>Figure 2-13. The producer-consumer problem with a fatal race condition.</command></para>
      <programlisting>#define N 100                /* number of slots in the buffer */
int count = 0;               /* number of items in the buffer */

void producer(void)
{
  int item;

  while (TRUE){              /* repeat forever */
    item = produce_item();   /* generate next item */
    if (count == N) sleep(); /* if buffer is full, go to sleep */
    insert_item(item);       /* put item in buffer */
    count = count + 1;       /* increment count of items in buffer */
    if (count == 1) wakeup(consumer);  /* was buffer empty? */
  }
}


void consumer(void)
{
  int item;

  while (TRUE){              /* repeat forever */
    if (count == 0) sleep(); /* if buffer is empty, got to sleep */
    item = remove_item();    /* take item out of buffer */
    count = count  1;        /* decrement count of items in  buffer */
    if (count ==N  1) wakeup(producer);  /* was buffer full? */
    consume_item(item);      /* print item */
  }
}


      </programlisting>

      <para>
	To express system calls such as <command>sleep</command> and <command>wakeup</command> in C, we will show them as calls to library routines. They are not part of the standard C library but presumably would be available on any system that actually had these system calls. The procedures <emphasis>enter_item</emphasis> and <emphasis>remove_item</emphasis>, which are not shown, handle the bookkeeping of putting items into the buffer and taking items out of the buffer.
      </para>
      <para>
	Now let us get back to the race condition. It can occur because access to <emphasis>count</emphasis> is unconstrained. The following situation could possibly occur. The buffer is empty and the consumer has just read <emphasis>count</emphasis> to see if it is 0. At that instant, the scheduler decides to stop running the consumer temporarily and start running the producer. The producer enters an item in the buffer, increments <emphasis>count</emphasis>, and notices that it is now 1. Reasoning that <emphasis>count</emphasis> was just 0, and thus the consumer must be sleeping, the producer calls <emphasis>wakeup</emphasis> to wake the consumer up.
      </para>
      <para>
	Unfortunately, the consumer is not yet logically asleep, so the wakeup signal is lost. When the consumer next runs, it will test the value of count it previously read, find it to be 0, and go to sleep. Sooner or later the producer will fill up the buffer and also go to sleep. Both will sleep forever.
      </para>
      <para>
	The essence of the problem here is that a wakeup sent to a process that is not (yet) sleeping is lost. If it were not lost, everything would work. A quick fix is to modify the rules to add a <command><emphasis>wakeup waiting bit</emphasis></command> to the picture. When a wakeup is sent to a process that is still awake, this bit is set. Later, when the process tries to go to sleep, if the wakeup waiting bit is on, it will be turned off, but the process will stay awake. The wakeup waiting bit is a piggy bank for wakeup signals.
      </para>
      <para>
	While the wakeup waiting bit saves the day in this simple example, it is easy to construct examples with three or more processes in which one wakeup waiting bit is insufficient. We could make another patch, and add a second wakeup waiting bit, or maybe 8 or 32 of them, but in principle the problem is still there.
      </para>
    </sect2>

    <sect2 id="sect-2.2.5">
      <title>2.2.5. Semaphores</title>
      <para>
	This was the situation until E. W. Dijkstra (1965) suggested using an integer variable to count the number of wakeups saved for future use. In his proposal, a new variable type, called a <command><emphasis>semaphore</emphasis></command>, was introduced. A semaphore could have the value 0, indicating that no wakeups were saved, or some positive value if one or more wakeups were pending.
      </para>
      <para>
	Dijkstra proposed having two operations, <command>down</command> and <command>up</command> (which are generalizations of <command>sleep</command> and <command>wakeup</command>, respectively). The <command>down</command> operation on a semaphore checks to see if the value is greater than 0. If so, it decrements the value (i.e., uses up one stored wakeup) and just continues. If the value is 0, the process is put to sleep without completing the <command>down</command> for the moment. Checking the value, changing it, and possibly going to sleep is all done as a single, indivisible, <command><emphasis>atomic action</emphasis></command>. It is guaranteed that once a semaphore operation has started, no other process can access the semaphore until the operation has completed or blocked. This atomicity is absolutely essential to solving synchronization problems and avoiding race conditions.
      </para>
      <para>
	The <command>up</command> operation increments the value of the semaphore addressed. If one or more processes were sleeping on that semaphore, unable to complete an earlier <command>down</command> operation, one of them is chosen by the system (e.g., at random) and is allowed to complete its <command>down</command>. Thus, after an <command>up</command> on a semaphore with processes sleeping on it, the semaphore will still be 0, but there will be one fewer process sleeping on it. The operation of incrementing the semaphore and waking up one process is also indivisible. No process ever blocks doing an <command>up</command>, just as no process ever blocks doing a <command>wakeup</command> in the earlier model.
      </para>
      <para>
	As an aside, in Dijkstra's original paper, he used the names <command>p</command> and <command>v</command> instead of <command>down</command> and <command>up</command>, respectively, but since these have no mnemonic significance to people who do not speak Dutch (and only marginal significance to those who do), we will use the terms <command>down</command> and <command>up</command> instead. These were first introduced in Algol 68.
      </para>

      <para>      
	<bridgehead renderas="sect4"> <literallayout>
	              Solving the Producer-Consumer Problem using Semaphores
	</literallayout></bridgehead>
      </para>

      <para>
	Semaphores solve the lost-wakeup problem, as shown in <tag><link linkend="2-14">Fig. 2-14</link></tag>. It is essential that they be implemented in an indivisible way. The normal way is to implement <command>up</command> and <command>down</command> as system calls, with the operating system briefly disabling all interrupts while it is testing the semaphore, updating it, and putting the process to sleep, if necessary. As all of these actions take only a few instructions, no harm is done in disabling interrupts. If multiple CPUs are being used, each semaphore should be protected by a lock variable, with the <command>TSL</command> instruction used to make sure that only one CPU at a time examines the semaphore. Be sure you understand that using <command>TSL</command> to prevent several CPUs from accessing the semaphore at the same time is quite different from busy waiting by the producer or consumer waiting for the other to empty or fill the buffer. The semaphore operation will only take a few microseconds, whereas the producer or consumer might take arbitrarily long.
      </para>

      <para id="2-14"><command>Figure 2-13. The producer-consumer problem with a fatal race condition.</command></para>
      <programlisting>#define N 100               /* number of slots in the buffer */
typedef int semaphore;      /* semaphores are a special kind of int */
semaphore mutex = 1;        /* controls access to critical region */
semaphore empty = N;        /* counts empty buffer slots */
semaphore full = 0;         /* counts full buffer slots */

void producer(void)
{
  int item;

  while (TRUE){             /* TRUE is the constant 1 */
    item = produce_item();  /* generate something to put in buffer */
    down(&amp;empty);           /* decrement empty count */
    down(&amp;mutex);           /* enter critical region */
    insert_item(item);      /* put new item in buffer */
    up(&amp;mutex);             /* leave critical region */
    up(&amp;full);              /* increment count of full slots */
  }
}

void consumer(void)
{
  int item;

  while (TRUE){             /* infinite loop */
    down(&amp;full);            /* decrement full count */
    down(&amp;mutex);           /* enter critical region */
    item = remove_item();   /* take item from buffer */
    up(&amp;mutex);             /* leave critical region */
    up(&amp;empty);             /* increment count of empty slots */
    consume_item(item);     /* do something with the item */
  }
}


      </programlisting>

      <para>
	This solution uses three semaphores: one called <emphasis>full</emphasis> for counting the number of slots that are full, one called <emphasis>empty</emphasis> for counting the number of slots that are empty, and one called <emphasis>mutex</emphasis> to make sure the producer and consumer do not access the buffer at the same time. <emphasis>Full</emphasis> is initially 0, <emphasis>empty</emphasis> is initially equal to the number of slots in the buffer, and <emphasis>mutex</emphasis> is initially 1. Semaphores that are initialized to 1 and used by two or more processes to ensure that only one of them can enter its critical region at the same time are called <command><emphasis>binary semaphores</emphasis></command>. If each process does a <command>down</command> just before entering its critical region and an <command>up</command> just after leaving it, mutual exclusion is guaranteed.
      </para>
      <para>
	Now that we have a good interprocess communication primitive at our disposal, let us go back and look at the interrupt sequence of <tag><link linkend="2-5">Fig. 2-5</link></tag> again. In a system-using semaphores, the natural way to hide interrupts is to have a semaphore, initially set to 0, associated with each I/O device. Just after starting an I/O device, the managing process does a <command>down</command> on the associated semaphore, thus blocking immediately. When the interrupt comes in, the interrupt handler then does an <command>up</command> on the associated semaphore, which makes the relevant process ready to run again. In this model, step 6 in <tag><link linkend="2-5">Fig. 2-5</link></tag> consists of doing an <command>up</command> on the device's semaphore, so that in step 7 the scheduler will be able to run the device manager. Of course, if several processes are now ready, the scheduler may choose to run an even more important process next. We will look at how scheduling is done later in this chapter.
      </para>
      <para>
	In the example of <tag><link linkend="2-14">Fig. 2-14</link></tag>, we have actually used semaphores in two different ways. This difference is important enough to make explicit. The <emphasis>mutex</emphasis> semaphore is used for mutual exclusion. It is designed to guarantee that only one process at a time will be reading or writing the buffer and the associated variables. This mutual exclusion is required to prevent chaos. We will study mutual exclusion and how to achieve it more in the next section.
      </para>
      <para>
	The other use of semaphores is for <command><emphasis>synchronization</emphasis></command>. The <emphasis>full</emphasis> and <emphasis>empty</emphasis> semaphores are needed to guarantee that certain event sequences do or do not occur. In this case, they ensure that the producer stops running when the buffer is full, and the consumer stops running when it is empty. This use is different from mutual exclusion.
      </para>
    </sect2>

    <sect2 id="sect-2.2.6">
      <title>2.2.6. Mutexes</title>
      <para>
	When the semaphore's ability to count is not needed, a simplified version of the semaphore, called a mutex, is sometimes used. Mutexes are good only for managing mutual exclusion to some shared resource or piece of code. They are easy and efficient to implement, which makes them especially useful in thread packages that are implemented entirely in user space.
      </para>
      <para>
	A <command><emphasis>mutex</emphasis></command> is a variable that can be in one of two states: unlocked or locked. Consequently, only 1 bit is required to represent it, but in practice an integer often is used, with 0 meaning unlocked and all other values meaning locked. Two procedures are used with mutexes. When a process (or thread) needs access to a critical region, it calls <emphasis>mutex_lock</emphasis>. If the mutex is currently unlocked (meaning that the critical region is available), the call succeeds and the calling thread is free to enter the critical region.
      </para>
      <para>
	On the other hand, if the mutex is already locked, the caller is blocked until the process in the critical region is finished and calls <emphasis>mutex_unlock</emphasis>. If multiple processes are blocked on the mutex, one of them is chosen at random and allowed to acquire the lock.
      </para>
    </sect2>

    <sect2 id="sect-2.2.7">
      <title>2.2.7. Monitors</title>
      <para>
	With semaphores interprocess communication looks easy, right? Forget it. Look closely at the order of the <command>down</command>s before entering or removing items from the buffer in <tag><link linkend="2-14">Fig. 2-14</link></tag>. Suppose that the two <command>down</command>s in the producer's code were reversed in order, so <emphasis>mutex</emphasis> was decremented before empty instead of after it. If the buffer were completely full, the producer would block, with mutex set to 0. Consequently, the next time the consumer tried to access the buffer, it would do a <command>down</command> on <emphasis>mutex</emphasis>, now 0, and block too. Both processes would stay blocked forever and no more work would ever be done. This unfortunate situation is called a <command><emphasis>deadlock</emphasis></command>. We will study deadlocks in detail in <tag><xref linkend="Chapter3"/></tag>.
      </para>
      <para>
	This problem is pointed out to show how careful you must be when using semaphores. One subtle error and everything comes to a grinding halt. It is like programming in assembly language, only worse, because the errors are race conditions, deadlocks, and other forms of unpredictable and irreproducible behavior.
      </para>
      <para>
	To make it easier to write correct programs, Brinch Hansen (1973) and Hoare (1974) proposed a higher level synchronization primitive called a <command><emphasis>monitor</emphasis></command>. Their proposals differed slightly, as described below. A monitor is a collection of procedures, variables, and data structures that are all grouped together in a special kind of module or package. Processes may call the procedures in a monitor whenever they want to, but they cannot directly access the monitor's internal data structures from procedures declared outside the monitor. This rule, which is common in modern object-oriented languages such as Java, was relatively unusual for its time, although objects can be traced back to Simula 67. <tag><link linkend="2-15">Figure 2-15</link></tag> illustrates a monitor written in an imaginary language, Pidgin Pascal.
      </para>

      <para id="2-15"><command>Figure 2-15. A monitor.</command></para>
      <programlisting>monitor example
integer i;
condition c;


procedure producer (x);
 .
 .
 .
end;


procedure consumer (x);
 .
 .
 .
end;
end monitor;

      </programlisting>

      <para>
	Monitors have a key property that makes them useful for achieving mutual exclusion: only one process can be active in a monitor at any instant. Monitors are a programming language construct, so the compiler knows they are special and can handle calls to monitor procedures differently from other procedure calls. Typically, when a process calls a monitor procedure, the first few instructions of the procedure will check to see if any other process is currently active within the monitor. If so, the calling process will be suspended until the other process has left the monitor. If no other process is using the monitor, the calling process may enter.
      </para>
      <para>
	It is up to the compiler to implement the mutual exclusion on monitor entries, but a common way is to use a mutex or binary semaphore. Because the compiler, not the programmer, arranges for the mutual exclusion, it is much less likely that something will go wrong. In any event, the person writing the monitor does not have to be aware of how the compiler arranges for mutual exclusion. It is sufficient to know that by turning all the critical regions into monitor procedures, no two processes will ever execute their critical regions at the same time.
      </para>
      <para>
	Although monitors provide an easy way to achieve mutual exclusion, as we have seen above, that is not enough. We also need a way for processes to block when they cannot proceed. In the producer-consumer problem, it is easy enough to put all the tests for buffer-full and buffer-empty in monitor procedures, but how should the producer block when it finds the buffer full?
      </para>
      <para>
	The solution lies in the introduction of <command><emphasis>condition variables</emphasis></command>, along with two operations on them, <command>wait</command> and <command>signal</command>. When a monitor procedure discovers that it cannot continue (e.g., the producer finds the buffer full), it does a <command>wait</command> on some condition variable, say, <emphasis>full</emphasis>. This action causes the calling process to block. It also allows another process that had been previously prohibited from entering the monitor to enter now.
      </para>
      <para>
	This other process, for example, the consumer, can wake up its sleeping partner-by doing a <command>signal</command> on the condition variable that its partner is waiting on. To avoid having two active processes in the monitor at the same time, we need a rule telling what happens after a <command>signal</command>. Hoare proposed letting the newly awakened process run, suspending the other one. Brinch Hansen proposed finessing the problem by requiring that a process doing a <command>signal</command> <emphasis>must</emphasis> exit the monitor immediately. In other words, a <command>signal</command> statement may appear only as the final statement in a monitor procedure. We will use Brinch Hansen's proposal because it is conceptually simpler and is also easier to implement. If a <command>signal</command> is done on a condition variable on which several processes are waiting, only one of them, determined by the system scheduler, is revived.
      </para>
      <para>
	There is also a third solution, not proposed by either Hoare or Brinch Hansen. This is to let the signaler continue to run and allow the waiting process to start running only after the signaler has exited the monitor.
      </para>
      <para>
	Condition variables are not counters. They do not accumulate signals for later use the way semaphores do. Thus if a condition variable is signaled with no one waiting on it, the signal is lost. In other words, the <command>wait</command> must come before the <command>signal</command>. This rule makes the implementation much simpler. In practice it is not a problem because it is easy to keep track of the state of each process with variables, if need be. A process that might otherwise do a <command>signal</command> can see that this operation is not necessary by looking at the variables.
      </para>
      <para>
	A skeleton of the producer-consumer problem with monitors is given in <tag><link linkend="2-16">Fig. 2-16</link></tag> in Pidgin Pascal. The advantage of using Pidgin Pascal here is that it is pure and simple and follows the Hoare/Brinch Hansen model exactly.
      </para>

      <para id="2-16"><command>Figure 2-16. An outline of the producer-consumer problem with monitors. Only one monitor procedure at a time is active. The buffer has N slots.</command></para>
      <programlisting>monitor ProducerConsumer
    condition full, empty;
    integer count;

    procedure insert(item: integer);
    begin
        if count = N then wait(full);
        insert_item(item);
        count := count + 1;
        if count = 1 then signal(empty)
    end;

    function remove: integer;
    begin
        if count = 0 then wait(empty);
        remove = remove_item;
        count := count  1;
        if count = N  1 then signal(full)
    end;

    count := 0;
end monitor;


procedure producer;
begin
    while true do
    begin
        item = produce_item;
        ProducerConsumer.insert(item)
    end
end;


procedure consumer;
begin
    while true do
    begin
        item = ProducerConsumer.remove;
        consume_item(item)
    end
end;



      </programlisting>

      <para>
	You may be thinking that the operations <command>wait</command> and <command>signal</command> look similar to <command>sleep</command> and <command>wakeup</command>, which we saw earlier had fatal race conditions. They are very similar, but with one crucial difference: <command>sleep</command> and <command>wakeup</command> failed because while one process was trying to go to sleep, the other one was trying to wake it up. With monitors, that cannot happen. The automatic mutual exclusion on monitor procedures guarantees that if, say, the producer inside a monitor procedure discovers that the buffer is full, it will be able to complete the <command>wait</command> operation without having to worry about the possibility that the scheduler may switch to the consumer just before the <command>wait</command> completes. The consumer will not even be let into the monitor at all until the <command>wait</command> is finished and the producer is marked as no longer runnable.
      </para>
      <para>
	Although Pidgin Pascal is an imaginary language, some real programming languages also support monitors, although not always in the form designed by Hoare and Brinch Hansen. One such language is Java. Java is an object-oriented language that supports user-level threads and also allows methods (procedures) to be grouped together into classes. By adding the keyword <command>synchronized</command> to a method declaration, Java guarantees that once any thread has started executing that method, no other thread will be allowed to start executing any other <command>synchronized</command> method in that class.
      </para>
      <para>
	Synchronized methods in Java differ from classical monitors in an essential way: Java does not have condition variables. Instead, it offers two procedures, <emphasis>wait</emphasis> and <emphasis>notify</emphasis> that are the equivalent of <emphasis>sleep</emphasis> and <emphasis>wakeup</emphasis> except that when they are used inside synchronized methods, they are not subject to race conditions.
      </para>
      <para>
	By making the mutual exclusion of critical regions automatic, monitors make parallel programming much less error-prone than with semaphores. Still, they too have some drawbacks. It is not for nothing that <tag><link linkend="2-16">Fig. 2-16</link></tag> is written in Pidgin Pascal rather than in C, as are the other examples in this book. As we said earlier, monitors are a programming language concept. The compiler must recognize them and arrange for the mutual exclusion somehow. C, Pascal, and most other languages do not have monitors, so it is unreasonable to expect their compilers to enforce any mutual exclusion rules. In fact, how could the compiler even know which procedures were in monitors and which were not?
      </para>
      <para>
	These same languages do not have semaphores either, but adding semaphores is easy: all you need to do is add two short assembly code routines to the library to issue the <command>up</command> and <command>down</command> system calls. The compilers do not even have to know that they exist. Of course, the operating systems have to know about the semaphores, but at least if you have a semaphore-based operating system, you can still write the user programs for it in C or C++ (or even FORTRAN if you are masochistic enough). With monitors, you need a language that has them built in.
      </para>
      <para>
	Another problem with monitors, and also with semaphores, is that they were designed for solving the mutual exclusion problem on one or more CPUs that all have access to a common memory. By putting the semaphores in the shared memory and protecting them with <command>TSL</command> instructions, we can avoid races. When we go to a distributed system consisting of multiple CPUs, each with its own private memory, connected by a local area network, these primitives become inapplicable. The conclusion is that semaphores are too low level and monitors are not usable except in a few programming languages. Also, none of the primitives provide for information exchange between machines. Something else is needed.
      </para>
    </sect2>

    <sect2 id="sect-2.2.8">
      <title>2.2.8. Message Passing</title>
      <para>
	That something else is <command><emphasis>message passing</emphasis></command>. This method of interprocess communication uses two primitives, <command>send</command> and <command>receive</command>, which, like semaphores and unlike monitors, are system calls rather than language constructs. As such, they can easily be put into library procedures, such as
      </para>
      <para>
	<command>send(destination, &amp;message);</command>
      </para>
      <para>
	and
      </para>
      <para>
	<command>receive(source, &amp;message);</command>
      </para>
      <para>
	The former call sends a message to a given destination and the latter one receives a message from a given source (or from <emphasis>ANY</emphasis>, if the receiver does not care). If no message is available, the receiver could block until one arrives. Alternatively, it could return immediately with an error code.
      </para>

      <para>      
	<bridgehead renderas="sect4"> <literallayout>
	              Design Issues for Message Passing Systems
	</literallayout></bridgehead>
      </para>

      <para>
	Message passing systems have many challenging problems and design issues that do not arise with semaphores or monitors, especially if the communicating processes are on different machines connected by a network. For example, messages can be lost by the network. To guard against lost messages, the sender and receiver can agree that as soon as a message has been received, the receiver will send back a special <command><emphasis>acknowledgement</emphasis></command> message. If the sender has not received the acknowledgement within a certain time interval, it retransmits the message.
      </para>
      <para>
	Now consider what happens if the message itself is received correctly, but the acknowledgement is lost. The sender will retransmit the message, so the receiver will get it twice. It is essential that the receiver can distinguish a new message from the retransmission of an old one. Usually, this problem is solved by putting consecutive sequence numbers in each original message. If the receiver gets a message bearing the same sequence number as the previous message, it knows that the message is a duplicate that can be ignored.
      </para>
      <para>
	Message systems also have to deal with the question of how processes are named, so that the process specified in a <command>send</command> or <command>receive</command> call is unambiguous. <command><emphasis>Authentication</emphasis></command> is also an issue in message systems: how can the client tell that he is communicating with the real file server, and not with an imposter?
      </para>
      <para>
	At the other end of the spectrum, there are also design issues that are important when the sender and receiver are on the same machine. One of these is performance. Copying messages from one process to another is always slower than doing a semaphore operation or entering a monitor. Much work has gone into making message passing efficient. Cheriton (1984), for example, has suggested limiting message size to what will fit in the machine's registers, and then doing message passing using the registers.
      </para>

      <para>      
	<bridgehead renderas="sect4"> <literallayout>
	              The Producer-Consumer Problem with Message Passing
	</literallayout></bridgehead>
      </para>

      <para>
	Now let us see how the producer-consumer problem can be solved with message passing and no shared memory. A solution is given in <tag><link linkend="2-17">Fig. 2-17</link></tag>. We assume that all messages are the same size and that messages sent but not yet received are buffered automatically by the operating system. In this solution, a total of <emphasis>N</emphasis> messages is used, analogous to the <emphasis>N</emphasis> slots in a shared memory buffer. The consumer starts out by sending <emphasis>N</emphasis> empty messages to the producer. Whenever the producer has an item to give to the consumer, it takes an empty message and sends back a full one. In this way, the total number of messages in the system remains constant in time, so they can be stored in a given amount of memory known in advance.
      </para>

      <para id="2-17"><command>Figure 2-17. The producer-consumer problem with N messages.</command></para>
      <programlisting>#define N 100                  /* number of slots in the buffer */

void producer(void)
{
  int item;
  message m;                   /* message buffer */

  while (TRUE) {
    item = produce_item();     /* generate something to put in buffer */
    receive(consumer, &amp;m);     /* wait for an empty to arrive */
    build_message(&amp;m, item);   /* construct a message to send */
    send(consumer, &amp;m);        /* send item to consumer */
  }
}

void consumer(void)
{
  int item, i;
  message m;

  for (i = 0; i &lt; N; i++) send(producer, &amp;m); /* send N empties */
  while (TRUE) {
    receive(producer, &amp;m);                    /* get message containing item */
    item = extract_item(&amp;m);                  /* extract item from message */
    send(producer, &amp;m);                       /* send back empty reply */
    consume_item(item);                       /* do some1thing with the item */
  }
}



      </programlisting>

      <para>
	If the producer works faster than the consumer, all the messages will end up full, waiting for the consumer; the producer will be blocked, waiting for an empty to come back. If the consumer works faster, then the reverse happens: all the messages will be empties waiting for the producer to fill them up; the consumer will be blocked, waiting for a full message.
      </para>
      <para>
	Many variants are possible with message passing. For starters, let us look at how messages are addressed. One way is to assign each process a unique address and have messages be addressed to processes. A different way is to invent a new data structure, called a <command><emphasis>mailbox</emphasis></command>. A mailbox is a place to buffer a certain number of messages, typically specified when the mailbox is created. When mailboxes are used, the address parameters in the <command>send</command> and <command>receive</command> calls are mailboxes, not processes. When a process tries to send to a mailbox that is full, it is suspended until a message is removed from that mailbox, making room for a new one.
      </para>
      <para>
	For the producer-consumer problem, both the producer and consumer would create mailboxes large enough to hold N messages. The producer would send messages containing data to the consumer's mailbox, and the consumer would send empty messages to the producer's mailbox. When mailboxes are used, the buffering mechanism is clear: the destination mailbox holds messages that have been sent to the destination process but have not yet been accepted.
      </para>
      <para>
	The other extreme from having mailboxes is to eliminate all buffering. When this approach is followed, if the <command>send</command> is done before the <command>receive</command>, the sending process is blocked until the <command>receive</command> happens, at which time the message can be copied directly from the sender to the receiver, with no intermediate buffering. Similarly, if the <command>receive</command> is done first, the receiver is blocked until a <command>send</command> happens. This strategy is often known as a <command><emphasis>rendezvous</emphasis></command>. It is easier to implement than a buffered message scheme but is less flexible since the sender and receiver are forced to run in lockstep.
      </para>
      <para>
	The processes that make up the MINIX 3 operating system itself use the rendezvous method with fixed size messages for communication among themselves. User processes also use this method to communicate with operating system components, although a programmer does not see this, since library routines mediate systems calls. Interprocess communication between user processes in MINIX 3 (and UNIX) is via pipes, which are effectively mailboxes. The only real difference between a message system with mailboxes and the pipe mechanism is that pipes do not preserve message boundaries. In other words, if one process writes 10 messages of 100 bytes to a pipe and another process reads 1000 bytes from that pipe, the reader will get all 10 messages at once. With a true message system, each <command>read</command> should return only one message. Of course, if the processes agree always to read and write fixed-size messages from the pipe, or to end each message with a special character (e.g., linefeed), no problems arise.
      </para>
      <para>
	Message passing is commonly used in parallel programming systems. One well-known message-passing system, for example, is <command><emphasis>MPI</emphasis></command> (<command><emphasis>Message-Passing Interface</emphasis></command>). It is widely used for scientific computing. For more information about it, see for example Gropp et al. (1994) and Snir et al. (1996).
      </para>
    </sect2>
  </sect1>

  <sect1 id="sect-2.3">
    <title>2.3. Classical IPC Problems</title>
    <para>
      The operating systems literature is full of interprocess communication problems that have been widely discussed using a variety of synchronization methods. In the following sections we will examine two of the better-known problems.
    </para>

    <sect2 id="sect-2.3.1">
      <title>2.3.1. The Dining Philosophers Problem</title>
      <para>
	In 1965, Dijkstra posed and solved a synchronization problem he called the dining philosophers problem. Since that time, everyone inventing yet another synchronization primitive has felt obligated to demonstrate how wonderful the new primitive is by showing how elegantly it solves the dining philosophers problem. The problem can be stated quite simply as follows. Five philosophers are seated around a circular table. Each philosopher has a plate of spaghetti. The spaghetti is so slippery that a philosopher needs two forks to eat it. Between each pair of plates is one fork. The layout of the table is illustrated in <tag><link linkend="2-18">Fig. 2-18</link></tag>.
      </para>

      <para id="2-18"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/2-18.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 2-18. Lunch time in the Philosophy Department.</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	The life of a philosopher consists of alternate periods of eating and thinking. (This is something of an abstraction, even for philosophers, but the other activities are irrelevant here.) When a philosopher gets hungry, she tries to acquire her left and right fork, one at a time, in either order. If successful in acquiring two forks, she eats for a while, then puts down the forks and continues to think. The key question is: can you write a program for each philosopher that does what it is supposed to do and never gets stuck? (It has been pointed out that the two-fork requirement is somewhat artificial; perhaps we should switch from Italian to Chinese food, substituting rice for spaghetti and chopsticks for forks.)
      </para>
      <para>
	<tag><link linkend="2-19">Figure 2-19</link></tag> shows the obvious solution. The procedure <emphasis>take_fork</emphasis> waits until the specified fork is available and then seizes it. Unfortunately, the obvious solution is wrong. Suppose that all five philosophers take their left forks simultaneously. None will be able to take their right forks, and there will be a deadlock.
      </para>

      <para id="2-19"><command>Figure 2-19. A nonsolution to the dining philosophers problem.</command></para>
      <programlisting>#define N 5                /* number of philosophers */

void philosopher(int i)    /* i: philosopher number, from 0 to 4 */
{
  while (TRUE) {
    think();               /* philosopher is thinking */
    take_fork(i);          /* take left fork */
    take_fork((i+1) % N);  /* take right fork; % is modulo operator */
    eat();                 /* yum-yum, spaghetti */
    put_fork(i);           /* put left fork back on the table */
    put_fork((i+1) % N);   /* put right fork back on the table */
  }
}


      </programlisting>

      <para>
	We could modify the program so that after taking the left fork, the program checks to see if the right fork is available. If it is not, the philosopher puts down the left one, waits for some time, and then repeats the whole process. This proposal too, fails, although for a different reason. With a little bit of bad luck, all the philosophers could start the algorithm simultaneously, picking up their left forks, seeing that their right forks were not available, putting down their left forks, waiting, picking up their left forks again simultaneously, and so on, forever. A situation like this, in which all the programs continue to run indefinitely but fail to make any progress is called <command><emphasis>starvation</emphasis></command>. (It is called starvation even when the problem does not occur in an Italian or a Chinese restaurant.)
      </para>
      <para>
	Now you might think, "If the philosophers would just wait a random time instead of the same time after failing to acquire the right-hand fork, the chance that everything would continue in lockstep for even an hour is very small." This observation is true, and in nearly all applications trying again later is not a problem. For example, in a local area network using Ethernet, a computer sends a packet only when it detects no other computer is sending one. However, because of transmission delays, two computers separated by a length of cable may send packets that overlapa collision. When a collision of packets is detected each computer waits a random time and tries again; in practice this solution works fine. However, in some applications one would prefer a solution that always works and cannot fail due to an unlikely series of random numbers. Think about safety control in a nuclear power plant.
      </para>
      <para>
	One improvement to <tag><link linkend="2-19">Fig. 2-19</link></tag> that has no deadlock and no starvation is to protect the five statements following the call to <emphasis>think</emphasis> by a binary semaphore. Before starting to acquire forks, a philosopher would do a <command>down</command> on <emphasis>mutex</emphasis>. After replacing the forks, she would do an <command>up</command> on <emphasis>mutex</emphasis>. From a theoretical viewpoint, this solution is adequate. From a practical one, it has a performance bug: only one philosopher can be eating at any instant. With five forks available, we should be able to allow two philosophers to eat at the same time.
      </para>
      <para>
	The solution presented in <tag><link linkend="2-20">Fig. 2-20</link></tag> is deadlock-free and allows the maximum parallelism for an arbitrary number of philosophers. It uses an array, <emphasis>state</emphasis>, to keep track of whether a philosopher is eating, thinking, or hungry (trying to acquire forks). A philosopher may move into eating state only if neither neighbor is eating. Philosopher <emphasis>i</emphasis>'s neighbors are defined by the macros <emphasis>LEFT</emphasis> and <emphasis>RIGHT</emphasis>. In other words, if <emphasis>i</emphasis> is 2, <emphasis>LEFT</emphasis> is 1 and <emphasis>RIGHT</emphasis> is 3.
      </para>

      <para id="2-20"><command>Figure 2-20. A solution to the dining philosophers problem.</command></para>
      <programlisting>#define N            5             /* number of philosophers */
#define LEFT         (i+N-1)%N     /* number of i's left neighbor */
#define RIGHT        (i+1)%N       /* number of i's right neighbor */
#define THINKING     0             /* philosopher is thinking */
#define HUNGRY       1             /* philosopher is trying to get forks */
#define EATING       2             /* philosopher is eating */
typedef int semaphore;             /* semaphores are a special kind of int */
int state[N];                      /* array to keep track of everyone's state */
semaphore mutex = 1;               /* mutual exclusion for critical regions */
semaphore s[N];                    /* one semaphore per philosopher */

void philosopher(int i)            /* i: philosopher number, from 0 to N1 */
{
  while (TRUE){                    /* repeat forever */
    think();                       /* philosopher is thinking */
    take_forks(i);                 /* acquire two forks or block */
    eat();                         /* yum-yum, spaghetti */
    put_forks(i);                  /* put both forks back on table */
  }
}

void take_forks(int i)             /* i: philosopher number, from 0 to N1 */
{
  down(&amp;mutex);                    /* enter critical region */
  state[i] = HUNGRY;               /* record fact that philosopher i is hungry */
  test(i);                         /* try to acquire 2 forks */
  up(&amp;mutex);                      /* exit critical region */
  down(&amp;s[i]);                     /* block if forks were not acquired */
}

void put_forks(i)                  /* i: philosopher number, from 0 to N1 */
{
  down(&amp;mutex);                    /* enter critical region */
  state[i] = THINKING;             /* philosopher has finished eating */
  test(LEFT);                      /* see if left neighbor can now eat */
  test(RIGHT);                     /* see if right neighbor can now eat */
  up(&amp;mutex);                      /* exit critical region */
}

void test(i)                       /* i: philosopher number, from 0 to N1*/
{
     if (state[i] == HUNGRY &amp;&amp; state[LEFT] != EATING &amp;&amp; state[RIGHT] != EATING) {
           state[i] = EATING;
           up(&amp;s[i]);
     }
}


      </programlisting>

      <para>
	The program uses an array of semaphores, one per philosopher, so hungry philosophers can block if the needed forks are busy. Note that each process runs the procedure <emphasis>philosopher</emphasis> as its main code, but the other procedures, <emphasis>take_forks</emphasis>, <emphasis>put_forks</emphasis>, and test are ordinary procedures and not separate processes.
      </para>
    </sect2>

    <sect2 id="sect-2.3.2">
      <title>2.3.2. The Readers and Writers Problem</title>
      <para>
	The dining philosophers problem is useful for modeling processes that are competing for exclusive access to a limited number of resources, such as I/O devices. Another famous problem is the readers and writers problem which models access to a database (Courtois et al., 1971). Imagine, for example, an airline reservation system, with many competing processes wishing to read and write it. It is acceptable to have multiple processes reading the database at the same time, but if one process is updating (writing) the database, no other process may have access to the database, not even a reader. The question is how do you program the readers and the writers? One solution is shown in <tag><link linkend="2-21">Fig. 2-21</link></tag>.
      </para>

      <para id="2-21"><command>Figure 2-21. A solution to the readers and writers problem.</command></para>
      <programlisting>typedef int semaphore;           /* use your imagination */
semaphore mutex = 1;             /* controls access to 'rc' */
semaphore db = 1;                /* controls access to the database */
int rc = 0;                      /* # of processes reading or wanting to */

void reader(void)
{
  while (TRUE){                      /* repeat forever */
    down(&amp;mutex);                /* get exclusive access to 'rc' */
    rc = rc + 1;                 /* one reader more now */
    if (rc == 1) down(&amp;db);      /* if this is the first reader ... */
    up(&amp;mutex);                  /* release exclusive access to 'rc' */
    read_data_base();            /* access the data */
    down(&amp;mutex);                /* get exclusive access to 'rc' */
    rc = rc  1;                  /* one reader fewer now */
    if (rc == 0) up(&amp;db);        /* if this is the last reader ... */
    up(&amp;mutex);                  /* release exclusive access to 'rc' */
    use_data_read();             /* noncritical region */
  }
}


void writer(void)
{
  while (TRUE){                      /* repeat forever */
    think_up_data();              /* noncritical region */
    down(&amp;db);                    /* get exclusive access */
    write_data_base();            /* update the data */
    up(&amp;db);                      /* release exclusive access */
  }
}


      </programlisting>

      <para>
	In this solution, the first reader to get access to the data base does a <command>down</command> on the semaphore <emphasis>db</emphasis>. Subsequent readers merely have to increment a counter, <emphasis>rc</emphasis>. As readers leave, they decrement the counter and the last one out does an <command>up</command> on the semaphore, allowing a blocked writer, if there is one, to get in.
      </para>
      <para>
	The solution presented here implicitly contains a subtle decision that is worth commenting on. Suppose that while a reader is using the data base, another reader comes along. Since having two readers at the same time is not a problem, the second reader is admitted. A third and subsequent readers can also be admitted if they come along.
      </para>
      <para>
	Now suppose that a writer comes along. The writer cannot be admitted to the data base, since writers must have exclusive access, so the writer is suspended. Later, additional readers show up. As long as at least one reader is still active, subsequent readers are admitted. As a consequence of this strategy, as long as there is a steady supply of readers, they will all get in as soon as they arrive. The writer will be kept suspended until no reader is present. If a new reader arrives, say, every 2 seconds, and each reader takes 5 seconds to do its work, the writer will never get in.
      </para>
      <para>
	To prevent this situation, the program could be written slightly differently: When a reader arrives and a writer is waiting, the reader is suspended behind the writer instead of being admitted immediately. In this way, a writer has to wait for readers that were active when it arrived to finish but does not have to wait for readers that came along after it. The disadvantage of this solution is that it achieves less concurrency and thus lower performance. Courtois et al. present a solution that gives priority to writers. For details, we refer you to the paper.
      </para>
    </sect2>
  </sect1>

  <sect1 id="sect-2.4">
    <title>2.4. Scheduling</title>
    <para>
      In the examples of the previous sections, we have often had situations in which two or more processes (e.g., producer and consumer) were logically runnable. When a computer is multiprogrammed, it frequently has multiple processes competing for the CPU at the same time. When more than one process is in the ready state and there is only one CPU available, the operating system must decide which process to run first. The part of the operating system that makes the choice is called the <command><emphasis>scheduler</emphasis></command>; the algorithm it uses is called the <command><emphasis>scheduling algorithm</emphasis></command>.
    </para>
    <para>
      Many scheduling issues apply both to processes and threads. Initially, we will focus on process scheduling, but later we will take a brief look at some issues specific to thread scheduling.
    </para>

    <sect2 id="sect-2.4.1">
      <title>2.4.1. Introduction to Scheduling</title>
      <para>
	Back in the old days of batch systems with input in the form of card images on a magnetic tape, the scheduling algorithm was simple: just run the next job on the tape. With timesharing systems, the scheduling algorithm became more complex, because there were generally multiple users waiting for service. There may be one or more batch streams as well (e.g., at an insurance company, for processing claims). On a personal computer you might think there would be only one active process. After all, a user entering a document on a word processor is unlikely to be simultaneously compiling a program in the background. However, there are often background jobs, such as electronic mail daemons sending or receiving e-mail. You might also think that computers have gotten so much faster over the years that the CPU is rarely a scarce resource any more. However, new applications tend to demand more resources. Processing digital photographs or watching real time video are examples.
      </para>

      <para>      
	<bridgehead renderas="sect4"> <literallayout>
	              Process Behavior
	</literallayout></bridgehead>
      </para>

      <para>
	Nearly all processes alternate bursts of computing with (disk) I/O requests, as shown in <tag><link linkend="2-22">Fig. 2-22</link></tag>. Typically the CPU runs for a while without stopping, then a system call is made to read from a file or write to a file. When the system call completes, the CPU computes again until it needs more data or has to write more data, and so on. Note that some I/O activities count as computing. For example, when the CPU copies bits to a video RAM to update the screen, it is computing, not doing I/O, because the CPU is in use. I/O in this sense is when a process enters the blocked state waiting for an external device to complete its work.
      </para>

      <para id="2-22"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/2-22.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 2-22. Bursts of CPU usage alternate with periods of waiting for I/O. (a) A CPU-bound process. (b) An I/O-bound process.</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	The important thing to notice about <tag><link linkend="2-22">Fig. 2-22</link></tag> is that some processes, such as the one in <tag><link linkend="2-22">Fig. 2-22(a)</link></tag>, spend most of their time computing, while others, such as the one in <tag><link linkend="2-22">Fig. 2-22(b)</link></tag>, spend most of their time waiting for I/O. The former are called <command><emphasis>compute-bound</emphasis></command>; the latter are called <command><emphasis>I/O-bound</emphasis></command>. Compute-bound processes typically have long CPU bursts and thus infrequent I/O waits, whereas I/O-bound processes have short CPU bursts and thus frequent I/O waits. Note that the key factor is the length of the CPU burst, not the length of the I/O burst. I/O-bound processes are I/O bound because they do not compute much between I/O requests, not because they have especially long I/O requests. It takes the same time to read a disk block no matter how much or how little time it takes to process the data after they arrive.
      </para>
      <para>
	It is worth noting that as CPUs get faster, processes tend to get more I/O-bound. This effect occurs because CPUs are improving much faster than disks. As a consequence, the scheduling of I/O-bound processes is likely to become a more important subject in the future. The basic idea here is that if an I/O-bound process wants to run, it should get a chance quickly so it can issue its disk request and keep the disk busy.
      </para>

      <para>      
	<bridgehead renderas="sect4"> <literallayout>
	              When to Schedule
	</literallayout></bridgehead>
      </para>

      <para>
	There are a variety of situations in which scheduling may occur. First, scheduling is absolutely required on two occasions:
      </para>
      <orderedlist>
	<listitem>
	  <para>
	    When a process exits.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    When a process blocks on I/O, or a semaphore.
	  </para>
	</listitem>
      </orderedlist>
      <para>
	In each of these cases the process that had most recently been running becomes unready, so another must be chosen to run next.
      </para>
      <para>
	There are three other occasions when scheduling is usually done, although logically it is not absolutely necessary at these times:
      </para>
      <orderedlist>
	<listitem>
	  <para>
	    When a new process is created.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    When an I/O interrupt occurs.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    When a clock interrupt occurs.
	  </para>
	</listitem>
      </orderedlist>
      <para>
	In the case of a new process, it makes sense to reevaluate priorities at this time. In some cases the parent may be able to request a different priority for its child.
      </para>
      <para>
	In the case of an I/O interrupt, this usually means that an I/O device has now completed its work. So some process that was blocked waiting for I/O may now be ready to run.
      </para>
      <para>
	In the case of a clock interrupt, this is an opportunity to decide whether the currently running process has run too long. Scheduling algorithms can be divided into two categories with respect to how they deal with clock interrupts. A <command><emphasis>non-preemptive</emphasis></command> scheduling algorithm picks a process to run and then just lets it run until it blocks (either on I/O or waiting for another process) or until it voluntarily releases the CPU. In contrast, a <command><emphasis>preemptive</emphasis></command> scheduling algorithm picks a process and lets it run for a maximum of some fixed time. If it is still running at the end of the time interval, it is suspended and the scheduler picks another process to run (if one is available). Doing preemptive scheduling requires having a clock interrupt occur at the end of the time interval to give control of the CPU back to the scheduler. If no clock is available, nonpreemptive scheduling is the only option.
      </para>

      <para>      
	<bridgehead renderas="sect4"> <literallayout>
	              Categories of Scheduling Algorithms
	</literallayout></bridgehead>
      </para>

      <para>
	Not surprisingly, in different environments different scheduling algorithms are needed. This situation arises because different application areas (and different kinds of operating systems) have different goals. In other words, what the scheduler should optimize for is not the same in all systems. Three environments worth distinguishing are
      </para>
      <orderedlist>
	<listitem>
	  <para>
	    Batch.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    Interactive.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    Real time.
	  </para>
	</listitem>
      </orderedlist>
      <para>
	In batch systems, there are no users impatiently waiting at their terminals for a quick response. Consequently, nonpreemptive algorithms, or preemptive algorithms with long time periods for each process are often acceptable. This approach reduces process switches and thus improves performance.
      </para>
      <para>
	In an environment with interactive users, preemption is essential to keep one process from hogging the CPU and denying service to the others. Even if no process intentionally ran forever, due to a program bug, one process might shut out all the others indefinitely. Preemption is needed to prevent this behavior.
      </para>
      <para>
	In systems with real-time constraints, preemption is, oddly enough, sometimes not needed because the processes know that they may not run for long periods of time and usually do their work and block quickly. The difference with interactive systems is that real-time systems run only programs that are intended to further the application at hand. Interactive systems are general purpose and may run arbitrary programs that are not cooperative or even malicious.
      </para>

      <para>      
	<bridgehead renderas="sect4"> <literallayout>
	              Scheduling Algorithm Goals 
	</literallayout></bridgehead>
      </para>

      <para>
	In order to design a scheduling algorithm, it is necessary to have some idea of what a good algorithm should do. Some goals depend on the environment (batch, interactive, or real time), but there are also some that are desirable in all cases. Some goals are listed in <tag><link linkend="2-23">Fig. 2-23</link></tag>. We will discuss these in turn below.
      </para>

      <para id="2-23">
	<literallayout>
  <command>Figure 2-23. Some goals of the scheduling algorithm under different circumstances.</command>
	</literallayout>
	<literallayout>
<command>All systems</command>
      Fairness giving each process a fair share of the CPU
      Policy enforcement seeing that stated policy is carried out
      Balance keeping all parts of the system busy


<command>Batch systems</command>
      Throughput maximize jobs per hour
      Turnaround time minimize time between submission and termination
      CPU utilization keep the CPU busy all the time


<command>Interactive systems</command>
      Response time respond to requests quickly
      Proportionality meet users' expectations

 
<command>Realtime systems</command>
Meeting deadlines avoid losing data
Predictability avoid quality degradation in multimedia systems
 

	</literallayout>
      </para>

      <para>
	Under all circumstances, fairness is important. Comparable processes should get comparable service. Giving one process much more CPU time than an equivalent one is not fair. Of course, different categories of processes may be treated differently. Think of safety control and doing the payroll at a nuclear reactor's computer center.
      </para>
      <para>
	Somewhat related to fairness is enforcing the system's policies. If the local policy is that safety control processes get to run whenever they want to, even if it means the payroll is 30 sec late, the scheduler has to make sure this policy is enforced.
      </para>
      <para>
	Another general goal is keeping all parts of the system busy when possible. If the CPU and all the I/O devices can be kept running all the time, more work gets done per second than if some of the components are idle. In a batch system, for example, the scheduler has control of which jobs are brought into memory to run. Having some CPU-bound processes and some I/O-bound processes in memory together is a better idea than first loading and running all the CPU-bound jobs and then, when they are finished, loading and running all the I/O-bound jobs. If the latter strategy is used, when the CPU-bound processes are running, they will fight for the CPU and the disk will be idle. Later, when the I/O-bound jobs come in, they will fight for the disk and the CPU will be idle. Better to keep the whole system running at once by a careful mix of processes.
      </para>
      <para>
	The managers of corporate computer centers that run many batch jobs (e.g., processing insurance claims) typically look at three metrics to see how well their systems are performing: <command><emphasis>throughput</emphasis></command>, <command><emphasis>turnaround time</emphasis></command>, and <command><emphasis>CPU utilization</emphasis></command>. Throughput is the number of jobs per second that the system completes. All things considered, finishing 50 jobs per second is better than finishing 40 jobs per second. Turnaround time is the average time from the moment that a batch job is submitted until the moment it is completed. It measures how long the average user has to wait for the output. Here the rule is: Small is Beautiful.
      </para>
      <para>
	A scheduling algorithm that maximizes throughput may not necessarily minimize turnaround time. For example, given a mix of short jobs and long jobs, a scheduler that always ran short jobs and never ran long jobs might achieve an excellent throughput (many short jobs per second) but at the expense of a terrible turnaround time for the long jobs. If short jobs kept arriving at a steady rate, the long jobs might never run, making the mean turnaround time infinite while achieving a high throughput.
      </para>
      <para>
	CPU utilization is also an issue with batch systems because on the big mainframes where batch systems run, the CPU is still a major expense. Thus computer center managers feel guilty when it is not running all the time. Actually though, this is not such a good metric. What really matters is how many jobs per second come out of the system (throughput) and how long it takes to get a job back (turnaround time). Using CPU utilization as a metric is like rating cars based on how many times per second the engine turns over.
      </para>
      <para>
	For interactive systems, especially timesharing systems and servers, different goals apply. The most important one is to minimize <command><emphasis>response time</emphasis></command>, that is the time between issuing a command and getting the result. On a personal computer where a background process is running (for example, reading and storing email from the network), a user request to start a program or open a file should take precedence over the background work. Having all interactive requests go first will be perceived as good service.
      </para>
      <para>
	A somewhat related issue is what might be called <command><emphasis>proportionality</emphasis></command>. Users have an inherent (but often incorrect) idea of how long things should take. When a request that is perceived as complex takes a long time, users accept that, but when a request that is perceived as simple takes a long time, users get irritated. For example, if clicking on a icon that calls up an Internet provider using an analog modem takes 45 seconds to establish a connection, the user will probably accept that as a fact of life. On the other hand, if clicking on an icon that breaks the connection takes 45 seconds, the user will probably be swearing a blue streak by the 30-sec mark and frothing at the mouth by 45 sec. This behavior is due to the common user perception that placing a phone call and getting a connection is supposed to take a lot longer than just hanging up. In some cases (such as this one), the scheduler cannot do anything about the response time, but in other cases it can, especially when the delay is due to a poor choice of process order.
      </para>
      <para>
	Real-time systems have different properties than interactive systems, and thus different scheduling goals. They are characterized by having deadlines that must or at least should be met. For example, if a computer is controlling a device that produces data at a regular rate, failure to run the data-collection process on time may result in lost data. Thus the foremost need in a real-time system is meeting all (or most) deadlines.
      </para>
      <para>
	In some real-time systems, especially those involving multimedia, predictability is important. Missing an occasional deadline is not fatal, but if the audio process-runs too erratically, the sound quality will deteriorate rapidly. Video is also an issue, but the ear is much more sensitive to jitter than the eye. To avoid this problem, process scheduling must be highly predictable and regular.
      </para>
    </sect2>

    <sect2 id="sect-2.4.2">
      <title>2.4.2. Scheduling in Batch Systems</title>
      <para>
It is now time to turn from general scheduling issues to specific scheduling algorithms. In this section we will look at algorithms used in batch systems. In the following ones we will examine interactive and real-time systems. It is worth pointing out that some algorithms are used in both batch and interactive systems. We will study these later. Here we will focus on algorithms that are only suitable in batch systems.
      </para>

      <para>      
	<bridgehead renderas="sect4"> <literallayout>
	              First-Come First-Served 
	</literallayout></bridgehead>
      </para>

      <para>
	Probably the simplest of all scheduling algorithms is nonpreemptive <command><emphasis>first-come first-served</emphasis></command>. With this algorithm, processes are assigned the CPU in the order they request it. Basically, there is a single queue of ready processes. When the first job enters the system from the outside in the morning, it is started immediately and allowed to run as long as it wants to. As other jobs come in, they are put onto the end of the queue. When the running process blocks, the first process on the queue is run next. When a blocked process becomes ready, like a newly arrived job, it is put on the end of the queue.
      </para>
      <para>
	The great strength of this algorithm is that it is easy to understand and equally easy to program. It is also fair in the same sense that allocating scarce sports or concert tickets to people who are willing to stand on line starting at 2A .M . is fair. With this algorithm, a single linked list keeps track of all ready processes. Picking a process to run just requires removing one from the front of the queue. Adding a new job or unblocked process just requires attaching it to the end of the queue. What could be simpler?
      </para>
      <para>
	Unfortunately, first-come first-served also has a powerful disadvantage. Suppose that there is one compute-bound process that runs for 1 sec at a time and many I/O-bound processes that use little CPU time but each have to perform 1000 disk reads in order to complete. The compute-bound process runs for 1 sec, then it reads a disk block. All the I/O processes now run and start disk reads. When the compute-bound process gets its disk block, it runs for another 1 sec, followed by all the I/O-bound processes in quick succession.
      </para>
      <para>
	The net result is that each I/O-bound process gets to read 1 block per second and will take 1000 sec to finish. With a scheduling algorithm that preempted the compute-bound process every 10 msec, the I/O-bound processes would finish in 10 sec instead of 1000 sec, and without slowing down the compute-bound process very much.
      </para>

      <para>      
	<bridgehead renderas="sect4"> <literallayout>
	              Shortest Job First 
	</literallayout></bridgehead>
      </para>

      <para>
	Now let us look at another nonpreemptive batch algorithm that assumes the run times are known in advance. In an insurance company, for example, people can predict quite accurately how long it will take to run a batch of 1000 claims, since similar work is done every day. When several equally important jobs are sitting in the input queue waiting to be started, the scheduler picks the <command><emphasis>shortest job first</emphasis></command>. Look at <tag><link linkend="2-24">Fig. 2-24</link></tag>. Here we find four jobs <emphasis>A, B, C,</emphasis> and <emphasis>D</emphasis> with run times of 8, 4, 4, and 4 minutes, respectively. By running them in that order, the turnaround time for <emphasis>A</emphasis> is 8 minutes, for <emphasis>B</emphasis> is 12 minutes, for <emphasis>C</emphasis> is 16 minutes, and for <emphasis>D</emphasis> is 20 minutes for an average of 14 minutes.
      </para>

      <para id="2-24"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/2-24.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 2-24. An example of shortest job first scheduling. (a) Running four jobs in the original order. (b) Running them in shortest job first order.</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	Now let us consider running these four jobs using shortest job first, as shown in <tag><link linkend="2-24">Fig. 2-24(b)</link></tag>. The turnaround times are now 4, 8, 12, and 20 minutes for an average of 11 minutes. Shortest job first is provably optimal. Consider the case of four jobs, with run times of <emphasis>a, b, c,</emphasis> and <emphasis>d</emphasis>, respectively. The first job finishes at time a, the second finishes at time <emphasis>a + b</emphasis>, and so on. The mean turnaround time is (4 a + 3 b + 2 c + d) / 4. It is clear that a contributes more to the average than the other times, so it should be the shortest job, with <emphasis>b</emphasis> next, then <emphasis>c</emphasis>, and finally <emphasis>d</emphasis> as the longest as it affects only its own turnaround time. The same argument applies equally well to any number of jobs.
      </para>
      <para>
	It is worth pointing out that shortest job first is only optimal when all the jobs are available simultaneously. As a counterexample, consider five jobs, <emphasis>A</emphasis> through <emphasis>E</emphasis>, with run times of 2, 4, 1, 1, and 1, respectively. Their arrival times are 0, 0, 3, 3, and 3. Initially, only <emphasis>A</emphasis> or <emphasis>B</emphasis> can be chosen, since the other three jobs have not arrived yet. Using shortest job first we will run the jobs in the order <emphasis>A, B, C, D, E,</emphasis> for an average wait of 4.6. However, running them in the order <emphasis>B, C, D, E,</emphasis> A has an average wait of 4.4.
      </para>

      <para>      
	<bridgehead renderas="sect4"> <literallayout>
	              Shortest Remaining Time Next 
	</literallayout></bridgehead>
      </para>

      <para>
	A preemptive version of shortest job first is <command><emphasis>shortest remaining time next</emphasis></command>. With this algorithm, the scheduler always chooses the process whose remaining run time is the shortest. Again here, the run time has to be known in advance. When a new job arrives, its total time is compared to the current process' remaining time. If the new job needs less time to finish than the current process, the current process is suspended and the new job started. This scheme allows new short jobs to get good service.
      </para>

      <para>      
	<bridgehead renderas="sect4"> <literallayout>
	              Three-Level Scheduling 
	</literallayout></bridgehead>
      </para>

      <para>
	From a certain perspective, batch systems allow scheduling at three different levels, as illustrated in <tag><link linkend="2-25">Fig. 2-25</link></tag>. As jobs arrive at the system, they are initially placed in an input queue stored on the disk. The <command><emphasis>admission scheduler</emphasis></command> decides which jobs to admit to the system. The others are kept in the input queue until they are selected. A typical algorithm for admission control might be to look for a mix of compute-bound jobs and I/O-bound jobs. Alternatively, short jobs could be admitted quickly whereas longer jobs would have to wait. The admission scheduler is free to hold some jobs in the input queue and admit jobs that arrive later if it so chooses.
      </para>

      <para id="2-25"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/2-25.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 2-25. Three-level scheduling.</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	Once a job has been admitted to the system, a process can be created for it and it can contend for the CPU. However, it might well happen that the number of processes is so large that there is not enough room for all of them in memory. In that case, some of the processes have to be swapped out to disk. The second level of scheduling is deciding which processes should be kept in memory and which ones should be kept on disk. We will call this scheduler the <command><emphasis>memory scheduler</emphasis></command>, since it determines which processes are kept in memory and which on the disk.
      </para>
      <para>
	This decision has to be reviewed frequently to allow the processes on disk to get some service. However, since bringing a process in from disk is expensive, the review probably should not happen more often than once per second, maybe less often. If the contents of main memory are shuffled too often, a large amount of disk bandwidth will be wasted, slowing down file I/O.
      </para>
      <para>
	To optimize system performance as a whole, the memory scheduler might well want to carefully decide how many processes it wants in memory, called the <command><emphasis>degree of multiprogramming</emphasis></command>, and what kind of processes. If it has information about which processes are compute bound and which are I/O bound, it can try to keep a mix of these process types in memory. As a very crude approximation, if a certain class of process computes about 20% of the time, keeping five of them around is roughly the right number to keep the CPU busy.
      </para>
      <para>
	To make its decisions, the memory scheduler periodically reviews each process-on disk to decide whether or not to bring it into memory. Among the criteria that it can use to make its decision are the following ones:
      </para>
      <orderedlist>
	<listitem>
	  <para>
	    How long has it been since the process was swapped in or out?
	  </para>
	</listitem>
	<listitem>
	  <para>
	    How much CPU time has the process had recently?
	  </para>
	</listitem>
	<listitem>
	  <para>
	    How big is the process? (Small ones do not get in the way.)
	  </para>
	</listitem>
	<listitem>
	  <para>
	    How important is the process?
	  </para>
	</listitem>
      </orderedlist>
      <para>
	The third level of scheduling is actually picking one of the ready processes in main memory to run next. Often this is called the <command><emphasis>CPU scheduler</emphasis></command> and is the one people usually mean when they talk about the "scheduler." Any suitable algorithm can be used here, either preemptive or nonpreemptive. These include the ones described above as well as a number of algorithms to be described in the next section.
      </para>
    </sect2>

    <sect2 id="sect-2.4.3">
      <title>2.4.3. Scheduling in Interactive Systems</title>
      <para>
	We will now look at some algorithms that can be used in interactive systems. All of these can also be used as the CPU scheduler in batch systems as well. While three-level scheduling is not possible here, two-level scheduling (memory scheduler and CPU scheduler) is possible and common. Below we will focus on the CPU scheduler and some common scheduling algorithms.
      </para>

      <para>      
	<bridgehead renderas="sect4"> <literallayout>
	              Round-Robin Scheduling 
	</literallayout></bridgehead>
      </para>
	
      <para>
	Now let us look at some specific scheduling algorithms. One of the oldest, simplest, fairest, and most widely used algorithms is <command><emphasis>round robin</emphasis></command>. Each process is assigned a time interval, called its <command><emphasis>quantum</emphasis></command>, which it is allowed to run. If the process is still running at the end of the quantum, the CPU is preempted and given to another process. If the process has blocked or finished before the quantum has elapsed, the CPU switching is done when the process blocks, of course. Round robin is easy to implement. All the scheduler needs to do is maintain a list of runnable processes, as shown in <tag><link linkend="2-26">Fig. 2-26(a)</link></tag>. When the process uses up its quantum, it is put on the end of the list, as shown in <tag><link linkend="2-26">Fig. 2-26(b)</link></tag>.
      </para>

      <para id="2-26"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/2-26.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 2-26. Round-robin scheduling. (a) The list of runnable processes. (b) The list of runnable processes after B uses up its quantum.</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	The only interesting issue with round robin is the length of the quantum. Switching from one process to another requires a certain amount of time for doing the administrationsaving and loading registers and memory maps, updating various tables and lists, flushing and reloading the memory cache, etc. Suppose that this <command><emphasis>process switch</emphasis></command> or <command><emphasis>context switch</emphasis></command>, as it is sometimes called, takes 1 msec, including switching memory maps, flushing and reloading the cache, etc. Also suppose that the quantum is set at 4 msec. With these parameters, after doing 4 msec of useful work, the CPU will have to spend 1 msec on process switching. Twenty percent of the CPU time will be wasted on administrative overhead. Clearly, this is too much.
      </para>
      <para>
	To improve the CPU efficiency, we could set the quantum to, say, 100 msec. Now the wasted time is only 1 percent. But consider what happens on a timesharing system if ten interactive users hit the carriage return key at roughly the same time. Ten processes will be put on the list of runnable processes. If the CPU is idle, the first one will start immediately, the second one may not start until 100 msec later, and so on. The unlucky last one may have to wait 1 sec before getting a chance, assuming all the others use their full quanta. Most users will perceive a 1-sec response to a short command as sluggish.
      </para>
      <para>
	Another factor is that if the quantum is set longer than the mean CPU burst, preemption will rarely happen. Instead, most processes will perform a blocking operation before the quantum runs out, causing a process switch. Eliminating preemption improves performance because process switches then only happen when they are logically necessary, that is, when a process blocks and cannot continue because it is logically waiting for something.
      </para>
      <para>
	The conclusion can be formulated as follows: setting the quantum too short causes too many process switches and lowers the CPU efficiency, but setting it too long may cause poor response to short interactive requests. A quantum of around 2050 msec is often a reasonable compromise
      </para>

      <para>
	<bridgehead renderas="sect4"> <literallayout>
	              Priority Scheduling 
	</literallayout></bridgehead>
      </para>

      <para>
	Round-robin scheduling makes the implicit assumption that all processes are equally important. Frequently, the people who own and operate multiuser computers have different ideas on that subject. At a university, the pecking order may be deans first, then professors, secretaries, janitors, and finally students. The need to take external factors into account leads to <command><emphasis>priority scheduling</emphasis></command>. The basic idea is straightforward: Each process is assigned a priority, and the runnable process with the highest priority is allowed to run.
      </para>
      <para>
	Even on a PC with a single owner, there may be multiple processes, some more important than others. For example, a daemon process sending electronic mail in the background should be assigned a lower priority than a process displaying a video film on the screen in real time.
      </para>
      <para>
	To prevent high-priority processes from running indefinitely, the scheduler may decrease the priority of the currently running process at each clock tick (i.e., at each clock interrupt). If this action causes its priority to drop below that of the next highest process, a process switch occurs. Alternatively, each process may be assigned a maximum time quantum that it is allowed to run. When this quantum is used up, the next highest priority process is given a chance to run.
      </para>
      <para>
	Priorities can be assigned to processes statically or dynamically. On a military-computer, processes started by generals might begin at priority 100, processes started by colonels at 90, majors at 80, captains at 70, lieutenants at 60, and so on. Alternatively, at a commercial computer center, high-priority jobs might cost 100 dollars an hour, medium priority 75 dollars an hour, and low priority 50 dollars an hour. The UNIX system has a command, nice, which allows a user to voluntarily reduce the priority of his process, in order to be nice to the other users. Nobody ever uses it.
      </para>
      <para>
	Priorities can also be assigned dynamically by the system to achieve certain system goals. For example, some processes are highly I/O bound and spend most of their time waiting for I/O to complete. Whenever such a process wants the CPU, it should be given the CPU immediately, to let it start its next I/O request, which can then proceed in parallel with another process actually computing. Making the I/O-bound process wait a long time for the CPU will just mean having it around occupying memory for an unnecessarily long time. A simple algorithm for giving good service to I/O-bound processes is to set the priority to 1 / f, where f is the fraction of the last quantum that a process used. A process that used only 1 msec of its 50 msec quantum would get priority 50, while a process that ran 25 msec before blocking would get priority 2, and a process that used the whole quantum would get priority 1.
      </para>
      <para>
	It is often convenient to group processes into priority classes and use priority scheduling among the classes but round-robin scheduling within each class. <tag><link linkend="2-27">Figure 2-27</link></tag> shows a system with four priority classes. The scheduling algorithm is as follows: as long as there are runnable processes in priority class 4, just run each one for one quantum, round-robin fashion, and never bother with lower priority classes. If priority class 4 is empty, then run the class 3 processes round robin. If classes 4 and 3 are both empty, then run class 2 round robin, and so on. If priorities are not adjusted occasionally, lower priority classes may all starve to death.
      </para>

      <para id="2-27"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/2-27.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 2-27. A scheduling algorithm with four priority classes.</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	MINIX 3 uses a similar system to <tag><link linkend="2-27">Fig. 2-27</link></tag>, although there are sixteen priority classes in the default configuration. In MINIX 3, components of the operating system run as processes. MINIX 3 puts tasks (I/O drivers) and servers (memory manager, file system, and network) in the highest priority classes. The initial priority of each task or service is defined at compile time; I/O from a slow device may be given lower priority than I/O from a fast device or even a server. User processes generally have lower priority than system components, but all priorities can change during execution.
      </para>

      <para>
	<bridgehead renderas="sect4"> <literallayout>
	              Multiple Queues 
	</literallayout></bridgehead>
      </para>

      <para>
	One of the earliest priority schedulers was in CTSS (Corbató et al., 1962). CTSS had the problem that process switching was very slow because the 7094 could hold only one process in memory. Each switch meant swapping the current process to disk and reading in a new one from disk. The CTSS designers quickly realized that it was more efficient to give CPU-bound processes a large quantum once in a while, rather than giving them small quanta frequently (to reduce swapping). On the other hand, giving all processes a large quantum would mean poor response time, as we have already observed. Their solution was to set up priority classes. Processes in the highest class were run for one quantum. Processes in the next highest class were run for two quanta. Processes in the next class were run for four quanta, and so on. Whenever a process used up all the quanta allocated to it, it was moved down one class.
      </para>
      <para>
	As an example, consider a process that needed to compute continuously for 100 quanta. It would initially be given one quantum, then swapped out. Next time it would get two quanta before being swapped out. On succeeding runs it would get 4, 8, 16, 32, and 64 quanta, although it would have used only 37 of the final 64 quanta to complete its work. Only 7 swaps would be needed (including the initial load) instead of 100 with a pure round-robin algorithm. Furthermore, as the process sank deeper and deeper into the priority queues, it would be run less and less frequently, saving the CPU for short, interactive processes.
      </para>
      <para>
	The following policy was adopted to prevent a process that needed to run for a long time when it first started but became interactive later, from being punished forever. Whenever a carriage return was typed at a terminal, the process belonging to that terminal was moved to the highest priority class, on the assumption that it was about to become interactive. One fine day, some user with a heavily CPU-bound process discovered that just sitting at the terminal and typing carriage returns at random every few seconds did wonders for his response time. He told all his friends. Moral of the story: getting it right in practice is much harder than getting it right in principle.
      </para>
      <para>
	Many other algorithms have been used for assigning processes to priority classes. For example, the influential XDS 940 system (Lampson, 1968), built at Berkeley, had four priority classes, called terminal, I/O, short quantum, and long quantum. When a process that was waiting for terminal input was finally awakened, it went into the highest priority class (terminal). When a process waiting for a disk block became ready, it went into the second class. When a process was still running when its quantum ran out, it was initially placed in the third class. However, if a process used up its quantum too many times in a row without blocking for terminal or other I/O, it was moved down to the bottom queue. Many other systems use something similar to favor interactive users and processes over background ones.
      </para>

      <para>
	<bridgehead renderas="sect4"> <literallayout>
	              Shortest Process Next 
	</literallayout></bridgehead>
      </para>

      <para>
	Because shortest job first always produces the minimum average response time for batch systems, it would be nice if it could be used for interactive processes as well. To a certain extent, it can be. Interactive processes generally follow the pattern of wait for command, execute command, wait for command, execute command, and so on. If we regard the execution of each command as a separate "job," then we could minimize overall response time by running the shortest one first. The only problem is figuring out which of the currently runnable processes is the shortest one.
      </para>
      <para>
	One approach is to make estimates based on past behavior and run the process with the shortest estimated running time. Suppose that the estimated time per command for some terminal is <emphasis>T</emphasis><subscript>0</subscript>. Now suppose its next run is measured to be <emphasis>T</emphasis><subscript>1</subscript>. We could update our estimate by taking a weighted sum of these two numbers, that is, <emphasis>aT</emphasis> <subscript>0</subscript> + (1 <emphasis>a</emphasis>) <emphasis>T</emphasis> <subscript>1</subscript>. Through the choice of a we can decide to have the estimation process forget old runs quickly, or remember them for a long time. With a = 1/2, we get successive estimates of
      </para>

      <para/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/107.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	After three new runs, the weight of T0 in the new estimate has dropped to 1/8.
      </para>
      <para>
	The technique of estimating the next value in a series by taking the weighted average of the current measured value and the previous estimate is sometimes called <command><emphasis>aging</emphasis></command>. It is applicable to many situations where a prediction must be made based on previous values. Aging is especially easy to implement when a = 1/2. All that is needed is to add the new value to the current estimate and divide the sum by 2 (by shifting it right 1 bit).
      </para>

      <para>
	<bridgehead renderas="sect4"> <literallayout>
	              Guaranteed Scheduling 
	</literallayout></bridgehead>
      </para>

      <para>
	A completely different approach to scheduling is to make real promises to the users about performance and then live up to them. One promise that is realistic to make and easy to live up to is this: If there are <emphasis>n</emphasis> users logged in while you are working, you will receive about 1 /<emphasis>n</emphasis> of the CPU power. Similarly, on a single-user system with <emphasis>n</emphasis> processes running, all things being equal, each one should get 1 /<emphasis>n</emphasis> of the CPU cycles.
      </para>
      <para>
	To make good on this promise, the system must keep track of how much CPU each process has had since its creation. It then computes the amount of CPU each one is entitled to, namely the time since creation divided by n. Since the amount of CPU time each process has actually had is also known, it is straightforward to compute the ratio of actual CPU time consumed to CPU time entitled. A ratio of 0.5 means that a process has only had half of what it should have had, and a ratio of 2.0 means that a process has had twice as much as it was entitled to. The algorithm is then to run the process with the lowest ratio until its ratio has moved above its closest competitor.
      </para>

      <para>
	<bridgehead renderas="sect4"> <literallayout>
	              Lottery Scheduling 
	</literallayout></bridgehead>
      </para>

      <para>
	While making promises to the users and then living up to them is a fine idea, it is difficult to implement. However, another algorithm can be used to give similarly predictable results with a much simpler implementation. It is called <command><emphasis>lottery scheduling</emphasis></command> (Waldspurger and Weihl, 1994).
      </para>
      <para>
	The basic idea is to give processes lottery tickets for various system resources, such as CPU time. Whenever a scheduling decision has to be made, a lottery ticket is chosen at random, and the process holding that ticket gets the resource. When applied to CPU scheduling, the system might hold a lottery 50 times a second, with each winner getting 20 msec of CPU time as a prize.
      </para>
      <para>
	To paraphrase George Orwell: "All processes are equal, but some processes are more equal." More important processes can be given extra tickets, to increase their odds of winning. If there are 100 tickets outstanding, and one process holds 20 of them, it will have a 20 percent chance of winning each lottery. In the long run, it will get about 20 percent of the CPU. In contrast to a priority scheduler, where it is very hard to state what having a priority of 40 actually means, here the rule is clear: a process holding a fraction f of the tickets will get about a fraction f of the resource in question.
      </para>
      <para>
	Lottery scheduling has several interesting properties. For example, if a new process shows up and is granted some tickets, at the very next lottery it will have a chance of winning in proportion to the number of tickets it holds. In other words, lottery scheduling is highly responsive.
      </para>
      <para>
	Cooperating processes may exchange tickets if they wish. For example, when a client process sends a message to a server process and then blocks, it may give all of its tickets to the server, to increase the chance of the server running next. When the server is finished, it returns the tickets so the client can run again. In fact, in the absence of clients, servers need no tickets at all.
      </para>
      <para>
	Lottery scheduling can be used to solve problems that are difficult to handle with other methods. One example is a video server in which several processes are feeding video streams to their clients, but at different frame rates. Suppose that the processes need frames at 10, 20, and 25 frames/sec. By allocating these processes 10, 20, and 25 tickets, respectively, they will automatically divide the CPU in approximately the correct proportion, that is, 10 : 20 : 25.
      </para>

      <para>
	<bridgehead renderas="sect4"> <literallayout>
	              Fair-Share Scheduling 
	</literallayout></bridgehead>
      </para>

      <para>
	So far we have assumed that each process is scheduled on its own, without regard to who its owner is. As a result, if user 1 starts up 9 processes and user 2 starts up 1 process, with round robin or equal priorities, user 1 will get 90% of the CPU and user 2 will get only 10% of it.
      </para>
      <para>
	To prevent this situation, some systems take into account who owns a process before scheduling it. In this model, each user is allocated some fraction of the CPU and the scheduler picks processes in such a way as to enforce it. Thus if two users have each been promised 50% of the CPU, they will each get that, no matter how many processes they have in existence.
      </para>
      <para>
	As an example, consider a system with two users, each of which has been promised 50% of the CPU. User 1 has four processes, <emphasis>A, B, C,</emphasis> and <emphasis>D</emphasis>, and user 2 has only 1 process, <emphasis>E</emphasis>. If round-robin scheduling is used, a possible scheduling sequence that meets all the constraints is this one:
      </para>
      <para>
	A E B E C E D E A E B E C E D E ...
      </para>
      <para>
	On the other hand, if user 1 is entitled to twice as much CPU time as user 2, we might get
      </para>
      <para>
	A B E C D E A B E C D E ...
      </para>
      <para>
	Numerous other possibilities exist, of course, and can be exploited, depending on what the notion of fairness is.
      </para>
    </sect2>

    <sect2 id="sect-2.4.4">
      <title>2.4.4. Scheduling in Real-Time Systems</title>
      <para>
	A <command><emphasis>real-time</emphasis></command> system is one in which time plays an essential role. Typically, one or more physical devices external to the computer generate stimuli, and the computer must react appropriately to them within a fixed amount of time. For example, the computer in a compact disc player gets the bits as they come off the drive and must convert them into music within a very tight time interval. If the calculation takes too long, the music will sound peculiar. Other real-time systems are patient monitoring in a hospital intensive-care unit, the autopilot in an aircraft, and robot control in an automated factory. In all these cases, having the right answer but having it too late is often just as bad as not having it at all.
      </para>
      <para>
	Real-time systems are generally categorized as <command><emphasis>hard real time</emphasis></command>, meaning there are absolute deadlines that must be met, or else, and <command><emphasis>soft real time</emphasis></command>, meaning that missing an occasional deadline is undesirable, but nevertheless tolerable. In both cases, real-time behavior is achieved by dividing the program into a number of processes, each of whose behavior is predictable and known in advance. These processes are generally short lived and can run to completion in well under a second. When an external event is detected, it is the job of the scheduler to schedule the processes in such a way that all deadlines are met.
      </para>
      <para>
	The events that a real-time system may have to respond to can be further categorized as <command><emphasis>periodic</emphasis></command> (occurring at regular intervals) or <command><emphasis>aperiodic</emphasis></command> (occurring unpredictably). A system may have to respond to multiple periodic event streams. Depending on how much time each event requires for processing, it may not even be possible to handle them all. For example, if there are <emphasis>m</emphasis> periodic events and event <emphasis>i</emphasis> occurs with period <emphasis>P</emphasis><subscript>i</subscript> and requires <emphasis>C</emphasis><subscript>i</subscript> seconds of CPU time to handle each event, then the load can only be handled if
      </para>

      <para/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/109.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	A real-time system that meets this criteria is said to be <command><emphasis>schedulable</emphasis></command>.
      </para>
      <para>
	As an example, consider a soft real-time system with three periodic events, with periods of 100, 200, and 500 msec, respectively. If these events require 50, 30, and 100 msec of CPU time per event, respectively, the system is schedulable because 0.5 + 0.15 + 0.2 &lt; 1. If a fourth event with a period of 1 sec is added, the system will remain schedulable as long as this event does not need more than 150 msec of CPU time per event. Implicit in this calculation is the assumption that the context-switching overhead is so small that it can be ignored.
      </para>
      <para>
	Real-time scheduling algorithms can be static or dynamic. The former make their scheduling decisions before the system starts running. The latter make their scheduling decisions at run time. Static scheduling only works when there is perfect information available in advance about the work needed to be done and the deadlines that have to be met. Dynamic scheduling algorithms do not have these restrictions.
      </para>
    </sect2>

    <sect2 id="sect-2.4.5">
      <title>2.4.5. Policy versus Mechanism</title>
      <para>
	Up until now, we have tacitly assumed that all the processes in the system belong to different users and are thus competing for the CPU. While this is often true, sometimes it happens that one process has many children running under its control. For example, a database management system process may have many children. Each child might be working on a different request, or each one might have some specific function to perform (query parsing, disk access, etc.). It is entirely possible that the main process has an excellent idea of which of its children are the most important (or the most time critical) and which the least. Unfortunately, none of the schedulers discussed above accept any input from user processes about scheduling decisions. As a result, the scheduler rarely makes the best choice.
      </para>
      <para>
	The solution to this problem is to separate the <command><emphasis>scheduling mechanism</emphasis></command> from the <command><emphasis>scheduling policy</emphasis></command>. What this means is that the scheduling algorithm is parameterized in some way, but the parameters can be filled in by user processes. Let us consider the database example once again. Suppose that the kernel uses a priority scheduling algorithm but provides a system call by which a process can set (and change) the priorities of its children. In this way the parent can control in detail how its children are scheduled, even though it does not do the scheduling itself. Here the mechanism is in the kernel but policy is set by a user process.
      </para>
    </sect2>

    <sect2 id="sect-2.4.6">
      <title>2.4.6. Thread Scheduling</title>
      <para>
	When several processes each have multiple threads, we have two levels of parallelism present: processes and threads. Scheduling in such systems differs substantially depending on whether user-level threads or kernel-level threads (or both) are supported.
      </para>
      <para>
	Let us consider user-level threads first. Since the kernel is not aware of the existence of threads, it operates as it always does, picking a process, say, <emphasis>A</emphasis>, and giving <emphasis>A</emphasis> control for its quantum. The thread scheduler inside <emphasis>A</emphasis> decides which thread to run, say <emphasis>A1</emphasis>. Since there are no clock interrupts to multiprogram threads, this thread may continue running as long as it wants to. If it uses up the process' entire quantum, the kernel will select another process to run.
      </para>
      <para>
	When the process <emphasis>A</emphasis> finally runs again, thread <emphasis>A1</emphasis> will resume running. It will continue to consume all of <emphasis>A</emphasis>'s time until it is finished. However, its antisocial behavior will not affect other processes. They will get whatever the scheduler considers their appropriate share, no matter what is going on inside process <emphasis>A</emphasis>.
      </para>
      <para>
	Now consider the case that A's threads have relatively little work to do per CPU burst, for example, 5 msec of work within a 50-msec quantum. Consequently, each one runs for a little while, then yields the CPU back to the thread scheduler. This might lead to the sequence <emphasis>A1, A2, A3, A1, A2, A3, A1, A2, A3, A1,</emphasis> before the kernel switches to process B. This situation is illustrated in <tag><link linkend="2-28">Fig. 2-28(a)</link></tag>.
      </para>

      <para id="2-28"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/2-28.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 2-28. (a) Possible scheduling of user-level threads with a 50-msec process quantum and threads that run 5 msec per CPU burst. (b) Possible scheduling of kernel-level threads with the same characteristics as (a).</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	The scheduling algorithm used by the run-time system can be any of the ones described above. In practice, round-robin scheduling and priority scheduling are most common. The only constraint is the absence of a clock to interrupt a thread that has run too long.
      </para>
      <para>
	Now consider the situation with kernel-level threads. Here the kernel picks a particular thread to run. It does not have to take into account which process the thread belongs to, but it can if it wants to. The thread is given a quantum and is forceably suspended if it exceeds the quantum. With a 50-msec quantum but threads that block after 5 msec, the thread order for some period of 30 msec might be <emphasis>A1, B1, A2, B2, A3, B3,</emphasis> something not possible with these parameters and user-level threads. This situation is partially depicted in <tag><link linkend="2-28">Fig. 2-28(b)</link></tag>.
      </para>
      <para>
	A major difference between user-level threads and kernel-level threads is the performance. Doing a thread switch with user-level threads takes a handful of machine instructions. With kernel-level threads it requires a full context switch, changing the memory map, and invalidating the cache, which is several orders of magnitude slower. On the other hand, with kernel-level threads, having a thread block on I/O does not suspend the entire process as it does with user-level threads.
      </para>
      <para>
	Since the kernel knows that switching from a thread in process A to a thread in process B is more expensive that running a second thread in process A (due to having to change the memory map and having the memory cache spoiled), it can take this information into account when making a decision. For example, given two threads that are otherwise equally important, with one of them belonging to the same process as a thread that just blocked and one belonging to a different process, preference could be given to the former.
      </para>
      <para>
	Another important factor to consider is that user-level threads can employ an application-specific thread scheduler. For example, consider a web server which has a dispatcher thread to accept and distribute incoming requests to worker threads. Suppose that a worker thread has just blocked and the dispatcher thread and two worker threads are ready. Who should run next? The run-time system, knowing what all the threads do, can easily pick the dispatcher to run next, so it can start another worker running. This strategy maximizes the amount of parallelism in an environment where workers frequently block on disk I/O. With kernel-level threads, the kernel would never know what each thread did (although they could be assigned different priorities). In general, however, application-specific thread schedulers can tune an application better than the kernel can.
      </para>
    </sect2>
  </sect1>

  <sect1 id="sect-2.5">
    <title>2.5. Overview of Processes in MINIX 3</title>
    <para>
      Having completed our study of the principles of process management, interprocess communication, and scheduling, we can now take a look at how they are applied in MINIX 3. Unlike UNIX, whose kernel is a monolithic program not split up into modules, MINIX 3 itself is a collection of processes that communicate with each other and also with user processes, using a single interprocess communication primitivemessage passing. This design gives a more modular and flexible structure, making it easy, for example, to replace the entire file system by a completely different one, without having even to recompile the kernel.
    </para>

    <sect2 id="sect-2.5.1">
      <title>2.5.1. The Internal Structure of MINIX 3</title>
      <para>
	Let us begin our study of MINIX 3 by taking a bird's-eye view of the system. MINIX 3 is structured in four layers, with each layer performing a well-defined function. The four layers are illustrated in <tag><link linkend="2-29">Fig. 2-29</link></tag>.
      </para>

      <para id="2-29"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/2-29.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 2-29. MINIX 3 is structured in four layers. Only processes in the bottom layer may use privileged (kernel mode) instructions.</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	The <command><emphasis>kernel</emphasis></command> in the bottom layer schedules processes and manages the transitions between the ready, running, and blocked states of <tag><link linkend="2-2">Fig. 2-2</link></tag>. The kernel also handles all messages between processes. Message handling requires checking for legal destinations, locating the send and receive buffers in physical memory, and copying bytes from sender to receiver. Also part of the kernel is support for access to I/O ports and interrupts, which on modern processors require use of privileged <command><emphasis>kernel mode</emphasis></command> instructions not available to ordinary processes.
      </para>
      <para>
	In addition to the kernel itself, this layer contains two modules that function similarly to device drivers. The <command><emphasis>clock task</emphasis></command> is an I/O device driver in the sense that it interacts with the hardware that generates timing signals, but it is not user-accessible like a disk or communications line driverit interfaces only with the kernel.
      </para>
      <para>
	One of the main functions of layer 1 is to provide a set of privileged <command><emphasis>kernel calls</emphasis></command> to the drivers and servers above it. These include reading and writing I/O ports, copying data between address spaces, and so on. Implementation of these calls is done by the <command><emphasis>system task</emphasis></command>. Although the system task and the clock task are compiled into the kernel's address space, they are scheduled as separate processes and have their own call stacks.
      </para>
      <para>
	Most of the kernel and all of the clock and system tasks are written in C. However, a small amount of the kernel is written in assembly language. The assembly language parts deal with interrupt handling, the low-level mechanics of managing context switches between processes (saving and restoring registers and the like), and low-level parts of manipulating the MMU hardware. By and large, the assembly-language code handles those parts of the kernel that deal directly with the hardware at a very low level and which cannot be expressed in C. These parts have to be rewritten when MINIX 3 is ported to a new architecture.
      </para>
      <para>
	The three layers above the kernel could be considered to be a single layer because the kernel fundamentally treats them all of them the same way. Each one is limited to <command><emphasis>user mode</emphasis></command> instructions, and each is scheduled to run by the kernel. None of them can access I/O ports directly. Furthermore, none of them can access memory outside the segments allotted to it.
      </para>
      <para>
	However, processes potentially have special privileges (such as the ability to make kernel calls). This is the real difference between processes in layers 2, 3, and 4. The processes in layer 2 have the most privileges, those in layer 3 have some privileges, and those in layer 4 have no special privileges. For example, processes in layer 2, called <command><emphasis>device drivers</emphasis></command>, are allowed to request that the system task read data from or write data to I/O ports on their behalf. A driver is needed for each device type, including disks, printers, terminals, and network interfaces. If other I/O devices are present, a driver is needed for each one of those, as well. Device drivers may also make other kernel calls, such as requesting that newly-read data be copied to the address space of a different process.
      </para>
      <para>
	The third layer contains <command><emphasis>servers</emphasis></command>, processes that provide useful services to the user processes. Two servers are essential. The <command><emphasis>process manager (PM)</emphasis></command> carries out all the MINIX 3 system calls that involve starting or stopping process execution, such as <command>fork</command>, <command>exec</command>, and <command>exit</command>, as well as system calls related to signals, such as <command>alarm</command> and <command>kill</command>, which can alter the execution state of a process. The process manager also is responsible for managing memory, for instance, with the brk system call. The <command><emphasis>file system (FS)</emphasis></command> carries out all the file system calls, such as <command>read</command>, <command>mount</command>, and <command>chdir</command>.
      </para>
      <para>
	It is important to understand the difference between kernel calls and POSIX system calls. Kernel calls are low-level functions provided by the system task to allow the drivers and servers to do their work. Reading a hardware I/O port is a typical kernel call. In contrast, the POSIX system calls such as <command>read</command>, <command>fork</command>, and <command>unlink</command> are high-level calls defined by the POSIX standard, and are available to user programs in layer 4. User programs contain many POSIX calls but no kernel calls. Occasionally when we are not being careful with our language we may call a kernel call a system call. The mechanisms used to make these calls are similar, and kernel calls can be considered a special subset of system calls.
      </para>
      <para>
	In addition to the PM and FS, other servers exist in layer 3. They perform functions that are specific to MINIX 3. It is safe to say that the functionality of the process manager and the file system will be found in any operating system. The <command><emphasis>information server (IS)</emphasis></command> handles jobs such as providing debugging and status information about other drivers and servers, something that is more necessary in a system like MINIX 3, designed for experimentation, than would be the case for a commercial operating system which users cannot alter. The <command><emphasis>reincarnation server (RS)</emphasis></command> starts, and if necessary restarts, device drivers that are not loaded into memory at the same time as the kernel. In particular, if a driver fails during operation, the reincarnation server detects this failure, kills the driver if it is not already dead, and starts a fresh copy of the driver, making the system highly fault tolerant. This functionality is absent from most operating systems. On a networked system the optional <command><emphasis>network server (inet)</emphasis></command> is also in level 3. Servers cannot do I/O directly, but they can communicate with drivers to request I/O. Servers can also communicate with the kernel via the system task.
      </para>
      <para>
	As we noted at the start of <tag><xref linkend="Chapter1"/></tag>, operating systems do two things: manage resources and provide an extended machine by implementing system calls. In MINIX 3 the resource management is largely done by the drivers in layer 2, with help from the kernel layer when privileged access to I/O ports or the interrupt system is required. System call interpretation is done by the process manager and file system servers in layer 3. The file system has been carefully designed as a file "server" and could be moved to a remote machine with few changes.
      </para>
      <para>
	The system does not need to be recompiled to include additional servers. The process manager and the file system can be supplemented with the network server and other servers by attaching additional servers as required when MINIX 3 starts up or later. Device drivers, although typically started when the system is started, can also be started later. Both device drivers and servers are compiled and stored on disk as ordinary executable files, but when properly started up they are granted access to the special privileges needed. A user program called <command><emphasis>service</emphasis></command> provides an interface to the reincarnation server which manages this. Although the drivers and servers are independent processes, they differ from user processes in that normally they never terminate while the system is active.
      </para>
      <para>
	We will often refer to the drivers and servers in layers 2 and 3 as <command><emphasis>system processes</emphasis></command>. Arguably, system processes are part of the operating system. They do not belong to any user, and many if not all of them will be activated before the first user logs on. Another difference between system processes and user processes is that system processes have higher execution priority than user processes. In fact, normally drivers have higher execution priority than servers, but this is not automatic. Execution priority is assigned on a case-by-case basis in MINIX 3; it is possible for a driver that services a slow device to be given lower priority than a server that must respond quickly.
      </para>
      <para>
	Finally, layer 4 contains all the user processesshells, editors, compilers, and user-written <emphasis>a.out</emphasis> programs. Many user processes come and go as users log in, do work, and log out. A running system normally has some user processes that are started when the system is booted and which run forever. One of these is <emphasis>init</emphasis>, which we will describe in the next section. Also, several daemons are likely to be running. A <command><emphasis>daemon</emphasis></command> is a background process that executes periodically or always waits for some event, such as the arrival of a packet from the network. In a sense a daemon is a server that is started independently and runs as a user process. Like true servers installed at startup time, it is possible to configure a daemon to have a higher priority than ordinary user processes.
      </para>
      <para>
	A note about the terms <command><emphasis>task</emphasis></command> and <command><emphasis>device driver</emphasis></command> is needed. In older versions of MINIX all device drivers were compiled together with the kernel, which gave them access to data structures belonging to the kernel and each other. They also could all access I/O ports directly. They were referred to as "tasks" to distinguish them from pure independent user-space processes. In MINIX 3, device drivers have been implemented completely in user-space. The only exception is the clock task, which is arguably not a device driver in the same sense as drivers that can be accessed through device files by user processes. Within the text we have taken pains to use the term "task" only when referring to the clock task or the system task, both of which are compiled into the kernel to function. We have been careful to replace the word "task" with "device driver" where we refer to user-space device drivers. However, function names, variable names, and comments in the source code have not been as carefully updated. Thus, as you look at source code during your study of MINIX 3 you may find the word "task" where "device driver" is meant.
      </para>
    </sect2>

    <sect2 id="sect-2.5.2">
      <title>2.5.2. Process Management in MINIX 3</title>
      <para>
	Processes in MINIX 3 follow the general process model described at length earlier in this chapter. Processes can create subprocesses, which in turn can create more subprocesses, yielding a tree of processes. In fact, all the user processes in the whole system are part of a single tree with <emphasis>init</emphasis> (see <tag><link linkend="2-29">Fig. 2-29</link></tag>) at the root. Servers and drivers are a special case, of course, since some of them must be started before any user process, including <emphasis>init</emphasis>.
      </para>

      <para>
	<bridgehead renderas="sect4"> <literallayout>
	              MINIX 3 Startup 
	</literallayout></bridgehead>
      </para>

      <para>
	How does an operating system start up? We will summarize the MINIX 3 startup sequence in the next few pages. For a look at how some other operating systems do this, see Dodge et al. (2005).
      </para>
      <para>
	On most computers with disk devices, there is a <command><emphasis>boot disk</emphasis></command> hierarchy. Typically, if a floppy disk is in the first floppy disk drive, it will be the boot disk. If no floppy disk is present and a CD-ROM is present in the first CD-ROM drive, it becomes the boot disk. If there is neither a floppy disk nor a CD-ROM present, the first hard drive becomes the boot disk. The order of this hierarchy may be configurable by entering the BIOS immediately after powering the computer up. Additional devices, especially other removable storage devices, may be supported as well.
      </para>
      <para>
	When the computer is turned on, if the boot device is a diskette, the hardware reads the first sector of the first track of the boot disk into memory and executes the code it finds there. On a diskette this sector contains the <command><emphasis>bootstrap</emphasis></command> program. It is very small, since it has to fit in one sector (512 bytes). The MINIX 3 bootstrap loads a larger program, boot, which then loads the operating system itself.
      </para>
      <para>
	In contrast, hard disks require an intermediate step. A hard disk is divided into <command><emphasis>partitions</emphasis></command>, and the first sector of a hard disk contains a small program and the disk's <command><emphasis>partition table</emphasis></command>. Collectively these two pieces are called the <command><emphasis>master boot record</emphasis></command>. The program part is executed to read the partition table and to select the <command><emphasis>active partition</emphasis></command>. The active partition has a bootstrap on its first sector, which is then loaded and executed to find and start a copy of boot in the partition, exactly as is done when booting from a diskette.
      </para>
      <para>
	CD-ROMs came along later in the history of computers than floppy disks and hard disks, and when support for booting from a CD-ROM is present it is capable of more than just loading one sector. A computer that supports booting from a CD-ROM can load a large block of data into memory immediately. Typically what is loaded from the CD-ROM is an exact copy of a bootable floppy disk, which is placed in memory and used as a <command><emphasis>RAM disk</emphasis></command>. After this first step control is transferred to the RAM disk and booting continues exactly as if a physical floppy disk were the boot device. On an older computer which has a CD-ROM drive but does not support booting from a CD-ROM, the bootable floppy disk image can be copied to a floppy disk which can then be used to start the system. The CD-ROM must be in the CD-ROM drive, of course, since the bootable floppy disk image expects that.
      </para>
      <para>
	In any case, the MINIX 3 <emphasis>boot</emphasis> program looks for a specific multipart file on the diskette or partition and loads the individual parts into memory at the proper locations. This is the <command><emphasis>boot image</emphasis></command>. The most important parts are the kernel (which include the clock task and the system task), the process manager, and the file system. Additionally, at least one disk driver must be loaded as part of the boot image. There are several other programs loaded in the boot image. These include the reincarnation server, the RAM disk, console, and log drivers, and <emphasis>init</emphasis>.
      </para>
      <para>
	It should be strongly emphasized that all parts of the boot image are separate programs. After the essential kernel, process manager and file system have been loaded many other parts could be loaded separately. An exception is the reincarnation server. It must be part of the boot image. It gives ordinary processes loaded after initialization the special priorities and privileges which make them into system processes, It can also restart a crashed driver, which explains its name. As mentioned above, at least one disk driver is essential. If the root file system is to be copied to a RAM disk, the memory driver is also required, otherwise it could be loaded later. The <emphasis>tty</emphasis> and <emphasis>log</emphasis> drivers are optional in the boot image. They are loaded early just because it is useful to be able to display messages on the console and save information to a log early in the startup process. <emphasis>Init</emphasis> could certainly be loaded later, but it controls initial configuration of the system, and it was easiest just to include it in the boot image file.
      </para>
      <para>
	Startup is not a trivial operation. Operations that are in the realms of the disk driver and the file system must be performed by boot before these parts of the system are active. In a later section we will detail how MINIX 3 is started. For now, suffice it to say that once the loading operation is complete the kernel starts running.
      </para>
      <para>
	During its initialization phase the kernel starts the system and clock tasks, and then the process manager and the file system. The process manager and the file system then cooperate in starting other servers and drivers that are part of the boot image. When all these have run and initialized themselves, they will block, waiting for something to do. MINIX 3 scheduling prioritizes processes. Only when all tasks, drivers, and servers loaded in the boot image have blocked will <emphasis>init</emphasis>, the first user process, be executed. System components loaded with the boot image or during initialization are shown in <tag><link linkend="2-30">Fig. 2-30</link></tag>.
      </para>

      <para/>
      <table xml:id="ex.calstable5" frame="all">
	<tgroup cols="3" align="left" colsep="0" rowsep="1">
	  <tbody>
	    <row>
	      <entry>
		<para><command>Component</command></para>
	      </entry>
	      <entry>
		<para><command>Description</command></para>
	      </entry>
	      <entry>
		<para><command>Loaded by</command></para>
	      </entry>
	    </row>
	    <row>
	      <entry>
		<para>kernel</para>
	      </entry>
	      <entry>
		<para>Kernel + clock and system tasks</para>
	      </entry>
	      <entry>
		<para>(in boot image)</para>
	      </entry>
	    </row>
	    <row>
	      <entry>
		<para>pm</para>
	      </entry>
	      <entry>
		<para>Process manager</para>
	      </entry>
	      <entry>
		<para>(in boot image)</para>
	      </entry>
	    </row>
	    <row>
	      <entry>
		<para>fs</para>
	      </entry>
	      <entry>
		<para>File system</para>
	      </entry>
	      <entry>
		<para>(in boot image)</para>
	      </entry>
	    </row>
	    <row>
	      <entry>
		<para>rs</para>
	      </entry>
	      <entry>
		<para>(Re)starts servers and drivers</para>
	      </entry>
	      <entry>
		<para>(in boot image)</para>
	      </entry>
	    </row>
	    <row>
	      <entry>
		<para>memory</para>
	      </entry>
	      <entry>
		<para>RAM disk driver</para>
	      </entry>
	      <entry>
		<para>(in boot image)</para>
	      </entry>
	    </row>
	    <row>
	      <entry>
		<para>log</para>
	      </entry>
	      <entry>
		<para>Buffers log output</para>
	      </entry>
	      <entry>
		<para>(in boot image)</para>
	      </entry>
	    </row>
	    <row>
	      <entry>
		<para>tty</para>
	      </entry>
	      <entry>
		<para>Console and keyboard driver</para>
	      </entry>
	      <entry>
		<para>(in boot image)</para>
	      </entry>
	    </row>
	    <row>
	      <entry>
		<para>driver</para>
	      </entry>
	      <entry>
		<para>Disk (at, bios, or floppy) driver</para>
	      </entry>
	      <entry>
		<para>(in boot image)</para>
	      </entry>
	    </row>
	    <row>
	      <entry>
		<para>init</para>
	      </entry>
	      <entry>
		<para>parent of all user processes</para>
	      </entry>
	      <entry>
		<para>(in boot image)</para>
	      </entry>
	    </row>
	    <row>
	      <entry>
		<para>floppy</para>
	      </entry>
	      <entry>
		<para>Floppy driver (if booted from hard disk)</para>
	      </entry>
	      <entry>
		<para>/etc/rc</para>
	      </entry>
	    </row>
	    <row>
	      <entry>
		<para>is</para>
	      </entry>
	      <entry>
		<para>Information server (for debug dumps)</para>
	      </entry>
	      <entry>
		<para>/etc/rc</para>
	      </entry>
	    </row>
	    <row>
	      <entry>
		<para>cmos</para>
	      </entry>
	      <entry>
		<para>Reads CMOS clock to set time</para>
	      </entry>
	      <entry>
		<para>/etc/rc</para>
	      </entry>
	    </row>
	    <row>
	      <entry>
		<para>random</para>
	      </entry>
	      <entry>
		<para>Random number generator</para>
	      </entry>
	      <entry>
		<para>/etc/rc</para>
	      </entry>
	    </row>
	    <row>
	      <entry>
		<para>printer</para>
	      </entry>
	      <entry>
		<para>Printer driver</para>
	      </entry>
	      <entry>
		<para>/etc/rc</para>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </table>

      <para id="2-30">
	<command>Figure 2-30. Some important MINIX 3 system components. Others such as an Ethernet driver and the inet server may also be present.</command>
      </para>

      <para>
	<bridgehead renderas="sect4"> <literallayout>
	              Initialization of the Process Tree 
	</literallayout></bridgehead>
      </para>

      <para>
	<command><emphasis>Init</emphasis></command> is the first user process, and also the last process loaded as part of the boot image. You might think building of a process tree such as that of <tag><link linkend="1-5">Fig. 1-5</link></tag> begins once <emphasis>init</emphasis> starts running. Well, not exactly. That would be true in a conventional operating system, but MINIX 3 is different. First, there are already quite a few system processes running by the time init gets to run. The tasks <emphasis>CLOCK</emphasis> and <emphasis>SYSTEM</emphasis> that run within the kernel are unique processes that are not visible outside of the kernel. They receive no PIDs and are not considered part of any tree of processes. The process manager is the first process to run in user space; it is given PID 0 and is neither a child nor a parent of any other process. The reincarnation server is made the parent of all the other processes started from the boot image (e.g., the drivers and servers). The logic of this is that the reincarnation server is the process that should be informed if any of these should need to be restarted.
      </para>
      <para>
	As we will see, even after <emphasis>init</emphasis> starts running there are differences between the way a process tree is built in MINIX 3 and the conventional concept. <emphasis>Init</emphasis> in a UNIX-like system is given PID 1, and even though <emphasis>init</emphasis> is not the first process to run, the traditional PID 1 is reserved for it in MINIX 3. Like all the user space processes in the boot image (except the process manager), <emphasis>init</emphasis> is made one of the children of the reincarnation server. As in a standard UNIX-like system, <emphasis>init</emphasis> first executes the <command><emphasis>/etc/rc</emphasis></command> shell script. This script starts additional drivers and servers that are not part of the boot image. Any program started by the <emphasis>rc</emphasis> script will be a child of <emphasis>init</emphasis>. One of the first programs run is a utility called service. Service itself runs as a child of init, as would be expected. But now things once again vary from the conventional.
      </para>
      <para>
	<emphasis>Service</emphasis> is the user interface to the reincarnation server. The reincarnation server starts an ordinary program and converts it into a system process. It starts <emphasis>floppy</emphasis> (if it was not used in booting the system), <emphasis>cmos</emphasis> (which is needed to read the real-time clock), and is, the information server which manages the debug dumps that are produced by pressing function keys (F1, F2, etc.) on the console keyboard. One of the actions of the reincarnation server is to adopt all system processes except the process manager as its own children.
      </para>
      <para>
	After the <emphasis>cmos</emphasis> device driver has been started the <emphasis>rc</emphasis> script can initialize the real-time clock. Up to this point all files needed must be found on the root device. The servers and drivers needed initially are in the <emphasis>/sbin</emphasis> directory; other commands needed for startup are in <emphasis>/bin</emphasis>. Once the initial startup steps have been completed other file systems such as <emphasis>/usr</emphasis> are mounted. An important function of the <emphasis>rc</emphasis> script is to check for file system problems that might have resulted from a previous system crash. The test is simplewhen the system is shutdown correctly by executing the <emphasis>shutdown</emphasis> command an entry is written to the login history file, <emphasis>/usr/adm/wtmp</emphasis>. The command <command>shutdown C</command> checks whether the last entry in <emphasis>wtmp</emphasis> is a shutdown entry. If not, it is assumed an abnormal shutdown occurred, and the <emphasis>fsck</emphasis> utility is run to check all file systems. The final job of <emphasis>/etc/rc</emphasis> is to start daemons. This may be done by subsidiary scripts. If you look at the output of a <command>ps axl</command> command, which shows both PIDs and parent PIDs (PPIDs), you will see that daemons such as <emphasis>update</emphasis> and <emphasis>usyslogd</emphasis> will normally be the among the first persistent processes which are children of <emphasis>init</emphasis>.
      </para>
      <para>
	Finally <emphasis>init</emphasis> reads the file <emphasis>/etc/ttytab</emphasis>, which lists all potential terminal devices. Those devices that can be used as login terminals (in the standard distribution, just the main console and up to three virtual consoles, but serial lines and network pseudo terminals can be added) have an entry in the getty field of <emphasis>/etc/ttytab</emphasis>, and <emphasis>init</emphasis> forks off a child process for each such terminal. Normally, each child executes <emphasis>/usr/bin/getty</emphasis> which prints a message, then waits for a name to be typed. If a particular terminal requires special treatment (e.g., a dial-up line) <emphasis>/etc/ttytab</emphasis> can specify a command (such as <emphasis>/usr/bin/stty</emphasis>) to be executed to initialize the line before running <emphasis>getty</emphasis>.
      </para>
      <para>
	When a user types a name to log in, <emphasis>/usr/bin/login</emphasis> is called with the name as its argument. <emphasis>Login</emphasis> determines if a password is required, and if so prompts for and verifies the password. After a successful login, <emphasis>login</emphasis> executes the user's shell (by default <emphasis>/bin/sh</emphasis>, but another shell may be specified in the <emphasis>/etc/passwd</emphasis> file). The shell waits for commands to be typed and then forks off a new process for each command. In this way, the shells are the children of <emphasis>init</emphasis>, the user processes are the grandchildren of <emphasis>init</emphasis>, and all the user processes in the system are part of a single tree. In fact, except for the tasks compiled into the kernel and the process manager, all processes, both system processes and user processes, form a tree. But unlike the process tree of a conventional UNIX system, <emphasis>init</emphasis> is not at the root of the tree, and the structure of the tree does not allow one to determine the order in which system processes were started.
      </para>
      <para>
	The two principal MINIX 3 system calls for process management are <command>fork</command> and <command>exec</command>. <command>Fork</command> is the only way to create a new process. <command>Exec</command> allows a process to execute a specified program. When a program is executed, it is allocated a portion of memory whose size is specified in the program file's header. It keeps this amount of memory throughout its execution, although the distribution among data segment, stack segment, and unused can vary as the process runs.
      </para>
      <para>
	All the information about a process is kept in the process table, which is divided up among the kernel, process manager, and file system, with each one having those fields that it needs. When a new process comes into existence (by <command>fork</command>), or an old process terminates (by <command>exit</command> or a signal), the process manager first updates its part of the process table and then sends messages to the file system and kernel telling them to do likewise.
      </para>
    </sect2>

    <sect2 id="sect-2.5.3">
      <title>2.5.3. Interprocess Communication in MINIX 3</title>
      <para>
	Three primitives are provided for sending and receiving messages. They are called by the C library procedures
      </para>
      <para>
	<command>send(dest, &amp;message);</command>
      </para>
      <para>
	to send a message to process dest,
      </para>
      <para>
	<command>receive(source, &amp;message);</command>
      </para>
      <para>
	to receive a message from process source (or ANY), and
      </para>
      <para>
	<command>sendrec(src_dst, &amp;message);</command>
      </para>
      <para>
	to send a message and wait for a reply from the same process. The second parameter in each call is the local address of the message data. The message passing mechanism in the kernel copies the message from the sender to the receiver. The reply (for <command>sendrec</command>) overwrites the original message. In principle this kernel mechanism could be replaced by a function which copies messages over a network to a corresponding function on another machine, to implement a distributed system. In practice this would be complicated somewhat by the fact that message contents sometimes include pointers to large data structures, and a distributed system would have to provide for copying the data itself over the network.
      </para>
      <para>
	Each task, driver or server process is allowed to exchange messages only with certain other processes. Details of how this is enforced will be described later. The usual flow of messages is downward in the layers of <tag><link linkend="2-29">Fig 2-29</link></tag>, and messages can be between processes in the same layer or between processes in adjacent layers. User processes cannot send messages to each other. User processes in layer 4 can initiate messages to servers in layer 3, servers in layer 3 can initiate messages to drivers in layer 2.
      </para>
      <para>
	When a process sends a message to a process that is not currently waiting for a message, the sender blocks until the destination does a <command>receive</command>. In other words, MINIX 3 uses the rendezvous method to avoid the problems of buffering sent, but not yet received, messages. The advantage of this approach is that it is simple and eliminates the need for buffer management (including the possibility of running out of buffers). In addition, because all messages are of fixed length determined at compile time, buffer overrun errors, a common source of bugs, are structurally prevented.
      </para>
      <para>
	The basic purpose of the restrictions on exchanges of messages is that if process <emphasis>A</emphasis> is allowed to generate a <command>send</command> or <command>sendrec</command> directed to process <emphasis>B</emphasis>, then process <emphasis>B</emphasis> can be allowed to call <command>receive</command> with <emphasis>A</emphasis> designated as the sender, but <emphasis>B</emphasis> should not be allowed to <command>send</command> to <emphasis>A</emphasis>. Obviously, if <emphasis>A</emphasis> tries to <command>send</command> to <emphasis>B</emphasis> and blocks, and <emphasis>B</emphasis> tries to <command>send</command> to <emphasis>A</emphasis> and blocks we have a deadlock. The "resource" that each would need to complete the operations is not a physical resource like an I/O device, it is a call to <command>receive</command> by the target of the message. We will have more to say about deadlocks in <tag><xref linkend="Chapter3"/></tag>.
      </para>
      <para>
	Occasionally something different from a blocking message is needed. There exists another important message-passing primitive. It is called by the C library procedure
      </para>
      <para>
	<command>notify(dest);</command>
      </para>
      <para>
	and is used when a process needs to make another process aware that something important has happened. A <command>notify</command> is nonblocking, which means the sender continues to execute whether or not the recipient is waiting. Because it does not block, a notification avoids the possibility of a message deadlock.
      </para>
      <para>
	The message mechanism is used to deliver a notification, but the information conveyed is limited. In the general case the message contains only the identity of the sender and a timestamp added by the kernel. Sometimes this is all that is necessary. For instance, the keyboard uses a <command>notify</command> call when one of the function keys (F1 to F12 and shifted F1 to F12) is pressed. In MINIX 3, function keys are used to trigger debugging dumps. The Ethernet driver is an example of a process that generates only one kind of debug dump and never needs to get any other communication from the console driver. Thus a notification to the Ethernet driver from the keyboard driver when the dump-Ethernet-stats key is pressed is unambiguous. In other cases a notification is not sufficient, but upon receiving a notification the target process can send a message to the originator of the notification to request more information.
      </para>
      <para>
	There is a reason notification messages are so simple. Because a <command>notify</command> call does not block, it can be made when the recipient has not yet done a <command>receive</command>. But the simplicity of the message means that a notification that cannot be received is easily stored so the recipient can be informed of it the next time the recipient calls <command>receive</command>. In fact, a single bit suffices. Notifications are meant for use between system processes, of which there can be only a relatively small number. Every system process has a bitmap for pending notifications, with a distinct bit for every system process. So if process <emphasis>A</emphasis> needs to send a notification to process <emphasis>B</emphasis> at a time when process <emphasis>B</emphasis> is not blocked on a receive, the message-passing mechanism sets a bit which corresponds to <emphasis>A</emphasis> in <emphasis>B</emphasis>'s bitmap of pending notifications. When <emphasis>B</emphasis> finally does a <command>receive</command>, the first step is to check its pending notifications bitmap. It can learn of attempted notifications from multiple sources this way. The single bit is enough to regenerate the information content of the notification. It tells the identity of the sender, and the message passing code in the kernel adds the timestamp when it is delivered. Timestamps are used primarily to see if timers have expired, so it does not matter that the timestamp may be for a time later than the time when the sender first tried to send the notification.
      </para>
      <para>
	There is a further refinement to the notification mechanism. In certain cases an additional field of the notification message is used. When the notification is generated to inform a recipient of an interrupt, a bitmap of all possible sources of interrupts is included in the message. And when the notification is from the system task a bitmap of all pending signals for the recipient is part of the message. The natural question at this point is, how can this additional information be stored when the notification must be sent to a process that is not trying to receive a message? The answer is that these bitmaps are in kernel data structures. They do not need to be copied to be preserved. If a notification must be deferred and reduced to setting a single bit, when the recipient eventually does a <command>receive</command> and the notification message is regenerated, knowing the origin of the notification is enough to specify which additional information needs to be included in the message. And for the recipient, the origin of the notification also tells whether or not the message contains additional information, and, if so, how it is to be interpreted,
      </para>
      <para>
	A few other primitives related to interprocess communication exist. They will be mentioned in a later section. They are less important than <command>send</command>, <command>receive</command>, <command>sendrec</command>, and <command>notify</command>.
      </para>
    </sect2>

    <sect2 id="sect-2.5.4">
      <title>2.5.4. Process Scheduling in MINIX 3</title>
      <para>
	The interrupt system is what keeps a multiprogramming operating system going. Processes block when they make requests for input, allowing other processes to execute. When input becomes available, the current running process is interrupted by the disk, keyboard, or other hardware. The clock also generates interrupts that are used to make sure a running user process that has not requested input eventually relinquishes the CPU, to give other processes their chance to run. It is the job of the lowest layer of MINIX 3 to hide these interrupts by turning them into messages. As far as processes are concerned, when an I/O device completes an operation it sends a message to some process, waking it up and making it eligible to run.
      </para>
      <para>
	Interrupts are also generated by software, in which case they are often called <command><emphasis>traps</emphasis></command>. The <command>send</command> and <command>receive</command> operations that we described above are translated by the system library into <command><emphasis>software interrupt</emphasis></command> instructions which have exactly the same effect as hardware-generated interruptsthe process that executes a software interrupt is immediately blocked and the kernel is activated to process the interrupt. User programs do not refer to <command>send</command> or <command>receive</command> directly, but any time one of the system calls listed in <tag><link linkend="1-19">Fig. 1-9</link></tag> is invoked, either directly or by a library routine, <command>sendrec</command> is used internally and a software interrupt is generated.
      </para>
      <para>
	Each time a process is interrupted (whether by a conventional I/O device or by the clock) or due to execution of a software interrupt instruction, there is an opportunity to redetermine which process is most deserving of an opportunity to run. Of course, this must be done whenever a process terminates, as well, but in a system like MINIX 3 interruptions due to I/O operations or the clock or message passing occur more frequently than process termination.
      </para>
      <para>
	The MINIX 3 scheduler uses a multilevel queueing system. Sixteen queues are defined, although recompiling to use more or fewer queues is easy. The lowest priority queue is used only by the <emphasis>IDLE</emphasis> process which runs when there is nothing else to do. User processes start by default in a queue several levels higher than the lowest one.
      </para>
      <para>
	Servers are normally scheduled in queues with priorities higher than allowed for user processes, drivers in queues with priorities higher than those of servers, and the clock and system tasks are scheduled in the highest priority queue. Not all of the sixteen available queues are likely to be in use at any time. Processes are started in only a few of them. A process may be moved to a different priority queue by the system or (within certain limits) by a user who invokes the nice command. The extra levels are available for experimentation, and as additional drivers are added to MINIX 3 the default settings can be adjusted for best performance. For instance, if it were desired to add a server to stream digital audio or video to a network, such a server might be assigned a higher starting priority than current servers, or the initial priority of a current server or driver might be reduced in order for the new server to achieve better performance.
      </para>
      <para>
	In addition to the priority determined by the queue on which a process is placed, another mechanism is used to give some processes an edge over others. The quantum, the time interval allowed before a process is preempted, is not the same for all processes. User processes have a relatively low quantum. Drivers and servers normally should run until they block. However, as a hedge against malfunction they are made preemptable, but are given a large quantum. They are allowed to run for a large but finite number of clock ticks, but if they use their entire quantum they are preempted in order not to hang the system. In such a case the timed-out process will be considered ready, and can be put on the end of its queue. However, if a process that has used up its entire quantum is found to have been the process that ran last, this is taken as a sign it may be stuck in a loop and preventing other processes with lower priority from running. In this case its priority is lowered by putting it on the end of a lower priority queue. If the process times out again and another process still has not been able to run, its priority will again be lowered. Eventually, something else should get a chance to run.
      </para>
      <para>
	A process that has been demoted in priority can earn its way back to a higher priority queue. If a process uses all of its quantum but is not preventing other processes from running it will be promoted to a higher priority queue, up to the maximum priority permitted for it. Such a process apparently needs its quantum, but is not being inconsiderate of others.
      </para>
      <para>
	Otherwise, processes are scheduled using a slightly modified round robin. If a process has not used its entire quantum when it becomes unready, this is taken to mean that it blocked waiting for I/O, and when it becomes ready again it is put on the head of the queue, but with only the left-over part of its previous quantum. This is intended to give user processes quick response to I/O. A process that became unready because it used its entire quantum is placed at the end of the queue in pure round robin fashion.
      </para>
      <para>
	With tasks normally having the highest priority, drivers next, servers below drivers, and user processes last, a user process will not run unless all system processes have nothing to do, and a system process cannot be prevented from running by a user process.
      </para>
      <para>
	When picking a process to run, the scheduler checks to see if any processes are queued in the highest priority queue. If one or more are ready, the one at the head of the queue is run. If none is ready the next lower priority queue is similarly tested, and so on. Since drivers respond to requests from servers and servers respond to requests from user processes, eventually all high priority processes should complete whatever work was requested of them. They will then block with nothing to do until user processes get a turn to run and make more requests. If no process is ready, the IDLE process is chosen. This puts the CPU in a low-power mode until the next interrupt occurs.
      </para>
      <para>
	At each clock tick, a check is made to see if the current process has run for more than its allotted quantum. If it has, the scheduler moves it to the end of its queue (which may require doing nothing if it is alone on the queue). Then the next process to run is picked, as described above. Only if there are no processes on higher-priority queues and if the previous process is alone on its queue will it get to run again immediately. Otherwise the process at the head of the highest priority nonempty queue will run next. Essential drivers and servers are given such large quanta that normally they are normally never preempted by the clock. But if something goes wrong their priority can be temporarily lowered to prevent the system from coming to a total standstill. Probably nothing useful can be done if this happens to an essential server, but it may be possible to shut the system down gracefully, preventing data loss and possibly collecting information that can help in debugging the problem.
      </para>
    </sect2>
  </sect1>

  <sect1 id="sect-2.6">
    <title>2.6. Implementation of Processes in MINIX 3</title>
    <para>
      We are now moving closer to looking at the actual code, so a few words about the notation we will use are perhaps in order. The terms "procedure," "function," and "routine" will be used interchangeably. Names of variables, procedures, and files will be written in italics, as in <emphasis>rw_flag</emphasis>. When a variable, procedure, or file name starts a sentence, it will be capitalized, but the actual names begin with lower case letters. There are a few exceptions, the tasks which are compiled into the kernel are identified by upper case names, such as <emphasis>CLOCK</emphasis>, <emphasis>SYSTEM</emphasis>, and <emphasis>IDLE</emphasis>. System calls will be in lower case Helvetica, for example, <command>read</command>.
    </para>
    <para>
      The book and the software, both of which are continuously evolving, did not "go to press" on the same day, so there may be minor discrepancies between the references to the code, the printed listing, and the CD-ROM version. Such differences generally only affect a line or two, however. The source code printed in the book has been simplified by omitting code used to compile options that are not discussed in the book. The complete version is on the CD-ROM. The MINIX 3 Web site (<tag><link xl:ref="www.minix3.org">www.minix3.org</link></tag>) has the current version, which has new features and additional software and documentation.
    </para>

    <sect2 id="sect-2.6.1">
      <title>2.6.1. Organization of the MINIX 3 Source Code</title>
      <para>
	The implementation of MINIX 3 as described in this book is for an IBM PC-type machine with an advanced processor chip (e.g., 80386, 80486, Pentium, Pentium Pro, II, III, 4, M, or D) that uses 32-bit words. We will refer to all of these as Intel 32-bit processors. The full path to the C language source code on a standard Intel-based platform is <emphasis>/usr/src/</emphasis> (a trailing "/" in a path name indicates that it refers to a directory). The source directory tree for other platforms may be in a different location. Throughout the book, MINIX 3 source code files will be referred to using a path starting with the top <emphasis>src/</emphasis> directory. An important subdirectory of the source tree is <emphasis>src/include/</emphasis>, where the master copy of the C header files are located. We will refer to this directory as <emphasis>include/</emphasis>.
      </para>
      <para>
	Each directory in the source tree contains a file named <command><emphasis>Makefile</emphasis></command> which directs the operation of the UNIX-standard make utility. The <emphasis>Makefile</emphasis> controls compilation of files in its directory and may also direct compilation of files in one or more subdirectories. The operation of <emphasis>make</emphasis> is complex and a full description is beyond the scope of this section, but it can be summarized by saying that <emphasis>make</emphasis> manages efficient compilation of programs involving multiple source files. <emphasis>Make</emphasis> assures that all necessary files are compiled. It tests previously compiled modules to see if they are up to date and recompiles any whose source files have been modified since the previous compilation. This saves time by avoiding recompilation of files that do not need to be recompiled. Finally, <emphasis>make</emphasis> directs the combination of separately compiled modules into an executable program and may also manage installation of the completed program.
      </para>
      <para>
	All or part of the <emphasis>src/</emphasis> tree can be relocated, since the <emphasis>Makefile</emphasis> in each source directory uses a relative path to C source directories. For instance, you may want to make a source directory on the root filesystem, <emphasis>/src/</emphasis>, for speedy compilation if the root device is a RAM disk. If you are developing a special version you can make a copy of <emphasis>src/</emphasis> under another name.
      </para>
      <para>
	The path to the C header files is a special case. During compilation every <emphasis>Makefile</emphasis> expects to find header files in <emphasis>/usr/include/</emphasis> (or the equivalent path on a non-Intel platform). However, <emphasis>src/tools/Makefile</emphasis>, used to recompile the system, expects to find a master copy of the headers in <emphasis>/usr/src/include</emphasis> (on an Intel system). Before recompiling the system, however, the entire <emphasis>/usr/include/</emphasis> directory tree is deleted and <emphasis>/usr/src/include/</emphasis> is copied to <emphasis>/usr/include/</emphasis>. This was done to make it possible to keep all files needed in the development of MINIX 3 in one place. This also makes it easy to maintain multiple copies of the entire source and headers tree for experimenting with different configurations of the MINIX 3 system. However, if you want to edit a header file as part of such an experiment, you must be sure to edit the copy in the <emphasis>src/include</emphasis> directory and not the one in <emphasis>/usr/include/</emphasis>.
      </para>
      <para>
	This is a good place to point out for newcomers to the C language how file names are quoted in a <command>#include</command> statement. Every C compiler has a default header directory where it looks for include files. Frequently, this is <emphasis>/usr/include/</emphasis>. When the name of a file to include is quoted between less-than and greater-than symbols ("&lt; ... &gt;") the compiler searches for the file in the default header directory or a specified subdirectory, for example,
      </para>
      <para>
	<command>#include &lt;filename&gt;</command>
      </para>
      <para>
	includes a file from <emphasis>/usr/include/</emphasis>.
      </para>
      <para>
	Many programs also require definitions in local header files that are not meant to be shared system-wide. Such a header may have the same name as and be meant to replace or supplement a standard header. When the name is quoted between ordinary quote characters ("'' ... ''") the file is searched for first in the same directory as the source file (or a specified subdirectory) and then, if not found there, in the default directory. Thus
      </para>
      <para>
	<command>#include ''filename''</command>
      </para>
      <para>
	reads a local file.
      </para>
      <para>
	The <emphasis>include/</emphasis> directory contains a number of POSIX standard header files. In addition, it has three subdirectories:
      </para>
      <para>
	<emphasis>sys/</emphasis>        <literallayout>additional POSIX headers.</literallayout>
      </para>
      <para>
	<emphasis>minix/</emphasis>      <literallayout>header files used by the MINIX 3 operating system.</literallayout>
      </para>
      <para>
	<emphasis>ibm/</emphasis>        <literallayout>header files with IBM PC-specific definitions.</literallayout>
      </para>
      <para>
	To support extensions to MINIX 3 and programs that run in the MINIX 3 environment, other files and subdirectories are also present in <emphasis>include/</emphasis> as provided on the CD-ROM and also on the MINIX 3 Web site. For instance, <emphasis>include/arpa/</emphasis> and the <emphasis>include/net/</emphasis> directory and its subdirectory <emphasis>include/net/gen/</emphasis> support network extensions. These are not necessary for compiling the basic MINIX 3 system, and files in these directories are not listed in <tag><xref linkend="AppendixB"/></tag>.
      </para>
      <para>
	In addition to <emphasis>src/include/</emphasis>, the <emphasis>src/</emphasis> directory contains three other important subdirectories with operating system source code:
      </para>
      <para>
	<emphasis>kernel/</emphasis>     <literallayout>layer 1 (scheduling, messages, clock and system tasks).</literallayout>
      </para>
      <para>
	<emphasis>drivers/</emphasis>    <literallayout>layer 2 (device drivers for disk, console, printer, etc.).</literallayout>
      </para>
      <para>
	<emphasis>servers/</emphasis>    <literallayout>layer 3 (process manager, file system, other servers).</literallayout>
      </para>
      <para>
	Three other source code directories are not printed or discussed in the text, but are essential to producing a working system:
      </para>
      <para>
	<emphasis>src/lib/</emphasis>    <literallayout>source code for library procedures (e.g., open, read).</literallayout>
      </para>
      <para>
	<emphasis>src/tools/</emphasis>  <literallayout>Makefile and scripts for building the MINIX 3 system.</literallayout>
      </para>
      <para>
	<emphasis>src/boot/</emphasis>   <literallayout>the code for booting and installing MINIX 3.</literallayout>
      </para>
      <para>
	The standard distribution of MINIX 3 includes many additional source files not discussed in this text. In addition to the process manager and file system source code, the system source directory <emphasis>src/servers/</emphasis> contains source code for the <emphasis>init</emphasis> program and the reincarnation server, <emphasis>rs</emphasis>, both of which are essential parts of a running MINIX 3 system. The network server source code is in <emphasis>src/servers/inet/</emphasis>. <emphasis>Src/drivers/</emphasis> has source code for device drivers not discussed in this text, including alternative disk drivers, sound cards, and network adapters. Since MINIX 3 is an experimental operating system, meant to be modified, there is a <emphasis>src/test/</emphasis> directory with programs designed to test thoroughly a newly compiled MINIX 3 system. An operating system exists, of course, to support commands (programs) that will run on it, so there is a large <emphasis>src/commands/</emphasis> directory with source code for the utility programs (e.g., <emphasis>cat, cp, date, ls, pwd</emphasis> and more than 200 others). Source code for some major open source applications originally developed by the GNU and BSD projects is here, too.
      </para>
      <para>
	The "book" version of MINIX 3 is configured with many of the optional parts omitted (trust us: we cannot fit everything into one book or into your head in a semester-long course). The "book" version is compiled using modified <emphasis>Makefile</emphasis> s that do not refer to unnecessary files. (A standard <emphasis>Makefile</emphasis> requires that files for optional components be present, even if not to be compiled.) Omitting these files and the conditional statements that select them makes reading the code easier.
      </para>
      <para>
	For convenience we will usually refer to simple file names when it it is clear from the context what the complete path is. However, be aware that some file names appear in more than one directory. For instance, there are several files named const.h. <emphasis>Src/kernel/const.h</emphasis> defines constants used in the kernel, while <emphasis>src/servers/pm/const.h</emphasis> defines constants used by the process manager, etc.
      </para>
      <para>
	The files in a particular directory will be discussed together, so there should not be any confusion. The files are listed in <tag><xref linkend="AppendixB"/></tag> in the order they are discussed in the text, to make it easier to follow along. Acquisition of a couple of bookmarks might be of use at this point, so you can go back and forth between the text and the listing. To keep the size of the listing reasonable, code for every file is not printed. In general, those functions that are described in detail in the text are listed in <tag><xref linkend="AppendixB"/></tag>; those that are just mentioned in passing are not listed, but the complete source is on the CD-ROM and Web site, both of which also provide an index to functions, definitions, and global variables in the source code.
      </para>
      <para>
	<tag><xref linkend="AppendixC"/></tag> contains an alphabetical list of all files described in <tag><xref linkend="AppendixB"/></tag>, divided into sections for headers, drivers, kernel, file system, and process manager. This appendix and the Web site and CD-ROM indices reference the listed objects by line number in the source code.
      </para>
      <para>
	The code for layer 1 is contained in the directory <emphasis>src/kernel/</emphasis>. Files in this directory support process control, the lowest layer of the MINIX 3 structure we saw in <tag><link linkend="2-29">Fig. 2-29</link></tag>. This layer includes functions which handle system initialization, interrupts, message passing and process scheduling. Intimately connected with these are two modules compiled into the same binary, but which run as independent processes. These are the system task which provides an interface between kernel services and processes in higher layers, and the clock task which provides timing signals to the kernel. In <tag><xref linkend="Chapter3"/></tag>, we will look at files in several of the subdirectories of <emphasis>src/drivers</emphasis>, which support various device drivers, the second layer in <tag><link linkend="2-29">Fig. 2-29</link></tag>. Then in <tag><xref linkend="Chapter4"/></tag>, we will look at the process manager files in <emphasis>src/servers/pm/</emphasis>. Finally, in <tag><xref linkend="Chapter5"/></tag>, we will study the file system, whose source files are located in <emphasis>src/servers/fs/</emphasis>.
      </para>
    </sect2>

    <sect2 id="sect-2.6.2">
      <title>2.6.2. Compiling and Running MINIX 3</title>
      <para>
	To compile MINIX 3, run <command>make</command> in <emphasis>src/tools/</emphasis>. There are several options, for installing MINIX 3 in different ways. To see the possibilities run <command>make</command> with no argument. The simplest method is <command>make image</command>.
      </para>
      <para>
	When <command>make image</command> is executed, a fresh copy of the header files in <emphasis>src/include/</emphasis> is copied to <emphasis>/usr/include/</emphasis>. Then source code files in <emphasis>src/kernel/</emphasis> and several subdirectories of <emphasis>src/servers/</emphasis> and <emphasis>src/drivers/</emphasis> are compiled to object files. All the object files in <emphasis>src/kernel/</emphasis> are linked to form a single executable program, <emphasis>kernel</emphasis>. The object files in <emphasis>src/servers/pm/</emphasis> are also linked together to form a single executable program, <emphasis>pm</emphasis>, and all the object files in <emphasis>src/servers/fs/</emphasis> are linked to form <emphasis>fs</emphasis>. The additional programs listed as part of the boot image in <tag><link linkend="2-30">Fig. 2-30</link></tag> are also compiled and linked in their own directories. These include <emphasis>rs</emphasis> and <emphasis>init</emphasis> in subdirectories of <emphasis>src/servers/</emphasis> and <emphasis>memory/</emphasis>, <emphasis>log/</emphasis>, and <emphasis>tty/</emphasis> in subdirectories of <emphasis>src/drivers/</emphasis>. The component designated "driver" in <tag><link linkend="2-30">Fig. 2-30</link></tag> can be one of several disk drivers; we discuss here a MINIX 3 system configured to boot from the hard disk using the standard <emphasis>at_wini</emphasis> driver, which will be compiled in <emphasis>src/drivers/at_wini/</emphasis>. Other drivers can be added, but most drivers need not be compiled into the boot image. The same is true for networking support; compilation of the basic MINIX 3 system is the same whether or not networking will be used.
      </para>
      <para>
	To install a working MINIX 3 system capable of being booted, a program called <emphasis>installboot</emphasis> (whose source is in <emphasis>src/boot/</emphasis>) adds names to <emphasis>kernel, pm, fs, init</emphasis>, and the other components of the boot image, pads each one out so that its length is a multiple of the disk sector size (to make it easier to load the parts independently), and concatenates them onto a single file. This new file is the boot image and can be copied into the <emphasis>/boot/</emphasis> directory or the <emphasis>/boot/image/</emphasis> directory of a floppy disk or a hard disk partition. Later, the boot monitor program can load the boot image and transfer control to the operating system.
      </para>
      <para>
	<tag><link linkend="2-31">Figure 2-31</link></tag> shows the layout of memory after the concatenated programs are separated and loaded. The kernel is loaded in low memory, all the other parts of the boot image are loaded above 1 MB. When user programs are run, the available memory above the kernel will be used first. When a new program will not fit there, it will be loaded in the high memory range, above init. Details, of course, depend upon the system configuration. For instance, the example in the figure is for a MINIX 3 file system configured with a block cache that can hold 512 4-KB disk blocks. This is a modest amount; more is recommended if adequate memory is available. On the other hand, if the size of the block cache were reduced drastically it would be possible to make the entire system fit into less than 640K of memory, with room for a few user processes as well.
      </para>

      <para id="2-31"/>
      <screenshot>
	<mediaobject>
	  <imageobject>
	    <imagedata align="center" fileref="images/2-31.jpg" format="jpg">
	    </imagedata>
	  </imageobject>
	  <caption>
	    <para>
	      <command>Figure 2-31. Memory layout after MINIX 3 has been loaded from the disk into memory. The kernel, servers, and drivers are independently compiled and linked programs, listed on the left. Sizes are approximate and not to scale.</command>
	    </para>
	  </caption>
	</mediaobject>
      </screenshot>

      <para>
	It is important to realize that MINIX 3 consists of several totally independent programs that communicate only by passing messages. A procedure called <emphasis>panic</emphasis> in the directory <emphasis>src/servers/fs/</emphasis> does not conflict with a procedure called <emphasis>panic</emphasis> in <emphasis>src/servers/pm/</emphasis> because they ultimately are linked into different executable files. The only procedures that the three pieces of the operating system have in common are a few of the library routines in <emphasis>src/lib/</emphasis>. This modular structure makes it very easy to modify, say, the file system, without having these changes affect the process manager. It also makes it straightforward to remove the file system altogether and to put it on a different machine as a file server, communicating with user machines by sending messages over a network.
      </para>
      <para>
	As another example of the modularity of MINIX 3, adding network support makes absolutely no difference to the process manager, the file system, or the kernel. Both an Ethernet driver and the inet server can be activated after the boot image is loaded; they would appear in <tag><link linkend="2-30">Fig. 2-30</link></tag> with the processes started by <emphasis>/etc/rc</emphasis>, and they would be loaded into one of the "Memory available for user programs" regions of <tag><link linkend="2-31">Fig. 2-31</link></tag>. A MINIX 3 system with networking enabled can be used as a remote terminal or an ftp and web server. Only if you want to allow incoming logins to the MINIX 3 system over the network would any part of MINIX 3 as described in the text need modification: this is tty, the console driver, which would need to be recompiled with pseudo terminals configured to allow remote logins.
      </para>
    </sect2>


  </sect1>

</chapter>
  <chapter id="Chapter3"><title>Input/Output</title>
<para>
One of the main functions of an operating system is to control all the computer's I/O (Input/Output) devices. It must issue commands to the devices, catch interrupts, and handle errors. It should also provide an interface between the devices and the rest of the system that is simple and easy to use. To the extent possible, the interface should be the same for all devices (device independence). The I/O code represents a significant fraction of the total operating system. Thus to really understand what an operating system does, you have to understand how I/O works. How the operating system manages I/O is the main subject of this chapter.
</para>
</chapter>
  <chapter id="Chapter4"><title>Memory Management</title>
<para>
Memory is an important resource that must be carefully managed. While the average home computer nowadays has two thousand times as much memory as the IBM 7094 (the largest computer in the world in the early 1960s), programs and the data they are expected to handle have also grown tremendously. To paraphrase Parkinson's law, "Programs and their data expand to fill the memory available to hold them." In this chapter we will study how operating systems manage memory.
</para>
</chapter>
  <chapter id="Chapter5"><title>File Systems</title>
<para>
All computer applications need to store and retrieve information. While a process is running, it can store a limited amount of information within its own address space. However, the storage capacity is restricted to the size of the virtual address space. For some applications this size is adequate, but for others, such as airline reservations, banking, or corporate record keeping, it is far too small.
</para>
</chapter>
  <chapter id="Chapter6"><title>Reading List and Bibliography</title>
<para>
In the previous five chapters we have touched upon a variety of topics. This chapter is intended as an aid to readers interested in pursuing their study of operating systems further. Section 6.1 is a list of suggested readings. Section 6.2 is an alphabetical bibliography of all books and articles cited in this book.
</para>
</chapter>
  <appendix id="AppendixA"><title>Installing MINIX3</title>
<para>
This appendix explains how to install MINIX 3. A complete MINIX 3 installation requires a Pentium (or compatible) with at least 16-MB of RAM, 1 GB of free disk space, an IDE CD-ROM and an IDE hard disk. A minimal installation (without the commands sources) requires 8 MB RAM and 50 MB of disk space. Serial ATA, USB, and SCSI disks are not supported at present. For USB CD-ROMS, see the Website: www.minix3.org.
</para>
</appendix>
  <appendix id="AppendixB"><title>The MINIX Source Code</title>
<para>


</para>
</appendix>
  <appendix id="AppendixC"><title>Index to Files</title>
<para>
Include directory


00000 include/ansi.h



00200 include/errno.h



00900 include/fcntl.h



00100 include/limits.h



00700 include/signal.h



00600 include/string.h



01000 include/termios.h



01300 include/timers.h



00400 include/unistd.h



04400 include/ibm/interrupt.h



04300 include/ibm/portio.h



04500 include/ibm/ports.h



03500 include/minix/callnr.h



03600 include/minix/com.h



02300 include/minix/config.h



02600 include/minix/const.h



04100 include/minix/devio.h



04200 include/minix/dmap.h



02200 include/minix/ioctl.h



03000 include/minix/ipc.h



02500 include/minix/sys_config.h



03200 include/minix/syslib.h



03400 include/minix/sysutil.h



02800 include/minix/type.h



01800 include/sys/dir.h



02100 include/sys/ioc_disk.h



02000 include/sys/ioctl.h



01600 include/sys/sigcontext.h



01700 include/sys/stat.h



01400 include/sys/types.h



01900 include/sys/wait.h
</para>
</appendix>
</book>
